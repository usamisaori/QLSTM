### 2.3.4 参数更新

本节简要讨论 QNN 的参数更新过程。如前所述，变分量子算法多是在经典计算机上完成参数更新过程，如图 2.3.1（d）所示。考虑本研究相关的回归问题，基于均方误差（Mean-Square Error，MSE）可以给出 QNN 在训练集 $\mathcal{D}$ 上的损失函数为：

$$\mathcal{C}(\overrightarrow{\theta};\mathcal{D}) = \frac{1}{n}\sum_{i=1}^{N}|\mathbb{E}(\sigma_z)-y_i| ^2$$

考虑到训练成本，一般使用随机梯度下降法，在小批量数据集 $\mathcal{B}$ 上计算损失函数，结果为 

$$\mathcal{C}(\overrightarrow{\theta};\mathcal{B}) = \frac{1}{n}\sum|\mathbb{E}(\sigma_z)-y_i| ^2$$

从而可以基于损失函数的梯度进行参数更新：

$$\overrightarrow{\theta}^{(t)} = \overrightarrow{\theta}^{(t-1)} - \eta \frac{\partial 
\mathcal{C}}{\partial \overrightarrow{\theta}}$$

实际情况下多会采用其他优化器（例如 Adam）来替代传统的梯度下降参数更新过程。此外注意到基于链式法则可以将损失函数的梯度 $\frac{\partial \mathcal{C}}{\partial \overrightarrow{\theta}}$ 表示为关于 QNN 输出期望的梯度 $\frac{\partial \mathbb{E}(\sigma_z)}{\partial \overrightarrow{\theta}}$ 的函数，而后者可以经由参数偏移（parameter-shift）的方法来简化计算[85]：

$$\nabla_{\theta_i} f(\overrightarrow{x}; \theta_i) =  \frac{1}{2}\left [f(\overrightarrow{x}; \theta_i + \frac{\pi}{2})  - f(\overrightarrow{x};\theta_i - \frac{\pi}{2}) \right ]$$