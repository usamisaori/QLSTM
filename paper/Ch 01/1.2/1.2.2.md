### 1.2.2 量子深度学习与量子神经网络

作为一种新型的计算模式，量子计算以量子力学系统能够完成的信息处理任务为研究对象，基于量子叠加、量子并行性、量子纠缠等量子特性为其在计算与信息处理上表现出优于经典计算的一定优势[40]。基于量子计算的量子算法在解决一些问题时可以显著超越现存的经典算法，这被称为量子加速[41]。量子机器学习（Quantum Machine Learning，QML）将量子计算引入经典机器学习领域，旨在试图借助上述量子特性来改进机器学习中的现有方法以更加高效的解决不同问题[42-43]。2008年发表的 Harrow-Hassidim-Lloyd（HHL）算法[44]给出了在特定条件下指数加速求解线性方程组的量子算法，此后以此为基础探索利用量子加速改良经典机器学习训练过程的思路得到了广泛研究，产生了包括量子主成分分析[45]、量子支持向量机[46]等在内的量子算法，且相对于对应的经典算法，这些量子算法在一定条件下能取得指数级的加速。

基于类似的动机通过将量子计算技术引入深度学习领域产生了量子深度学习（Quantum Deep Learning，QDL）相关的研究[47]。当前此类研究大体上可分为基于量子启发的经典深度学习算法研究以及基于量子神经网络的量子深度学习算法研究[48]。前者主要借由利用量子技术辅助或改良现有的深度学习技术，例如 Amin[49] 等提出了一种基于量子采样的方法以用于受限玻尔兹曼机生成的训练，并证明其效果比传统的吉布斯采样更优；Li[50] 等采用基于量子的粒子群优化算法以用于寻找最佳的 CNN 模型架构，并实现较传统方法更优的性能与鲁棒性；Adachi 与 Henderson[51] 探索了使用基于量子涨落特性的量子退火算法运用于经典深度学习优化的研究，表明其具有优于经典模拟退火的效率等。后者的研究则主要通过类比传统神经网络层构建基于量子线路或其他量子架构的量子神经网络（Quantum Neural Networks，QNN）来替代经典深度学习中的神经网络层以实现量子版本的深度学习。换言之，在此类研究中量子深度学习网络将由（或部分由）量子神经网络层构成。带参数可训练的变分量子线路（Variational Quantum Circuits，VQC）[52]相关研究的兴起使得此类量子深度学习研究正得到越来越广泛的关注。

如前所述，量子神经网络往往是类比经典神经网络结构，并借由量子线路等量子架构来构造经典神经网络的类似物，以期能够将量子计算中的量子并行、量子纠缠等量子特性引入经典神经网络架构进而改良经典深度学习。Kak 于1995年最早探索将量子计算与神经网络相结合，提出了量子神经网络计算的概念[53]，此后不同研究者尝试构建了不同类型的 QNN。早期的典型研究包括：Behrman 等[54]提出了基于量子点结构的量子点神经网络；Tóth 等[55]提出了基于耦合量子细胞的量子细胞神经网络；Kouda 等[56]将量子比特视作有状态的神经元提出了基于量子比特的神经网络；Matsui 等[57]提出了具有代表性的使用量子旋转门和受控非门构建神经网络的方法等。Jeswal 和 Chakraverty[58] 总结了常见的 QNN 实现类型，黄一鸣等[59]对 QNN 的发展历史作了详尽的综述并就 QNN 实现类型进行了归纳。显然早期的不同研究者对于 QNN 采用了不同的定义，Schuld 等[60]在系统分析了不同类型的 QNN 实现的基础上就有意义的 QNN 模型给出了三个前提条件：（1）系统的初始状态可以编码任意长度的二进制字符串；（2）计算过程可以反应经典神经网络的基本计算原理；（3）系统的演化过程基于量子效应，且符合量子力学的基本理论。但同时 Schuld 也指出多数的 QNN 可能无法完全满足上述条件，并且多数研究是在数学计算或仿真层面进行的讨论，存在物理可行性不明确等问题，这主要是受到量子硬件的客观发展情况的限制。

从历史发展来看，QNN 大致经历了早期阶段和近期量子处理器阶段[61]。早期阶段受限于硬件条件， QNN 无法在量子计算机上实现。此时期多数 QNN 模型的提出是基于量子计算相关的物理过程，但并未就具体的量子计算处理器结构（例如具体的量子比特与量子线路结构）作出描述。例如早期的基于量子点的 QNN 或基于简单量子门结构的 QNN 等。近来伴随着量子硬件研究的发展，QNN 模型的研究也愈发重视模型的物理可行性。近期量子处理器阶段的研究来看，QNN 更趋向于指代由量子线路或量子系统实现的具备网络结构与可训练参数的计算模型[62]。变分量子线路（VQC）作为一类带参数可训练的量子线路，其本身可以很好的类比经典神经网络结构，同时具备一定的物理可实现性，这都使得基于 VQC 构建 QNN 成为当前实现 QNN 模型的较主流的方法之一[48，52]。因而也有研究直接将 QNN 视作变分量子算法（Variational Quantum Algorithms，VQA）（即使用经典优化器训练参数化量子线路）的一个子类[63]。Zhao 等[61]就现阶段较主流的 QNN 实现方法作了归纳。

在近期量子处理器阶段已经涌现了一批 QNN 模型。Cong 等[64]基于经典 CNN 卷积层与池化层的思想，在量子线路上基于带参的酉门设计了量子版本的卷积层与池化层，提出了量子卷积神经网络（Quantum Convolutional Neural Network，QCNN），并在量子相位识别等问题上证明了该架构的有效性。Li 等[65]设计了省略池化层的 QCNN，并在图像识别问题上展现了其可行性。QCNN 有望在未来更大规模量子计算环境下获得超过经典计算的足够计算增益[64，66]。Lloyd 等[67]讨论了不同场景下量子生成对抗式网络（Quantum Generative Adversarial Network，QGAN）的运用情况，认为当训练数据为高维数据时相对经典对抗生成网络 QGAN 可能具有指数级别的性能优势。Dallaire-Demers 等[68]基于简单的数值实验证明了 QGAN 的可行性，并指出 QGAN 具备比经典版本更广泛的表征能力，例如可应用于学习生成加密数据等。Huggins 等[69]提出了一种量子张量神经网络（Quantum Tensor Neural Network，QTNN），其由带参酉门组成具有固定结构的树状网络，可用于处理判别与生成任务。QTNN 在缓解随机初始参数问题以及抗噪等方面具有显著的应用潜力。Takaki 等[70]构造了由具有递归结构的 VQC 组成的量子循环神经网络（Quantum Recurrent Neural Network，QRNN），同样基于简单的数值模拟证明了其可行性。Chen 等[71]基于混合量子-经典方案设计了量子长短期记忆网络（Quantum Long Short-Term Memory，QLSTM），在相似数量的参数约束下表现出了优于经典 LSTM 的学习能力，同时被认为具备比经典 LSTM 更好地学习局部特征的能力。Zhao 等[61]总结归纳了现阶段较主流的更多 QNN 类型。

Ezhov 和 Ventura[72] 总结讨论了相对于经典神经网络，QNN 所具备的潜在优势。包括相较于经典神经网络 QNN 被认为具有指数级的存储容量、相对较小的网络规模以及更简单的网络拓扑结构、更好的稳定性以及可靠性等。Abbas 等[63]的工作在真实量子硬件上证明了经精心设计的 QNN 较之传统神经网络有着更高的有效维度以及更快的训练能力。此外，基于 VQC 构建的 QNN 有望能获得相对更好的噪声鲁棒性[73]等等。因而 QNN 被认为是量子计算领域最有前途的架构之一，得到了广泛的关注。但尽管 QNN 具有巨大潜力，现阶段的 QNN 研究仍存在许多挑战，例如研究表明随着量子比特数增加，QNN 的损失函数梯度将以指数级增长的概率发生被称为贫瘠高原的梯度消失现象，进而使得大规模的 QNN 变得无法训练[74]。Pesah 等[75]最近的研究表明在合理架构的 QCNN 中可以避免这一问题，为彻底解决贫瘠高原问题带来希望。最后，当前量子技术发展处于被称为含噪声的中等规模量子（Noisy Intermediate-Scale Quantum，NISQ）时代[76]，这一时期量子设备上可用的量子位较少，同时存在显著的噪声问题。这意味着在当前真实物理设备上 QNN 的实现规模受到较大的限制，同时不得不考虑合理的纠错方法，因而在较长一段时间内采用量子设备与经典设备协同工作的混合量子-经典架构方案将成为主要的研究发展方向[77]。