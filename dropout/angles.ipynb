{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96963d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efc40d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33a08936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyqpanda import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01c40c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d48811",
   "metadata": {},
   "source": [
    "# 1. Prepare Dadaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "757bbfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/sumanthvrao/daily-climate-time-series-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91a53ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './../data/DailyDelhiClimateTrain.csv'\n",
    "test_path = './../data/DailyDelhiClimateTest.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef5a3d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [1,2,3,4]\n",
    "\n",
    "train = pd.read_csv(train_path, usecols=cols, engine=\"python\")\n",
    "test = pd.read_csv(test_path, usecols=cols, engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c112f41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train)=1462\n",
      "len(test)=114\n"
     ]
    }
   ],
   "source": [
    "print(f'len(train)={len(train)}')\n",
    "print(f'len(test)={len(test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ca701d",
   "metadata": {},
   "source": [
    "## 1.1 Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b3dc6f",
   "metadata": {},
   "source": [
    "### 1.1.1 outlier detection for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d076311",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1e47842f580>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(len(train)), train[\"meanpressure\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6f7435",
   "metadata": {},
   "source": [
    "### - remove outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb83d8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove outliers num: 9\n"
     ]
    }
   ],
   "source": [
    "unnormal_num = 0\n",
    "for i in range(len(train)):\n",
    "    mp = train.iloc[i][3]\n",
    "    if mp > 1200 or mp < 950:\n",
    "        unnormal_num += 1\n",
    "        train.iloc[i][3] = train.iloc[i + 1][3]\n",
    "print(f'remove outliers num: {unnormal_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c182752f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e4785ed070>,\n",
       " <matplotlib.lines.Line2D at 0x1e4785ed1f0>,\n",
       " <matplotlib.lines.Line2D at 0x1e4785ed2b0>,\n",
       " <matplotlib.lines.Line2D at 0x1e4785ed370>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2a352d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>meantemp</th>\n",
       "      <td>1462.0</td>\n",
       "      <td>25.495521</td>\n",
       "      <td>7.348103</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>18.857143</td>\n",
       "      <td>27.714286</td>\n",
       "      <td>31.305804</td>\n",
       "      <td>38.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>humidity</th>\n",
       "      <td>1462.0</td>\n",
       "      <td>60.771702</td>\n",
       "      <td>16.769652</td>\n",
       "      <td>13.428571</td>\n",
       "      <td>50.375000</td>\n",
       "      <td>62.625000</td>\n",
       "      <td>72.218750</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wind_speed</th>\n",
       "      <td>1462.0</td>\n",
       "      <td>6.802209</td>\n",
       "      <td>4.561602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.475000</td>\n",
       "      <td>6.221667</td>\n",
       "      <td>9.238235</td>\n",
       "      <td>42.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meanpressure</th>\n",
       "      <td>1462.0</td>\n",
       "      <td>1008.247674</td>\n",
       "      <td>7.437992</td>\n",
       "      <td>991.375000</td>\n",
       "      <td>1001.625000</td>\n",
       "      <td>1008.563492</td>\n",
       "      <td>1014.875000</td>\n",
       "      <td>1023.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count         mean        std         min          25%  \\\n",
       "meantemp      1462.0    25.495521   7.348103    6.000000    18.857143   \n",
       "humidity      1462.0    60.771702  16.769652   13.428571    50.375000   \n",
       "wind_speed    1462.0     6.802209   4.561602    0.000000     3.475000   \n",
       "meanpressure  1462.0  1008.247674   7.437992  991.375000  1001.625000   \n",
       "\n",
       "                      50%          75%          max  \n",
       "meantemp        27.714286    31.305804    38.714286  \n",
       "humidity        62.625000    72.218750   100.000000  \n",
       "wind_speed       6.221667     9.238235    42.220000  \n",
       "meanpressure  1008.563492  1014.875000  1023.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52776605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meantemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>meanpressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>84.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1015.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   meantemp  humidity  wind_speed  meanpressure\n",
       "0      10.0      84.5         0.0   1015.666667"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6e536f",
   "metadata": {},
   "source": [
    "### 1.1.2 outlier detection for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fa0149b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1e4784c7c40>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(len(test)), test[\"meanpressure\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b1874",
   "metadata": {},
   "source": [
    "### - remove outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d62ae65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.iloc[0][3] = test.iloc[1][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cf7014f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e478519f40>,\n",
       " <matplotlib.lines.Line2D at 0x1e4785270a0>,\n",
       " <matplotlib.lines.Line2D at 0x1e478527160>,\n",
       " <matplotlib.lines.Line2D at 0x1e478527220>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddd4f67",
   "metadata": {},
   "source": [
    "## 1.2 Transfer data to LSTM representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d1fe2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d28fd514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(data, window_size, predict_size):\n",
    "    scaler = StandardScaler()\n",
    "    data = scaler.fit_transform(np.array(data).reshape(-1, 1))\n",
    "    \n",
    "    data_in = []\n",
    "    data_out = []\n",
    "    \n",
    "    for i in range(data.shape[0] - window_size - predict_size):\n",
    "        data_in.append(data[i:i + window_size].reshape(1, window_size)[0])\n",
    "        data_out.append(data[i + window_size:i + window_size + predict_size].reshape(1, predict_size)[0])\n",
    "        \n",
    "    data_in = np.array(data_in).reshape(-1, window_size)\n",
    "    data_out = np.array(data_out).reshape(-1, predict_size)\n",
    "    \n",
    "    data_process = {'datain': data_in, 'dataout': data_out}\n",
    "    \n",
    "    return data_process, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b642d83",
   "metadata": {},
   "source": [
    "## 1.3 prepare train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1b5ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_size = 4\n",
    "window_size = features_size * 3 # features num * time steps\n",
    "predict_size = features_size # features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da56bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed, train_scaler = data_process(train, window_size, predict_size)\n",
    "X_train, y_train = train_processed['datain'], train_processed['dataout']\n",
    "\n",
    "test_processed, test_scaler = data_process(test, window_size, predict_size)\n",
    "X_test, y_test = test_processed['datain'], test_processed['dataout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89c5ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81da8b10",
   "metadata": {},
   "source": [
    "### - data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "881c39bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "\n",
    "train_data = Data.TensorDataset(X_train, y_train)\n",
    "test_data = Data.TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "000e5717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train_data: 5832\n",
      "size of test_data: 440\n"
     ]
    }
   ],
   "source": [
    "print(f'size of train_data: {len(train_data)}')\n",
    "print(f'size of test_data: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd5abd4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.6262, -0.4504, -0.6498,  1.7473, -0.6323, -0.4327, -0.6428,  1.7523,\n",
       "         -0.6329, -0.4445, -0.6389,  1.7543]),\n",
       " tensor([-0.6293, -0.4814, -0.6469,  1.7508]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f77a830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.6067, -0.4426, -0.6376,  1.7437, -0.6006, -0.4629, -0.6372,  1.7437,\n",
       "         -0.6039, -0.4520, -0.6346,  1.7438]),\n",
       " tensor([-0.6001, -0.4797, -0.6333,  1.7377]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99ad9fa",
   "metadata": {},
   "source": [
    "# 2. Quantum Enhanced LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1c6bb",
   "metadata": {},
   "source": [
    "## 2.1 initiate quantum environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f83f918",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitQMachine:\n",
    "    def __init__(self, qubitsCount, cbitsCount = 0, machineType = QMachineType.CPU):\n",
    "        self.machine = init_quantum_machine(machineType)\n",
    "        \n",
    "        self.qubits = self.machine.qAlloc_many(qubitsCount)\n",
    "        self.cbits = self.machine.cAlloc_many(cbitsCount)\n",
    "        \n",
    "        print(f'Init Quantum Machine with qubits:[{qubitsCount}] / cbits:[{cbitsCount}] Successfully')\n",
    "    \n",
    "    def __del__(self):\n",
    "        destroy_quantum_machine(self.machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b73cfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Quantum Machine with qubits:[5] / cbits:[0] Successfully\n"
     ]
    }
   ],
   "source": [
    "# maximum qubits size\n",
    "ctx = InitQMachine(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fd176a",
   "metadata": {},
   "source": [
    "## 2.2 Quantum Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c3c69",
   "metadata": {},
   "source": [
    "### - Tool Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "467e68dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(prog, filename=''):\n",
    "    dir_path = './images/'\n",
    "    \n",
    "    if filename != '':\n",
    "        draw_qprog(prog, 'pic', filename=f'{dir_path}{filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1db12c",
   "metadata": {},
   "source": [
    "### 2.2.1 Quantum Layer Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9372635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ef35c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayerBase(nn.Module):\n",
    "    def __init__(self, input_size, output_size, *, n_qubits, n_layers = 1, ctx = None):\n",
    "        super(QuantumLayerBase, self).__init__()\n",
    "        \n",
    "        self.data = None # need to input during forward\n",
    "    \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size # hidden size, not n_qubits\n",
    "        \n",
    "        # quantum infos\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.ctx = ctx\n",
    "        self.qubits = ctx.qubits\n",
    "        self.machine = ctx.machine\n",
    "        \n",
    "        # convert quantum input/output to match classical computation\n",
    "        self.qin = nn.Linear(self.input_size, self.n_qubits)\n",
    "        self.qout = nn.Linear(self.n_qubits, self.output_size)\n",
    "        \n",
    "    @property\n",
    "    def circuit(self):\n",
    "        raise NotImplementedError('Should init circuit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21d0b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(self):\n",
    "    HamiZ = [ PauliOperator({f'Z{i}': 1}) for i in range(len(self.qubits)) ]\n",
    "    res = [ eval(qop(self.circuit, Hami, self.machine, self.qubits))[0,0] for Hami in HamiZ ]\n",
    "    \n",
    "    return Parameter(Tensor(res[:self.n_qubits]))\n",
    "\n",
    "QuantumLayerBase.measure = measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8dd09ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, inputs):\n",
    "    y_t = self.qin(Parameter(inputs))\n",
    "    self.data = y_t[0]\n",
    "    \n",
    "    return self.qout(self.measure())\n",
    "\n",
    "QuantumLayerBase.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73293db3",
   "metadata": {},
   "source": [
    "### 2.2.2 Quantum Layer Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfa1811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_lock = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f45cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayer(QuantumLayerBase):\n",
    "    def __init__(self, input_size, output_size, *, n_qubits, degree = 1, n_layers = 1, ctx = None, dropout_rate = 0):\n",
    "        super(QuantumLayer, self).__init__(input_size, output_size, \n",
    "                                         n_qubits = n_qubits, n_layers = n_layers, ctx = ctx)\n",
    "        \n",
    "        self.degree = degree\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.angles = Parameter(torch.rand(n_layers + 1, degree, self.n_qubits))\n",
    "        \n",
    "    @property\n",
    "    def qparameters_size(self):\n",
    "        return self.angles.flatten().size()[0]\n",
    "        \n",
    "    @property\n",
    "    def circuit(self):\n",
    "        if self.data == None:\n",
    "            raise ValueError('Need to feed a input data!')\n",
    "        \n",
    "        n = self.n_qubits\n",
    "        q = self.qubits\n",
    "        x = self.data\n",
    "        p = self.angles\n",
    "        degree = self.degree\n",
    "        \n",
    "        # quantum gates - must use small case!\n",
    "        identity = VariationalQuantumGate_I\n",
    "        h = VariationalQuantumGate_H\n",
    "        ry = VariationalQuantumGate_RY\n",
    "        cz = VariationalQuantumGate_CZ\n",
    "        u = [\n",
    "            None,\n",
    "            VariationalQuantumGate_U1,\n",
    "            VariationalQuantumGate_U2,\n",
    "            VariationalQuantumGate_U3\n",
    "        ]\n",
    "        \n",
    "        # init variational quantum circuit\n",
    "        vqc = VariationalQuantumCircuit()\n",
    "\n",
    "        # in order to use each qubits => when n_qubits < len(ctx.qubits)\n",
    "        [ vqc.insert(identity(q[i])) for i in range(len(q)) ]\n",
    "        \n",
    "        [ vqc.insert( h(q[i]) ) for i in range(n) ]\n",
    "        [ vqc.insert( ry(q[i], var(x[i] * torch.pi / 2)) ) for i in range(n) ]\n",
    "        \n",
    "        for i in range(n):\n",
    "            for d in range(degree):\n",
    "                if dropout_lock and np.random.rand() <= self.dropout_rate:\n",
    "                    p.data[0][d][i] = 0\n",
    "                \n",
    "            vqc.insert( u[degree](q[i], *[ var(p[0][d][i]) for d in range(degree) ]) )\n",
    "        \n",
    "        for layer in range(self.n_layers):\n",
    "            for i in range(n - 1):\n",
    "                vqc.insert(cz(q[i], q[i + 1]))\n",
    "            vqc.insert(cz(q[n - 1], q[0]))\n",
    "            \n",
    "            for i in range(n):\n",
    "                for d in range(degree):\n",
    "                    if dropout_lock and np.random.rand() <= self.dropout_rate:\n",
    "                        p.data[layer + 1][d][i] = 0\n",
    "                vqc.insert( u[degree](q[i], *[ var(p[layer + 1][d][i]) for d in range(degree) ]) ) \n",
    "        \n",
    "        return vqc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216e6888",
   "metadata": {},
   "source": [
    "### 2.2.3 Plot Quantum layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "035d900b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyqpanda.pyQPanda.QProg at 0x1e47642c5b0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Tensor([[0.1, 0.2, 0.3, 0.4, 0.5]])\n",
    "layer = QuantumLayer(5, 5, n_qubits=5, n_layers=1, degree=3, dropout_rate=0.3, ctx=ctx)\n",
    "layer.data = data[0]\n",
    "vqc = layer.circuit\n",
    "prog = create_empty_qprog()\n",
    "prog.insert(vqc.feed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c004a59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'null'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_qprog(prog, 'pic', filename=f'pic/angles_yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d956e7",
   "metadata": {},
   "source": [
    "## 2.3 Quantum-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d437f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLSTMBase(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, *, ctx, dropout_rate=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ctx = ctx\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "    @property\n",
    "    def qparameters_size(self):\n",
    "        num = 0\n",
    "        for attr in dir(self):\n",
    "            if attr.endswith('_circuit'):\n",
    "                num += getattr(self, attr).qparameters_size\n",
    "        return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be3c3cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, inputs, init_states = None):\n",
    "    sequence_size, batch_size, _ = inputs.size()\n",
    "    hidden_sequence = []\n",
    "    \n",
    "    if init_states == None:\n",
    "        h_t, c_t = (\n",
    "            torch.zeros(1, batch_size, self.hidden_size).to(inputs.device),\n",
    "            torch.zeros(1, batch_size, self.hidden_size).to(inputs.device),\n",
    "        )\n",
    "    else:\n",
    "        h_t, c_t = init_states\n",
    "    \n",
    "    return hidden_sequence, (h_t, c_t)\n",
    "\n",
    "QLSTMBase.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce36bcc1",
   "metadata": {},
   "source": [
    "### 2.3.1 classical Quantum-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1539d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLSTM(QLSTMBase):\n",
    "    def __init__(self, input_size, hidden_size, *, ctx, dropout_rate=0):\n",
    "        super().__init__(input_size, hidden_size, ctx = ctx, dropout_rate = dropout_rate)\n",
    "        \n",
    "        # Parameters: angles\n",
    "        #  => Q * (n + 1) * degree\n",
    "        \n",
    "        # gates names:  qubits  layers  degree\n",
    "        # input gate:     5       2       3\n",
    "        # forget gate:    5       2       3\n",
    "        # candidate:      5       2       3\n",
    "        # output gate:    5       2       3\n",
    "        \n",
    "        # input gates\n",
    "        self.input_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                        n_qubits = 5, n_layers = 2, degree = 3, ctx = ctx, dropout_rate = dropout_rate) # 45\n",
    "        # forget gates\n",
    "        self.forget_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                         n_qubits = 5, n_layers = 2, degree = 3, ctx = ctx, dropout_rate = dropout_rate) # 45\n",
    "        # candidate\n",
    "        self.candidate_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                       n_qubits = 5, n_layers = 2, degree = 3, ctx = ctx, dropout_rate = dropout_rate) # 45\n",
    "        # output gates\n",
    "        self.output_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                         n_qubits = 5, n_layers = 2, degree = 3, ctx = ctx, dropout_rate = dropout_rate) # 45\n",
    "        \n",
    "    def forward(self, inputs, init_states = None):\n",
    "        hidden_sequence, (h_t, c_t) = super(QLSTM, self).forward(inputs, init_states)\n",
    "\n",
    "        for t in range(inputs.size()[0]):\n",
    "            x_t = inputs[t, :, :]\n",
    "            v_t = torch.cat((h_t[0], x_t), dim = 1)\n",
    "\n",
    "            # input gates\n",
    "            i_t = torch.sigmoid(self.input_circuit(v_t))\n",
    "            # forget gates\n",
    "            f_t = torch.sigmoid(self.forget_circuit(v_t))\n",
    "            # candidate for cell state update\n",
    "            g_t = torch.tanh(self.candidate_circuit(v_t))\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "\n",
    "            # output gates\n",
    "            o_t = torch.sigmoid(self.output_circuit(v_t))\n",
    "            # update output ht\n",
    "            h_t = o_t * (torch.tanh(c_t))\n",
    "\n",
    "            hidden_sequence.append(h_t)\n",
    "\n",
    "        # reshape hidden_seq p/ retornar\n",
    "        #\n",
    "        # [tensor([[[0.0444, ...]]] => tensor([[[0.0444, ...]]]\n",
    "        # \n",
    "        hidden_sequence = torch.cat(hidden_sequence, dim = 0)\n",
    "\n",
    "        return hidden_sequence, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caa3435",
   "metadata": {},
   "source": [
    "### 2.3.2 adjusted classical QLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d152d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustedQLSTM(QLSTMBase):\n",
    "    def __init__(self, input_size, hidden_size, *, ctx, dropout_rate = 0):\n",
    "        super().__init__(input_size, hidden_size, ctx = ctx, dropout_rate = dropout_rate)\n",
    "        \n",
    "        # gates names:  qubits  layers  degree\n",
    "        # input gate:     4       2       3\n",
    "        # forget gate:    5       2       3\n",
    "        # candidate:      4       1       3\n",
    "        # output gate:    3       2       2\n",
    "        \n",
    "        # input gates\n",
    "        self.input_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                        n_qubits = 4, n_layers = 2, degree = 3, ctx = ctx, dropout_rate = dropout_rate) # 36\n",
    "        # forget gates\n",
    "        self.forget_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                         n_qubits = 5, n_layers = 2, degree = 3, ctx = ctx, dropout_rate = dropout_rate) # 45\n",
    "        # candidate\n",
    "        self.candidate_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                       n_qubits = 4, n_layers = 1, degree = 3, ctx = ctx, dropout_rate = dropout_rate) # 24\n",
    "        # output gates\n",
    "        self.output_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                         n_qubits = 3, n_layers = 2, degree = 2, ctx = ctx, dropout_rate = dropout_rate) # 18\n",
    "        \n",
    "    def forward(self, inputs, init_states = None):\n",
    "        hidden_sequence, (h_t, c_t) = super(AdjustedQLSTM, self).forward(inputs, init_states)\n",
    "\n",
    "        for t in range(inputs.size()[0]):\n",
    "            x_t = inputs[t, :, :]\n",
    "            v_t = torch.cat((h_t[0], x_t), dim = 1)\n",
    "\n",
    "            # input gates\n",
    "            i_t = torch.sigmoid(self.input_circuit(v_t))\n",
    "            # forget gates\n",
    "            f_t = torch.sigmoid(self.forget_circuit(v_t))\n",
    "            # candidate for cell state update\n",
    "            g_t = torch.tanh(self.candidate_circuit(v_t))\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "\n",
    "            # output gates\n",
    "            o_t = torch.sigmoid(self.output_circuit(v_t))\n",
    "            # update output ht\n",
    "            h_t = o_t * (torch.tanh(c_t))\n",
    "\n",
    "            hidden_sequence.append(h_t)\n",
    "\n",
    "        hidden_sequence = torch.cat(hidden_sequence, dim = 0)\n",
    "\n",
    "        return hidden_sequence, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bda938",
   "metadata": {},
   "source": [
    "### 2.3.3 peephole QLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66ed785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeepholeQLSTM(QLSTMBase):\n",
    "    def __init__(self, input_size, hidden_size, *, ctx, dropout_rate = 0):\n",
    "        super().__init__(input_size, hidden_size, ctx = ctx, dropout_rate = dropout_rate)\n",
    "        \n",
    "        # gates names:  qubits  layers  degree\n",
    "        # input gate:     4       2       3\n",
    "        # forget gate:    5       2       3\n",
    "        # candidate:      4       1       3\n",
    "        # output gate:    3       2       2\n",
    "        \n",
    "        # input gates\n",
    "        self.input_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                        n_qubits = 4, n_layers = 2, degree = 3, ctx = ctx, dropout_rate = dropout_rate) # 36\n",
    "        # forget gates\n",
    "        self.forget_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                         n_qubits = 5, n_layers = 2, degree = 3, ctx = ctx, dropout_rate = dropout_rate) # 45\n",
    "        # candidate\n",
    "        self.candidate_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                       n_qubits = 4, n_layers = 1, degree = 3, ctx = ctx, dropout_rate = dropout_rate) # 24\n",
    "        # output gates\n",
    "        self.output_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                         n_qubits = 3, n_layers = 2, degree = 2, ctx = ctx, dropout_rate = dropout_rate) # 18\n",
    "        \n",
    "    def forward(self, inputs, init_states = None):\n",
    "        hidden_sequence, (h_t, c_t) = super(PeepholeQLSTM, self).forward(inputs, init_states)\n",
    "\n",
    "        for t in range(inputs.size()[0]):\n",
    "            x_t = inputs[t, :, :]\n",
    "            v_t = torch.cat((c_t[0], x_t), dim = 1)\n",
    "            \n",
    "            # input gates\n",
    "            i_t = torch.sigmoid(self.input_circuit(v_t))\n",
    "            # forget gates\n",
    "            f_t = torch.sigmoid(self.forget_circuit(v_t))\n",
    "            # candidate for cell state update\n",
    "            g_t = torch.tanh(self.candidate_circuit(v_t))\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "            \n",
    "            # output gates\n",
    "            o_t = torch.sigmoid(self.output_circuit(v_t))\n",
    "            # update output ht\n",
    "            h_t = o_t * (torch.tanh(c_t))\n",
    "\n",
    "            hidden_sequence.append(h_t)\n",
    "\n",
    "        hidden_sequence = torch.cat(hidden_sequence, dim = 0)\n",
    "\n",
    "        return hidden_sequence, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e631aeb",
   "metadata": {},
   "source": [
    "### 2.3.4 Coupled Input and Forget gates QLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f77c5ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFGQLSTM(QLSTMBase):\n",
    "    def __init__(self, input_size, hidden_size, *, ctx, dropout_rate = 0):\n",
    "        super().__init__(input_size, hidden_size, ctx = ctx, dropout_rate = dropout_rate)\n",
    "        \n",
    "        # gates names:  qubits  layers  degree\n",
    "        # coupled IF:     5       2       3\n",
    "        # candidate:      4       2       3\n",
    "        # output gate:    3       2       2\n",
    "        \n",
    "        # Coupled Input and Forget gate\n",
    "        self.coupled_IF_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                         n_qubits = 5, n_layers = 2, degree = 3, ctx = ctx, dropout_rate = dropout_rate) # 30\n",
    "        # candidate for cell state update\n",
    "        self.candidate_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                       n_qubits = 4, n_layers = 2, degree = 3, ctx = ctx, dropout_rate = dropout_rate) # 24\n",
    "        # output gates\n",
    "        self.output_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                         n_qubits = 3, n_layers = 2, degree = 2, ctx = ctx, dropout_rate = dropout_rate) # 12\n",
    "        \n",
    "    def forward(self, inputs, init_states = None):\n",
    "        hidden_sequence, (h_t, c_t) = super(CIFGQLSTM, self).forward(inputs, init_states)\n",
    "\n",
    "        for t in range(inputs.size()[0]):\n",
    "            x_t = inputs[t, :, :]\n",
    "            v_t = torch.cat((h_t[0], x_t), dim = 1)\n",
    "            \n",
    "            # coupled input and forget gate\n",
    "            f_t = torch.sigmoid(self.coupled_IF_circuit(v_t))\n",
    "            # candidate for cell state update\n",
    "            g_t = torch.tanh(self.candidate_circuit(v_t))\n",
    "            c_t = (f_t * c_t) + ((1 - f_t) * g_t)\n",
    "            \n",
    "            # output gates\n",
    "            o_t = torch.sigmoid(self.output_circuit(v_t))\n",
    "            # update output ht\n",
    "            h_t = o_t * (torch.tanh(c_t))\n",
    "\n",
    "            hidden_sequence.append(h_t)\n",
    "\n",
    "        hidden_sequence = torch.cat(hidden_sequence, dim = 0)\n",
    "\n",
    "        return hidden_sequence, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fded5be6",
   "metadata": {},
   "source": [
    "### 2.3.5 Recurrent Gate Units - QGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62d3eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QGRU(QLSTMBase):\n",
    "    def __init__(self, input_size, hidden_size, *, ctx, dropout_rate = 0):\n",
    "        super().__init__(input_size, hidden_size, ctx = ctx, dropout_rate = dropout_rate)\n",
    "        \n",
    "        # gates names:  qubits  layers  degree\n",
    "        # update gate:     5       1       3\n",
    "        # candidate:       4       1       3\n",
    "        # reset gate:      3       1       2\n",
    "        \n",
    "        # update gates\n",
    "        self.update_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                         n_qubits = 5, n_layers = 1, degree = 3, ctx = ctx, dropout_rate = dropout_rate) # 45\n",
    "        # candidate for hidden state update\n",
    "        self.candidate_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                       n_qubits = 4, n_layers = 1, degree = 3, ctx = ctx, dropout_rate = dropout_rate) # 36\n",
    "        # reset gates\n",
    "        self.reset_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                         n_qubits = 3, n_layers = 1, degree = 2, ctx = ctx, dropout_rate = dropout_rate) # 18\n",
    "        \n",
    "    def forward(self, inputs, init_states = None):\n",
    "        hidden_sequence, (h_t, c_t) = super(QGRU, self).forward(inputs, init_states)\n",
    "\n",
    "        for t in range(inputs.size()[0]):\n",
    "            x_t = inputs[t, :, :]\n",
    "            v_t = torch.cat((h_t[0], x_t), dim = 1)\n",
    "            \n",
    "            # update gates\n",
    "            z_t = torch.sigmoid(self.update_circuit(v_t))\n",
    "            # reset gates\n",
    "            r_t = torch.sigmoid(self.reset_circuit(v_t))\n",
    "        \n",
    "            v_hat_t = torch.cat(((r_t * h_t)[0], x_t), dim = 1)\n",
    "            # candidate for hidden state update\n",
    "            g_t = torch.tanh(self.candidate_circuit(v_hat_t))\n",
    "            h_t = (z_t * g_t) + (1 - z_t) * h_t \n",
    "\n",
    "            hidden_sequence.append(h_t)\n",
    "\n",
    "        hidden_sequence = torch.cat(hidden_sequence, dim = 0)\n",
    "\n",
    "        return hidden_sequence, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63514807",
   "metadata": {},
   "source": [
    "### - QLSTMs Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ab09833",
   "metadata": {},
   "outputs": [],
   "source": [
    "QLSTMMap = {\n",
    "    'classical': ('QLSTM', QLSTM),\n",
    "    'adjusted': ('QLSTM(adjusted)', AdjustedQLSTM),\n",
    "    'peephole': ('peephole QLSTM', PeepholeQLSTM),\n",
    "    'CIFG': ('CIFG-QLSTM', CIFGQLSTM),\n",
    "    'GRU': ('QGRU', QGRU)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6497332",
   "metadata": {},
   "source": [
    "## 2.4 Stacked QLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c6cb0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class StackedQLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, *, num_layers = 1, ctx = None, mode = 'classical', dropout_rate = 0):\n",
    "        super().__init__()\n",
    "        \n",
    "        label, qlstm = QLSTMMap.get(mode)\n",
    "        self.qlstms = nn.Sequential(OrderedDict([\n",
    "            (f'{label} {i + 1}', qlstm(input_size if i == 0 else hidden_size , hidden_size, ctx = ctx, dropout_rate = dropout_rate)) \n",
    "                for i in range(num_layers)\n",
    "        ]))\n",
    "\n",
    "    def forward(self, inputs, parameters = None):\n",
    "        outputs = None\n",
    "        \n",
    "        for i, qlstm in enumerate(self.qlstms):\n",
    "            if i != 0:\n",
    "                inputs = outputs\n",
    "            \n",
    "            outputs, parameters = qlstm(inputs, parameters)\n",
    "        \n",
    "        return outputs, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dbbb2f",
   "metadata": {},
   "source": [
    "# 3. Quantum Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8313baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_output, *, num_layers = 1, ctx = None, mode = 'classical', dropout_rate = 0):\n",
    "        super(QModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.qlstm = StackedQLSTM(input_size, hidden_size, \n",
    "                                  num_layers = num_layers, ctx = ctx, mode = mode, dropout_rate = dropout_rate)\n",
    "        self.predict = nn.Linear(hidden_size, num_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        # sequence lenth , batch_size, features length\n",
    "        # \n",
    "        h0 = torch.zeros(1, x.size(1), self.hidden_size)\n",
    "        c0 = torch.zeros(1, x.size(1), self.hidden_size)\n",
    "        \n",
    "        out, _ = self.qlstm(x, (h0, c0))\n",
    "        out = self.predict(out[0])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf519363",
   "metadata": {},
   "source": [
    "## 3.1 train QModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1abf2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "def train_model(model, datas, batch_size, *, loss_func, optimizer, epoch = 50, early_stop = False):\n",
    "    losses = []\n",
    "    sampler = RandomSampler(datas, num_samples = batch_size)\n",
    "    \n",
    "    last_loss = 0.0\n",
    "    for step in range(epoch):\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for index in sampler:\n",
    "            batch_x, batch_y = datas[index][0], datas[index][1]\n",
    "            b_x = batch_x.unsqueeze(0)\n",
    "            b_y = batch_y.unsqueeze(0)\n",
    "            \n",
    "            output = model(b_x)\n",
    "\n",
    "            loss = loss_func(output, b_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        loss = train_loss / batch_size\n",
    "        if early_stop and abs(loss - train_loss) < 1e-4:\n",
    "            break\n",
    "            \n",
    "        last_loss = loss\n",
    "        \n",
    "        print(f'Epoch {step + 1}/{epoch}: Loss: {loss}')\n",
    "        losses.append(loss)\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cdd42a",
   "metadata": {},
   "source": [
    "## 3.2 Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d05e6234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "def MAE_naive(actuals, predicteds):\n",
    "    n = len(actuals)\n",
    "    err = 0.0\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        err += np.abs(actuals[i] - actuals[i - 1])\n",
    "    return err / (n - 1)\n",
    "\n",
    "def calculate_accuarcy(model, X_test, y_test, scaler=test_scaler):\n",
    "    n = len(X_test)\n",
    "    \n",
    "    actuals = []\n",
    "    predicteds = []\n",
    "    \n",
    "    for i in range(0, n, predict_size):\n",
    "        actual = scaler.inverse_transform(y_test[i:i+1].data)\n",
    "        actuals.append(np.array(actual[0]))\n",
    "        predicted = scaler.inverse_transform(model(X_test[i:i+1]).data)\n",
    "        predicteds.append(np.array(predicted[0]))\n",
    "    \n",
    "    actuals = np.array(actuals)\n",
    "    predicteds = np.array(predicteds)\n",
    "    \n",
    "    mae = mean_absolute_error(actuals, predicteds)\n",
    "    mase = mae / MAE_naive(actuals.flatten(), predicteds.flatten())\n",
    "    mape = mean_absolute_percentage_error(actuals, predicteds)\n",
    "    mse = mean_squared_error(actuals, predicteds)\n",
    "    rmse = mse ** 0.5\n",
    "    \n",
    "    return np.array([(1 - mase) * 100, rmse, mse, mae, mape])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec9e103",
   "metadata": {},
   "source": [
    "## 3.3 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "38531f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_size = 4\n",
    "window_size = features_size * 3 # \n",
    "predict_size = features_size # features\n",
    "\n",
    "input_size = window_size\n",
    "num_output = predict_size\n",
    "\n",
    "hidden_size = 32\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ead111aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate:\n",
    "# classical: 0.0035\n",
    "# adjusted: 0.0028\n",
    "# peephole: 0.0027\n",
    "# CIFG: 0.004\n",
    "# GRU: 0.0035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e039b890",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "count: 1\n",
      "selected hidden_size:  32\n",
      "selected num_layers:  2\n",
      "selected dropout_rate:  0.0001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [52], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(qmodel\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0028\u001b[39m)\n\u001b[0;32m     15\u001b[0m loss_func \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m---> 17\u001b[0m losses \u001b[38;5;241m=\u001b[39m train_model(qmodel, train_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,          \n\u001b[0;32m     18\u001b[0m                loss_func \u001b[38;5;241m=\u001b[39m loss_func, optimizer \u001b[38;5;241m=\u001b[39m optimizer, epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     19\u001b[0m dropout_lock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     20\u001b[0m score \u001b[38;5;241m=\u001b[39m calculate_accuarcy(qmodel, X_test, y_test)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[1;32mIn [48], line 16\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, datas, batch_size, loss_func, optimizer, epoch, early_stop)\u001b[0m\n\u001b[0;32m     13\u001b[0m b_x \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     14\u001b[0m b_y \u001b[38;5;241m=\u001b[39m batch_y\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(output, b_y)\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [47], line 21\u001b[0m, in \u001b[0;36mQModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[0;32m     19\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[1;32m---> 21\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(out[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [46], line 20\u001b[0m, in \u001b[0;36mStackedQLSTM.forward\u001b[1;34m(self, inputs, parameters)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     18\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m outputs\n\u001b[1;32m---> 20\u001b[0m     outputs, parameters \u001b[38;5;241m=\u001b[39m \u001b[43mqlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs, parameters\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [41], line 40\u001b[0m, in \u001b[0;36mAdjustedQLSTM.forward\u001b[1;34m(self, inputs, init_states)\u001b[0m\n\u001b[0;32m     37\u001b[0m c_t \u001b[38;5;241m=\u001b[39m (f_t \u001b[38;5;241m*\u001b[39m c_t) \u001b[38;5;241m+\u001b[39m (i_t \u001b[38;5;241m*\u001b[39m g_t)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# output gates\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m o_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_circuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv_t\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# update output ht\u001b[39;00m\n\u001b[0;32m     42\u001b[0m h_t \u001b[38;5;241m=\u001b[39m o_t \u001b[38;5;241m*\u001b[39m (torch\u001b[38;5;241m.\u001b[39mtanh(c_t))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [33], line 5\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      2\u001b[0m y_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqin(Parameter(inputs))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m y_t[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn [32], line 3\u001b[0m, in \u001b[0;36mmeasure\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeasure\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      2\u001b[0m     HamiZ \u001b[38;5;241m=\u001b[39m [ PauliOperator({\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m}) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqubits)) ]\n\u001b[1;32m----> 3\u001b[0m     res \u001b[38;5;241m=\u001b[39m [ \u001b[38;5;28meval\u001b[39m(qop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcircuit, Hami, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmachine, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqubits))[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m Hami \u001b[38;5;129;01min\u001b[39;00m HamiZ ]\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Parameter(Tensor(res[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_qubits]))\n",
      "Cell \u001b[1;32mIn [32], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeasure\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      2\u001b[0m     HamiZ \u001b[38;5;241m=\u001b[39m [ PauliOperator({\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m}) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqubits)) ]\n\u001b[1;32m----> 3\u001b[0m     res \u001b[38;5;241m=\u001b[39m [ \u001b[38;5;28meval\u001b[39m(qop(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcircuit\u001b[49m, Hami, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmachine, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqubits))[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m Hami \u001b[38;5;129;01min\u001b[39;00m HamiZ ]\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Parameter(Tensor(res[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_qubits]))\n",
      "Cell \u001b[1;32mIn [35], line 44\u001b[0m, in \u001b[0;36mQuantumLayer.circuit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m [ vqc\u001b[38;5;241m.\u001b[39minsert(identity(q[i])) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(q)) ]\n\u001b[0;32m     43\u001b[0m [ vqc\u001b[38;5;241m.\u001b[39minsert( h(q[i]) ) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n) ]\n\u001b[1;32m---> 44\u001b[0m [ vqc\u001b[38;5;241m.\u001b[39minsert( ry(q[i], var(x[i] \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mpi \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)) ) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n) ]\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(degree):\n",
      "Cell \u001b[1;32mIn [35], line 44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     41\u001b[0m [ vqc\u001b[38;5;241m.\u001b[39minsert(identity(q[i])) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(q)) ]\n\u001b[0;32m     43\u001b[0m [ vqc\u001b[38;5;241m.\u001b[39minsert( h(q[i]) ) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n) ]\n\u001b[1;32m---> 44\u001b[0m [ vqc\u001b[38;5;241m.\u001b[39minsert( ry(q[i], var(\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)) ) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n) ]\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(degree):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "best_model = None\n",
    "count = 1\n",
    "for dropout_rate in [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.001, 0.002, 0.003, 0.005, 0.01, 0.02, 0.03, 0.05]:\n",
    "    print('-' * 20)\n",
    "    print('count:', count)\n",
    "    print('selected hidden_size: ', hidden_size)\n",
    "    print('selected num_layers: ', num_layers)\n",
    "    print('selected dropout_rate: ', dropout_rate)\n",
    "    count += 1\n",
    "    dropout_lock = True\n",
    "    qmodel = QModel(input_size, hidden_size, num_output, \n",
    "        num_layers = num_layers, ctx = ctx, mode='adjusted', dropout_rate=dropout_rate)\n",
    "    optimizer = torch.optim.AdamW(qmodel.parameters(), lr = 0.0028)\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    losses = train_model(qmodel, train_data, batch_size=20,          \n",
    "                   loss_func = loss_func, optimizer = optimizer, epoch = 100)\n",
    "    dropout_lock = False\n",
    "    score = calculate_accuarcy(qmodel, X_test, y_test).numpy()\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_model = qmodel\n",
    "        best_parameters = {\n",
    "            'hidden_size': hidden_size,\n",
    "            'num_layers': num_layers,\n",
    "            'dropout_rate': dropout_rate\n",
    "        }\n",
    "    print()\n",
    "\n",
    "print(\"Best score: \", best_score)\n",
    "print(\"Best parameters: \", best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b96a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "018bd0f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Epoch 1/200: Loss: 1.0326350241899491\n",
      "Epoch 2/200: Loss: 0.9297964811325073\n",
      "Epoch 3/200: Loss: 0.9325986504554749\n",
      "Epoch 4/200: Loss: 0.8861274987459182\n",
      "Epoch 5/200: Loss: 0.8775552332401275\n",
      "Epoch 6/200: Loss: 0.7984736770391464\n",
      "Epoch 7/200: Loss: 0.7236734837293625\n",
      "Epoch 8/200: Loss: 0.632143621519208\n",
      "Epoch 9/200: Loss: 0.696925880946219\n",
      "Epoch 10/200: Loss: 0.6033962685614824\n",
      "Epoch 11/200: Loss: 0.6983004646375776\n",
      "Epoch 12/200: Loss: 0.5744885351508856\n",
      "Epoch 13/200: Loss: 0.6393524065613747\n",
      "Epoch 14/200: Loss: 0.5464672682806849\n",
      "Epoch 15/200: Loss: 0.6724738266319037\n",
      "Epoch 16/200: Loss: 0.5416232362389565\n",
      "Epoch 17/200: Loss: 0.37759777829051017\n",
      "Epoch 18/200: Loss: 0.313361591566354\n",
      "Epoch 19/200: Loss: 0.3481754770502448\n",
      "Epoch 20/200: Loss: 0.23702495326288045\n",
      "Epoch 21/200: Loss: 0.16408379236236215\n",
      "Epoch 22/200: Loss: 0.11411468060687184\n",
      "Epoch 23/200: Loss: 0.074851279030554\n",
      "Epoch 24/200: Loss: 0.03545596579351695\n",
      "Epoch 25/200: Loss: 0.03816857549245469\n",
      "Epoch 26/200: Loss: 0.021506312406563664\n",
      "Epoch 27/200: Loss: 0.017939387063961476\n",
      "Epoch 28/200: Loss: 0.003885537229507463\n",
      "Epoch 29/200: Loss: 0.0029813756089424716\n",
      "Epoch 30/200: Loss: 0.0019030170711630489\n",
      "Epoch 31/200: Loss: 0.00125760178125347\n",
      "Epoch 32/200: Loss: 0.0021416760107968004\n",
      "Epoch 33/200: Loss: 0.0020333582928287798\n",
      "Epoch 34/200: Loss: 0.000608798331086291\n",
      "Epoch 35/200: Loss: 0.0004409254717756994\n",
      "Epoch 36/200: Loss: 0.00038279916602732553\n",
      "Epoch 37/200: Loss: 0.001320978173498588\n",
      "Epoch 38/200: Loss: 0.0015244259077007883\n",
      "Epoch 39/200: Loss: 0.000540318004641449\n",
      "Epoch 40/200: Loss: 0.0003367431712831603\n",
      "Epoch 41/200: Loss: 0.0009183970806589059\n",
      "Epoch 42/200: Loss: 0.0007119921370758675\n",
      "Epoch 43/200: Loss: 0.0004946523546095705\n",
      "Epoch 44/200: Loss: 0.0006214056942553725\n",
      "Epoch 45/200: Loss: 0.0005237254787061829\n",
      "Epoch 46/200: Loss: 0.0006725666651618667\n",
      "Epoch 47/200: Loss: 0.00048320776004402434\n",
      "Epoch 48/200: Loss: 0.000623827589697612\n",
      "Epoch 49/200: Loss: 0.00047314413895946925\n",
      "Epoch 50/200: Loss: 0.0005391444968154246\n",
      "Epoch 51/200: Loss: 0.0004936328197800322\n",
      "Epoch 52/200: Loss: 0.0004961712642398197\n",
      "Epoch 53/200: Loss: 0.0004086336921318434\n",
      "Epoch 54/200: Loss: 0.00047640331695220084\n",
      "Epoch 55/200: Loss: 0.000661848391428066\n",
      "Epoch 56/200: Loss: 0.0006378356569257448\n",
      "Epoch 57/200: Loss: 0.0005663334326527547\n",
      "Epoch 58/200: Loss: 0.0005072323401691392\n",
      "Epoch 59/200: Loss: 0.000537020891897555\n",
      "Epoch 60/200: Loss: 0.0005217704130700441\n",
      "Epoch 61/200: Loss: 0.0003449918478509062\n",
      "Epoch 62/200: Loss: 0.0008818400870950427\n",
      "Epoch 63/200: Loss: 0.0006906338784574473\n",
      "Epoch 64/200: Loss: 0.0006016853022629221\n",
      "Epoch 65/200: Loss: 0.0005522208966795006\n",
      "Epoch 66/200: Loss: 0.0005337720567695214\n",
      "Epoch 67/200: Loss: 0.0005759446119554922\n",
      "Epoch 68/200: Loss: 0.0005391614191466943\n",
      "Epoch 69/200: Loss: 0.0005239863987299032\n",
      "Epoch 70/200: Loss: 0.0007398773752356647\n",
      "Epoch 71/200: Loss: 0.0008480497839400414\n",
      "Epoch 72/200: Loss: 0.0007039196229015942\n",
      "Epoch 73/200: Loss: 0.0008641457028716104\n",
      "Epoch 74/200: Loss: 0.0007266031185281463\n",
      "Epoch 75/200: Loss: 0.0005436173594830507\n",
      "Epoch 76/200: Loss: 0.0005899898043935537\n",
      "Epoch 77/200: Loss: 0.0006554959616551059\n",
      "Epoch 78/200: Loss: 0.0006447650986956433\n",
      "Epoch 79/200: Loss: 0.0003706408426296548\n",
      "Epoch 80/200: Loss: 0.0008423158153163968\n",
      "Epoch 81/200: Loss: 0.000626620610091777\n",
      "Epoch 82/200: Loss: 0.0007200679459856474\n",
      "Epoch 83/200: Loss: 0.00045047262374282583\n",
      "Epoch 84/200: Loss: 0.0005182153816349455\n",
      "Epoch 85/200: Loss: 0.0008620444365078584\n",
      "Epoch 86/200: Loss: 0.0005728672987061145\n",
      "Epoch 87/200: Loss: 0.000544074102799641\n",
      "Epoch 88/200: Loss: 0.0005611186188616557\n",
      "Epoch 89/200: Loss: 0.0005468795658089221\n",
      "Epoch 90/200: Loss: 0.0013378058798480196\n",
      "Epoch 91/200: Loss: 0.000900915545571479\n",
      "Epoch 92/200: Loss: 0.000516166146917385\n",
      "Epoch 93/200: Loss: 0.00029496723027477856\n",
      "Epoch 94/200: Loss: 0.0006596716588319395\n",
      "Epoch 95/200: Loss: 0.0006706923247293162\n",
      "Epoch 96/200: Loss: 0.0007910408578027273\n",
      "Epoch 97/200: Loss: 0.0004860446108068572\n",
      "Epoch 98/200: Loss: 0.0006856798263470409\n",
      "Epoch 99/200: Loss: 0.0005775274883490055\n",
      "Epoch 100/200: Loss: 0.0006340641844872153\n",
      "Epoch 101/200: Loss: 0.00057018216321012\n",
      "Epoch 102/200: Loss: 0.0005132661932293559\n",
      "Epoch 103/200: Loss: 0.001025662067149824\n",
      "Epoch 104/200: Loss: 0.0008080188750682282\n",
      "Epoch 105/200: Loss: 0.0008594850864028558\n",
      "Epoch 106/200: Loss: 0.000682714044523891\n",
      "Epoch 107/200: Loss: 0.0009884395403787494\n",
      "Epoch 108/200: Loss: 0.0004862438192958507\n",
      "Epoch 109/200: Loss: 0.0005108180981551414\n",
      "Epoch 110/200: Loss: 0.0006756806161320128\n",
      "Epoch 111/200: Loss: 0.0006507244259410072\n",
      "Epoch 112/200: Loss: 0.0006234711368961144\n",
      "Epoch 113/200: Loss: 0.0004965849158907077\n",
      "Epoch 114/200: Loss: 0.0003651405657819851\n",
      "Epoch 115/200: Loss: 0.0008277882508991752\n",
      "Epoch 116/200: Loss: 0.0009959032569895499\n",
      "Epoch 117/200: Loss: 0.0007884759237640537\n",
      "Epoch 118/200: Loss: 0.0003481598765574745\n",
      "Epoch 119/200: Loss: 0.0005866690378752537\n",
      "Epoch 120/200: Loss: 0.0010676266756490804\n",
      "Epoch 121/200: Loss: 0.0006890271353768185\n",
      "Epoch 122/200: Loss: 0.0007353541639531613\n",
      "Epoch 123/200: Loss: 0.0012675337340624538\n",
      "Epoch 124/200: Loss: 0.0005373413776396773\n",
      "Epoch 125/200: Loss: 0.0009444124332731007\n",
      "Epoch 126/200: Loss: 0.0006131795031251386\n",
      "Epoch 127/200: Loss: 0.0006150022096335306\n",
      "Epoch 128/200: Loss: 0.0006440238026698353\n",
      "Epoch 129/200: Loss: 0.0005927512501330056\n",
      "Epoch 130/200: Loss: 0.0006360269186686196\n",
      "Epoch 131/200: Loss: 0.0006278097604081268\n",
      "Epoch 132/200: Loss: 0.0007939834748867724\n",
      "Epoch 133/200: Loss: 0.0008477812165438081\n",
      "Epoch 134/200: Loss: 0.0005340808398614172\n",
      "Epoch 135/200: Loss: 0.00040311062148248313\n",
      "Epoch 136/200: Loss: 0.00024039019638166792\n",
      "Epoch 137/200: Loss: 0.0008384407963603735\n",
      "Epoch 138/200: Loss: 0.0005011642111639958\n",
      "Epoch 139/200: Loss: 0.0006084598451707279\n",
      "Epoch 140/200: Loss: 0.0004542902643152047\n",
      "Epoch 141/200: Loss: 0.0007648818575034966\n",
      "Epoch 142/200: Loss: 0.0008963122294517234\n",
      "Epoch 143/200: Loss: 0.0008073184155364288\n",
      "Epoch 144/200: Loss: 0.0004557813146675471\n",
      "Epoch 145/200: Loss: 0.0005154711741852224\n",
      "Epoch 146/200: Loss: 0.0008570578453600319\n",
      "Epoch 147/200: Loss: 0.0007786298876453657\n",
      "Epoch 148/200: Loss: 0.0006801609581998491\n",
      "Epoch 149/200: Loss: 0.0010641790759109426\n",
      "Epoch 150/200: Loss: 0.0007640536439794232\n",
      "Epoch 151/200: Loss: 0.0009621736640838208\n",
      "Epoch 152/200: Loss: 0.0008103544891127967\n",
      "Epoch 153/200: Loss: 0.0005644132754241582\n",
      "Epoch 154/200: Loss: 0.0007054767658701167\n",
      "Epoch 155/200: Loss: 0.000761981151663349\n",
      "Epoch 156/200: Loss: 0.0008553050138289109\n",
      "Epoch 157/200: Loss: 0.001026382350391941\n",
      "Epoch 158/200: Loss: 0.0006707997246849118\n",
      "Epoch 159/200: Loss: 0.0007420502789500461\n",
      "Epoch 160/200: Loss: 0.0008169955966877751\n",
      "Epoch 161/200: Loss: 0.0008035736806050409\n",
      "Epoch 162/200: Loss: 0.0008687326004292118\n",
      "Epoch 163/200: Loss: 0.0006169417061755667\n",
      "Epoch 164/200: Loss: 0.0005950781658611958\n",
      "Epoch 165/200: Loss: 0.0004927210608002497\n",
      "Epoch 166/200: Loss: 0.0009942599757778226\n",
      "Epoch 167/200: Loss: 0.0005895495369259152\n",
      "Epoch 168/200: Loss: 0.0007475019825506024\n",
      "Epoch 169/200: Loss: 0.0004984708477422828\n",
      "Epoch 170/200: Loss: 0.0006953506637728424\n",
      "Epoch 171/200: Loss: 0.0005844046721904306\n",
      "Epoch 172/200: Loss: 0.0007559510271676117\n",
      "Epoch 173/200: Loss: 0.0007781455053191167\n",
      "Epoch 174/200: Loss: 0.0008539994310012844\n",
      "Epoch 175/200: Loss: 0.0005143768124980852\n",
      "Epoch 176/200: Loss: 0.0006281491261688644\n",
      "Epoch 177/200: Loss: 0.0006610720694880002\n",
      "Epoch 178/200: Loss: 0.0004898683399005676\n",
      "Epoch 179/200: Loss: 0.0005497411151253572\n",
      "Epoch 180/200: Loss: 0.000669974836637266\n",
      "Epoch 181/200: Loss: 0.000851449063338805\n",
      "Epoch 182/200: Loss: 0.0008325617942318786\n",
      "Epoch 183/200: Loss: 0.0009271362339859479\n",
      "Epoch 184/200: Loss: 0.0007998489931196673\n",
      "Epoch 185/200: Loss: 0.0005926589958107798\n",
      "Epoch 186/200: Loss: 0.0008583907365391497\n",
      "Epoch 187/200: Loss: 0.0008451222362054978\n",
      "Epoch 188/200: Loss: 0.0016697428527550073\n",
      "Epoch 189/200: Loss: 0.0012564291071612387\n",
      "Epoch 190/200: Loss: 0.0011482927788165399\n",
      "Epoch 191/200: Loss: 0.0011354702790413284\n",
      "Epoch 192/200: Loss: 0.0013687115111679304\n",
      "Epoch 193/200: Loss: 0.0011813438723038416\n",
      "Epoch 194/200: Loss: 0.0007814498741936405\n",
      "Epoch 195/200: Loss: 0.0007614379213919164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196/200: Loss: 0.0006666752757155337\n",
      "Epoch 197/200: Loss: 0.0007493815319321584\n",
      "Epoch 198/200: Loss: 0.0005634242435917259\n",
      "Epoch 199/200: Loss: 0.0008385791450564283\n",
      "Epoch 200/200: Loss: 0.0009481450069870334\n",
      "time costs: 1499.4080107212067\n",
      "accuarcy: 98.7998625187171\n",
      "epoch: 2\n",
      "Epoch 1/200: Loss: 1.0450002700090408\n",
      "Epoch 2/200: Loss: 0.9811436325311661\n",
      "Epoch 3/200: Loss: 0.9808244377374649\n",
      "Epoch 4/200: Loss: 0.9157161056995392\n",
      "Epoch 5/200: Loss: 0.8757891774177551\n",
      "Epoch 6/200: Loss: 0.7156798079609871\n",
      "Epoch 7/200: Loss: 0.8926077976822853\n",
      "Epoch 8/200: Loss: 0.6250160411000252\n",
      "Epoch 9/200: Loss: 0.5245318464934826\n",
      "Epoch 10/200: Loss: 0.4831108331680298\n",
      "Epoch 11/200: Loss: 0.417983490973711\n",
      "Epoch 12/200: Loss: 0.26162835136055945\n",
      "Epoch 13/200: Loss: 0.23613086789846421\n",
      "Epoch 14/200: Loss: 0.13140984158962965\n",
      "Epoch 15/200: Loss: 0.07427292689681053\n",
      "Epoch 16/200: Loss: 0.0415279105247464\n",
      "Epoch 17/200: Loss: 0.015333876758813859\n",
      "Epoch 18/200: Loss: 0.0042595578968757765\n",
      "Epoch 19/200: Loss: 0.01646303463494405\n",
      "Epoch 20/200: Loss: 0.010816034878371284\n",
      "Epoch 21/200: Loss: 0.0014598355177440681\n",
      "Epoch 22/200: Loss: 0.0010557140056334901\n",
      "Epoch 23/200: Loss: 0.0008956078865594464\n",
      "Epoch 24/200: Loss: 0.0016177020865143278\n",
      "Epoch 25/200: Loss: 0.0006276869139583141\n",
      "Epoch 26/200: Loss: 0.0015461086373761645\n",
      "Epoch 27/200: Loss: 0.0014639715831435752\n",
      "Epoch 28/200: Loss: 0.0004950903092321823\n",
      "Epoch 29/200: Loss: 0.0011282935172857833\n",
      "Epoch 30/200: Loss: 0.0007948075119202258\n",
      "Epoch 31/200: Loss: 0.0005029168689361541\n",
      "Epoch 32/200: Loss: 0.000353912806531298\n",
      "Epoch 33/200: Loss: 0.0004308970133934054\n",
      "Epoch 34/200: Loss: 0.00040440128668706166\n",
      "Epoch 35/200: Loss: 0.0003459993731667055\n",
      "Epoch 36/200: Loss: 0.00041020651242433813\n",
      "Epoch 37/200: Loss: 0.0005355582870834041\n",
      "Epoch 38/200: Loss: 0.0005028612426258405\n",
      "Epoch 39/200: Loss: 0.00037914819813522627\n",
      "Epoch 40/200: Loss: 0.00043857228474735165\n",
      "Epoch 41/200: Loss: 0.0003902748205291573\n",
      "Epoch 42/200: Loss: 0.0004090177350008162\n",
      "Epoch 43/200: Loss: 0.0004472245043871226\n",
      "Epoch 44/200: Loss: 0.000485614937133505\n",
      "Epoch 45/200: Loss: 0.0006822416147770127\n",
      "Epoch 46/200: Loss: 0.00041565704414097125\n",
      "Epoch 47/200: Loss: 0.00026180134645983346\n",
      "Epoch 48/200: Loss: 0.0004085152850166196\n",
      "Epoch 49/200: Loss: 0.00041254526504417297\n",
      "Epoch 50/200: Loss: 0.0005232207829976687\n",
      "Epoch 51/200: Loss: 0.0004864458984229714\n",
      "Epoch 52/200: Loss: 0.0003323944405565271\n",
      "Epoch 53/200: Loss: 0.0008168157348336536\n",
      "Epoch 54/200: Loss: 0.00025203927179973107\n",
      "Epoch 55/200: Loss: 0.0005007535648474005\n",
      "Epoch 56/200: Loss: 0.0004810521249510202\n",
      "Epoch 57/200: Loss: 0.0004069255002832506\n",
      "Epoch 58/200: Loss: 0.0007233544225528022\n",
      "Epoch 59/200: Loss: 0.00029106777819833953\n",
      "Epoch 60/200: Loss: 0.0002999486583576072\n",
      "Epoch 61/200: Loss: 0.0004330669651608332\n",
      "Epoch 62/200: Loss: 0.0007159652433983866\n",
      "Epoch 63/200: Loss: 0.00043890795277548025\n",
      "Epoch 64/200: Loss: 0.0003048511340239202\n",
      "Epoch 65/200: Loss: 0.0004836168678593822\n",
      "Epoch 66/200: Loss: 0.0008576086758694146\n",
      "Epoch 67/200: Loss: 0.0008190672278942657\n",
      "Epoch 68/200: Loss: 0.0006087355130148353\n",
      "Epoch 69/200: Loss: 0.0006631913063756655\n",
      "Epoch 70/200: Loss: 0.0004447437562703271\n",
      "Epoch 71/200: Loss: 0.00040731828867137667\n",
      "Epoch 72/200: Loss: 0.0003465070672973525\n",
      "Epoch 73/200: Loss: 0.0003209672486264026\n",
      "Epoch 74/200: Loss: 0.0004792000310771982\n",
      "Epoch 75/200: Loss: 0.0006685944324999582\n",
      "Epoch 76/200: Loss: 0.0004440521298420208\n",
      "Epoch 77/200: Loss: 0.00042795334779839324\n",
      "Epoch 78/200: Loss: 0.0005524359434275538\n",
      "Epoch 79/200: Loss: 0.0005220409593675868\n",
      "Epoch 80/200: Loss: 0.00034728351347439456\n",
      "Epoch 81/200: Loss: 0.0004634456303392653\n",
      "Epoch 82/200: Loss: 0.0005532118942937813\n",
      "Epoch 83/200: Loss: 0.0004637319629182457\n",
      "Epoch 84/200: Loss: 0.0008225113160733599\n",
      "Epoch 85/200: Loss: 0.00045626519686265966\n",
      "Epoch 86/200: Loss: 0.00047091145911508646\n",
      "Epoch 87/200: Loss: 0.000529154912874219\n",
      "Epoch 88/200: Loss: 0.0006395676278771134\n",
      "Epoch 89/200: Loss: 0.0004537233333394397\n",
      "Epoch 90/200: Loss: 0.0006767120776203228\n",
      "Epoch 91/200: Loss: 0.0005144069262314587\n",
      "Epoch 92/200: Loss: 0.00039989644592424155\n",
      "Epoch 93/200: Loss: 0.0002913003791036317\n",
      "Epoch 94/200: Loss: 0.0005275278832414188\n",
      "Epoch 95/200: Loss: 0.0004619982333679218\n",
      "Epoch 96/200: Loss: 0.0007202476685051806\n",
      "Epoch 97/200: Loss: 0.00044468549403973154\n",
      "Epoch 98/200: Loss: 0.0004940772669215221\n",
      "Epoch 99/200: Loss: 0.00043220184011261154\n",
      "Epoch 100/200: Loss: 0.0004215085289615672\n",
      "Epoch 101/200: Loss: 0.0006117899472883436\n",
      "Epoch 102/200: Loss: 0.0005083360262005954\n",
      "Epoch 103/200: Loss: 0.000834852215484716\n",
      "Epoch 104/200: Loss: 0.0005432977587133791\n",
      "Epoch 105/200: Loss: 0.0005871067851785483\n",
      "Epoch 106/200: Loss: 0.0006940773182577686\n",
      "Epoch 107/200: Loss: 0.00048294006192008967\n",
      "Epoch 108/200: Loss: 0.0007295559356862213\n",
      "Epoch 109/200: Loss: 0.0005382989079407707\n",
      "Epoch 110/200: Loss: 0.0004169049519987311\n",
      "Epoch 111/200: Loss: 0.0003992952606495237\n",
      "Epoch 112/200: Loss: 0.0004540100831491145\n",
      "Epoch 113/200: Loss: 0.0004645553178306727\n",
      "Epoch 114/200: Loss: 0.0009007916368318547\n",
      "Epoch 115/200: Loss: 0.0006457616382249398\n",
      "Epoch 116/200: Loss: 0.0005884743199203512\n",
      "Epoch 117/200: Loss: 0.0007259966725541745\n",
      "Epoch 118/200: Loss: 0.0005275956416880944\n",
      "Epoch 119/200: Loss: 0.0003746730973944068\n",
      "Epoch 120/200: Loss: 0.0007839367295673583\n",
      "Epoch 121/200: Loss: 0.00031596668268321084\n",
      "Epoch 122/200: Loss: 0.0003757118534849724\n",
      "Epoch 123/200: Loss: 0.0009490632397501031\n",
      "Epoch 124/200: Loss: 0.0006883170692162822\n",
      "Epoch 125/200: Loss: 0.0005018715355618042\n",
      "Epoch 126/200: Loss: 0.0005479097337229178\n",
      "Epoch 127/200: Loss: 0.0006699192450469127\n",
      "Epoch 128/200: Loss: 0.0005317803275829647\n",
      "Epoch 129/200: Loss: 0.0004805660657439148\n",
      "Epoch 130/200: Loss: 0.0005803087560707354\n",
      "Epoch 131/200: Loss: 0.0007876181683968752\n",
      "Epoch 132/200: Loss: 0.0007549968359853665\n",
      "Epoch 133/200: Loss: 0.0006723509570292663\n",
      "Epoch 134/200: Loss: 0.0007351683473643788\n",
      "Epoch 135/200: Loss: 0.0007124234636648908\n",
      "Epoch 136/200: Loss: 0.0007474931440810906\n",
      "Epoch 137/200: Loss: 0.0005933781729254406\n",
      "Epoch 138/200: Loss: 0.0006313284249699791\n",
      "Epoch 139/200: Loss: 0.0004458206321032776\n",
      "Epoch 140/200: Loss: 0.0007990212661752594\n",
      "Epoch 141/200: Loss: 0.0005799686515274516\n",
      "Epoch 142/200: Loss: 0.0006156447227112949\n",
      "Epoch 143/200: Loss: 0.0005329061958946113\n",
      "Epoch 144/200: Loss: 0.0004929315069148287\n",
      "Epoch 145/200: Loss: 0.0005621767657430609\n",
      "Epoch 146/200: Loss: 0.00042384720300105985\n",
      "Epoch 147/200: Loss: 0.0006731541820045095\n",
      "Epoch 148/200: Loss: 0.0011720115696334688\n",
      "Epoch 149/200: Loss: 0.000812314850918483\n",
      "Epoch 150/200: Loss: 0.0004600193815349485\n",
      "Epoch 151/200: Loss: 0.0005492314962566525\n",
      "Epoch 152/200: Loss: 0.0004826630938623566\n",
      "Epoch 153/200: Loss: 0.0004538740313364542\n",
      "Epoch 154/200: Loss: 0.0007667041572858579\n",
      "Epoch 155/200: Loss: 0.0008817066529445583\n",
      "Epoch 156/200: Loss: 0.0006653628264757572\n",
      "Epoch 157/200: Loss: 0.0005901221533349599\n",
      "Epoch 158/200: Loss: 0.0009957023379683961\n",
      "Epoch 159/200: Loss: 0.0012443954474292696\n",
      "Epoch 160/200: Loss: 0.0009123296098550781\n",
      "Epoch 161/200: Loss: 0.0008104248738163733\n",
      "Epoch 162/200: Loss: 0.00039968037945072867\n",
      "Epoch 163/200: Loss: 0.0005373339110519737\n",
      "Epoch 164/200: Loss: 0.000570535504448344\n",
      "Epoch 165/200: Loss: 0.0005849631765158847\n",
      "Epoch 166/200: Loss: 0.0005989228127873502\n",
      "Epoch 167/200: Loss: 0.0012320934532908724\n",
      "Epoch 168/200: Loss: 0.0007853394621633924\n",
      "Epoch 169/200: Loss: 0.0008374440509214765\n",
      "Epoch 170/200: Loss: 0.0004300379296182655\n",
      "Epoch 171/200: Loss: 0.0006795345658247243\n",
      "Epoch 172/200: Loss: 0.0007228641632536891\n",
      "Epoch 173/200: Loss: 0.0010967074302243418\n",
      "Epoch 174/200: Loss: 0.0008478962905428489\n",
      "Epoch 175/200: Loss: 0.0006764971642041929\n",
      "Epoch 176/200: Loss: 0.0007273344694112893\n",
      "Epoch 177/200: Loss: 0.0006723253683048825\n",
      "Epoch 178/200: Loss: 0.0007724932789415107\n",
      "Epoch 179/200: Loss: 0.0008105852662083635\n",
      "Epoch 180/200: Loss: 0.0006405788510164712\n",
      "Epoch 181/200: Loss: 0.0005629075109027326\n",
      "Epoch 182/200: Loss: 0.0004187320761047886\n",
      "Epoch 183/200: Loss: 0.0005525436201423873\n",
      "Epoch 184/200: Loss: 0.0005415145059487258\n",
      "Epoch 185/200: Loss: 0.0009227464015566511\n",
      "Epoch 186/200: Loss: 0.0007391063339127868\n",
      "Epoch 187/200: Loss: 0.0006193684464960824\n",
      "Epoch 188/200: Loss: 0.0004915429273751216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/200: Loss: 0.0006376831610396039\n",
      "Epoch 190/200: Loss: 0.0007207054724858609\n",
      "Epoch 191/200: Loss: 0.0005562450191064272\n",
      "Epoch 192/200: Loss: 0.000763837694103131\n",
      "Epoch 193/200: Loss: 0.0006576434345333837\n",
      "Epoch 194/200: Loss: 0.0004787099567693076\n",
      "Epoch 195/200: Loss: 0.0006678796272353793\n",
      "Epoch 196/200: Loss: 0.0007524640141127747\n",
      "Epoch 197/200: Loss: 0.000601085485322983\n",
      "Epoch 198/200: Loss: 0.0006066223253583303\n",
      "Epoch 199/200: Loss: 0.0010366124612119164\n",
      "Epoch 200/200: Loss: 0.0010755552846603677\n",
      "time costs: 1391.432116508484\n",
      "accuarcy: 97.55665037799986\n",
      "epoch: 3\n",
      "Epoch 1/200: Loss: 0.9711285829544067\n",
      "Epoch 2/200: Loss: 1.0033724695444106\n",
      "Epoch 3/200: Loss: 0.9579675376415253\n",
      "Epoch 4/200: Loss: 0.943780392408371\n",
      "Epoch 5/200: Loss: 0.8678381085395813\n",
      "Epoch 6/200: Loss: 0.8888887822628021\n",
      "Epoch 7/200: Loss: 0.7628397434949875\n",
      "Epoch 8/200: Loss: 0.7140692219138145\n",
      "Epoch 9/200: Loss: 0.5750763192772865\n",
      "Epoch 10/200: Loss: 0.518736620247364\n",
      "Epoch 11/200: Loss: 0.43128600120544436\n",
      "Epoch 12/200: Loss: 0.3825493738055229\n",
      "Epoch 13/200: Loss: 0.32304080985486505\n",
      "Epoch 14/200: Loss: 0.25584812052547934\n",
      "Epoch 15/200: Loss: 0.19951146151870489\n",
      "Epoch 16/200: Loss: 0.19785137074068188\n",
      "Epoch 17/200: Loss: 0.14372111801058055\n",
      "Epoch 18/200: Loss: 0.0958871342940256\n",
      "Epoch 19/200: Loss: 0.09098569685593247\n",
      "Epoch 20/200: Loss: 0.057581949622544926\n",
      "Epoch 21/200: Loss: 0.173499130748678\n",
      "Epoch 22/200: Loss: 0.26501786783337594\n",
      "Epoch 23/200: Loss: 0.05718910088762641\n",
      "Epoch 24/200: Loss: 0.02499206895008683\n",
      "Epoch 25/200: Loss: 0.006360980360477697\n",
      "Epoch 26/200: Loss: 0.002900730655528605\n",
      "Epoch 27/200: Loss: 0.0011430083701270632\n",
      "Epoch 28/200: Loss: 0.0018010269875958329\n",
      "Epoch 29/200: Loss: 0.008976645168149843\n",
      "Epoch 30/200: Loss: 0.007986910821637138\n",
      "Epoch 31/200: Loss: 0.0021962328857625833\n",
      "Epoch 32/200: Loss: 0.0009096973069972591\n",
      "Epoch 33/200: Loss: 0.0007324000886001159\n",
      "Epoch 34/200: Loss: 0.0005424987049991615\n",
      "Epoch 35/200: Loss: 0.0002358902431296883\n",
      "Epoch 36/200: Loss: 0.00037556832348855095\n",
      "Epoch 37/200: Loss: 0.00025181919863825897\n",
      "Epoch 38/200: Loss: 0.000370031590864528\n",
      "Epoch 39/200: Loss: 0.007149362893323996\n",
      "Epoch 40/200: Loss: 0.000951717576754163\n",
      "Epoch 41/200: Loss: 0.000949543523893226\n",
      "Epoch 42/200: Loss: 0.0003812943637967692\n",
      "Epoch 43/200: Loss: 0.00036236638752598085\n",
      "Epoch 44/200: Loss: 0.0006305060556769604\n",
      "Epoch 45/200: Loss: 0.0005836051124788356\n",
      "Epoch 46/200: Loss: 0.0004779312363098143\n",
      "Epoch 47/200: Loss: 0.00029241167021609726\n",
      "Epoch 48/200: Loss: 0.0003137020339636365\n",
      "Epoch 49/200: Loss: 0.0005106923777930206\n",
      "Epoch 50/200: Loss: 0.0002957447257358581\n",
      "Epoch 51/200: Loss: 0.00041163683499689796\n",
      "Epoch 52/200: Loss: 0.00040657043809915193\n",
      "Epoch 53/200: Loss: 0.000543762439974671\n",
      "Epoch 54/200: Loss: 0.00033362201174895744\n",
      "Epoch 55/200: Loss: 0.0007091171355568804\n",
      "Epoch 56/200: Loss: 0.0004546057873085374\n",
      "Epoch 57/200: Loss: 0.0010573545887382352\n",
      "Epoch 58/200: Loss: 0.0015021706494735554\n",
      "Epoch 59/200: Loss: 0.00033352151258441153\n",
      "Epoch 60/200: Loss: 0.0005100157894048607\n",
      "Epoch 61/200: Loss: 0.0005099296943626541\n",
      "Epoch 62/200: Loss: 0.0005078269288787852\n",
      "Epoch 63/200: Loss: 0.000462862512722495\n",
      "Epoch 64/200: Loss: 0.0003068532091219822\n",
      "Epoch 65/200: Loss: 0.00033355901141476354\n",
      "Epoch 66/200: Loss: 0.0004946454168930359\n",
      "Epoch 67/200: Loss: 0.00030504547607961284\n",
      "Epoch 68/200: Loss: 0.0006319209755019983\n",
      "Epoch 69/200: Loss: 0.00042149609703301394\n",
      "Epoch 70/200: Loss: 0.0008772989014687483\n",
      "Epoch 71/200: Loss: 0.0004752517246743082\n",
      "Epoch 72/200: Loss: 0.000696688546668156\n",
      "Epoch 73/200: Loss: 0.0005841572048666422\n",
      "Epoch 74/200: Loss: 0.000635958223574562\n",
      "Epoch 75/200: Loss: 0.000663684863866365\n",
      "Epoch 76/200: Loss: 0.0005340792097285885\n",
      "Epoch 77/200: Loss: 0.0003742901961231837\n",
      "Epoch 78/200: Loss: 0.0004831704030948458\n",
      "Epoch 79/200: Loss: 0.0005356006195142982\n",
      "Epoch 80/200: Loss: 0.0005041083610194619\n",
      "Epoch 81/200: Loss: 0.0006573197220859584\n",
      "Epoch 82/200: Loss: 0.0005575771354870085\n",
      "Epoch 83/200: Loss: 0.0005128905533638318\n",
      "Epoch 84/200: Loss: 0.0007509913153626258\n",
      "Epoch 85/200: Loss: 0.0005126719726831653\n",
      "Epoch 86/200: Loss: 0.00045507592512876726\n",
      "Epoch 87/200: Loss: 0.0005738980220485246\n",
      "Epoch 88/200: Loss: 0.0006237726465769811\n",
      "Epoch 89/200: Loss: 0.0006831730706380768\n",
      "Epoch 90/200: Loss: 0.0005458749872559565\n",
      "Epoch 91/200: Loss: 0.00038607406841038026\n",
      "Epoch 92/200: Loss: 0.000396352233383368\n",
      "Epoch 93/200: Loss: 0.00044028555294062244\n",
      "Epoch 94/200: Loss: 0.0005792492234832025\n",
      "Epoch 95/200: Loss: 0.00039115425333875465\n",
      "Epoch 96/200: Loss: 0.0005170839707716368\n",
      "Epoch 97/200: Loss: 0.0006076080033381003\n",
      "Epoch 98/200: Loss: 0.0006643607735895785\n",
      "Epoch 99/200: Loss: 0.0005658201637743332\n",
      "Epoch 100/200: Loss: 0.00037402753205242336\n",
      "Epoch 101/200: Loss: 0.000703103290470608\n",
      "Epoch 102/200: Loss: 0.0007623508616234175\n",
      "Epoch 103/200: Loss: 0.0005197795755520929\n",
      "Epoch 104/200: Loss: 0.0004731168644866557\n",
      "Epoch 105/200: Loss: 0.0004882821883256839\n",
      "Epoch 106/200: Loss: 0.0003712940551849897\n",
      "Epoch 107/200: Loss: 0.0005065834384367918\n",
      "Epoch 108/200: Loss: 0.00041502931435388746\n",
      "Epoch 109/200: Loss: 0.0003670915457405499\n",
      "Epoch 110/200: Loss: 0.0004438075910002226\n",
      "Epoch 111/200: Loss: 0.0005565100198509753\n",
      "Epoch 112/200: Loss: 0.0005920723832332442\n",
      "Epoch 113/200: Loss: 0.0007361183827015339\n",
      "Epoch 114/200: Loss: 0.0005389983442000811\n",
      "Epoch 115/200: Loss: 0.0006481111726316158\n",
      "Epoch 116/200: Loss: 0.0006160948780234321\n",
      "Epoch 117/200: Loss: 0.0006600721529366638\n",
      "Epoch 118/200: Loss: 0.00047346204210043654\n",
      "Epoch 119/200: Loss: 0.0008012636049897992\n",
      "Epoch 120/200: Loss: 0.0005298903528455412\n",
      "Epoch 121/200: Loss: 0.0004455679314560257\n",
      "Epoch 122/200: Loss: 0.0006235839338842198\n",
      "Epoch 123/200: Loss: 0.0006462521702815139\n",
      "Epoch 124/200: Loss: 0.0005539381347261952\n",
      "Epoch 125/200: Loss: 0.0005642760124828783\n",
      "Epoch 126/200: Loss: 0.0007621713339176495\n",
      "Epoch 127/200: Loss: 0.0005612367727735546\n",
      "Epoch 128/200: Loss: 0.0004964507945260266\n",
      "Epoch 129/200: Loss: 0.0006688741748803295\n",
      "Epoch 130/200: Loss: 0.0005873128211533185\n",
      "Epoch 131/200: Loss: 0.000638842505577486\n",
      "Epoch 132/200: Loss: 0.0005620415413432056\n",
      "Epoch 133/200: Loss: 0.0004899817138266371\n",
      "Epoch 134/200: Loss: 0.0005604599209164008\n",
      "Epoch 135/200: Loss: 0.0005005967576835246\n",
      "Epoch 136/200: Loss: 0.0005048582253948553\n",
      "Epoch 137/200: Loss: 0.00042271240636182483\n",
      "Epoch 138/200: Loss: 0.0005962525484392245\n",
      "Epoch 139/200: Loss: 0.00051842222283085\n",
      "Epoch 140/200: Loss: 0.0005960145848803222\n",
      "Epoch 141/200: Loss: 0.00048202638317889067\n",
      "Epoch 142/200: Loss: 0.0006409791654732544\n",
      "Epoch 143/200: Loss: 0.0006233307973161573\n",
      "Epoch 144/200: Loss: 0.0005000242354071815\n",
      "Epoch 145/200: Loss: 0.00045453500242729203\n",
      "Epoch 146/200: Loss: 0.00041628257749835027\n",
      "Epoch 147/200: Loss: 0.0005462001461637555\n",
      "Epoch 148/200: Loss: 0.0006867433097795583\n",
      "Epoch 149/200: Loss: 0.0006838423149019945\n",
      "Epoch 150/200: Loss: 0.0007482860304662608\n",
      "Epoch 151/200: Loss: 0.0005939618746197084\n",
      "Epoch 152/200: Loss: 0.00041020183998625727\n",
      "Epoch 153/200: Loss: 0.0005238381514573121\n",
      "Epoch 154/200: Loss: 0.00047820622385188474\n",
      "Epoch 155/200: Loss: 0.0009266369876058889\n",
      "Epoch 156/200: Loss: 0.00046836764759063956\n",
      "Epoch 157/200: Loss: 0.0005866329094715184\n",
      "Epoch 158/200: Loss: 0.0005661710260028485\n",
      "Epoch 159/200: Loss: 0.0006151935549496557\n",
      "Epoch 160/200: Loss: 0.0005778535461104184\n",
      "Epoch 161/200: Loss: 0.0005628736198559636\n",
      "Epoch 162/200: Loss: 0.0005475327285239473\n",
      "Epoch 163/200: Loss: 0.0006045350636668445\n",
      "Epoch 164/200: Loss: 0.00046944691421231256\n",
      "Epoch 165/200: Loss: 0.0006223952615982853\n",
      "Epoch 166/200: Loss: 0.0008720301873836433\n",
      "Epoch 167/200: Loss: 0.0005328287887095939\n",
      "Epoch 168/200: Loss: 0.00042947098045260647\n",
      "Epoch 169/200: Loss: 0.0007369564988039201\n",
      "Epoch 170/200: Loss: 0.0006145199928141665\n",
      "Epoch 171/200: Loss: 0.0008748044760068296\n",
      "Epoch 172/200: Loss: 0.0009731824431582937\n",
      "Epoch 173/200: Loss: 0.0010074361711303937\n",
      "Epoch 174/200: Loss: 0.0007468918671293068\n",
      "Epoch 175/200: Loss: 0.000534696178874583\n",
      "Epoch 176/200: Loss: 0.0007240222574182553\n",
      "Epoch 177/200: Loss: 0.0005930271478064242\n",
      "Epoch 178/200: Loss: 0.0006721676064898929\n",
      "Epoch 179/200: Loss: 0.00038698940006725023\n",
      "Epoch 180/200: Loss: 0.0006998784529059776\n",
      "Epoch 181/200: Loss: 0.000712759722591727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182/200: Loss: 0.0004606298578437418\n",
      "Epoch 183/200: Loss: 0.0005900447498788708\n",
      "Epoch 184/200: Loss: 0.0007317545738260378\n",
      "Epoch 185/200: Loss: 0.0008220446289669781\n",
      "Epoch 186/200: Loss: 0.0007445820097927936\n",
      "Epoch 187/200: Loss: 0.0005138350781635382\n",
      "Epoch 188/200: Loss: 0.0005135193998285104\n",
      "Epoch 189/200: Loss: 0.00048110624666151123\n",
      "Epoch 190/200: Loss: 0.0008558821159567742\n",
      "Epoch 191/200: Loss: 0.0006967905748751946\n",
      "Epoch 192/200: Loss: 0.0006417268154109479\n",
      "Epoch 193/200: Loss: 0.0007139336914406158\n",
      "Epoch 194/200: Loss: 0.0007835649518710853\n",
      "Epoch 195/200: Loss: 0.0009163936876575463\n",
      "Epoch 196/200: Loss: 0.0011520534637384117\n",
      "Epoch 197/200: Loss: 0.00112099028629018\n",
      "Epoch 198/200: Loss: 0.0008359444535017246\n",
      "Epoch 199/200: Loss: 0.0007079619981595897\n",
      "Epoch 200/200: Loss: 0.001328709558947594\n",
      "time costs: 1396.945961713791\n",
      "accuarcy: 97.7350103370031\n",
      "epoch: 4\n",
      "Epoch 1/200: Loss: 1.0051317900419234\n",
      "Epoch 2/200: Loss: 0.9985414415597915\n",
      "Epoch 3/200: Loss: 0.9704847306013107\n",
      "Epoch 4/200: Loss: 0.9070458829402923\n",
      "Epoch 5/200: Loss: 0.8633401274681092\n",
      "Epoch 6/200: Loss: 0.8815244555473327\n",
      "Epoch 7/200: Loss: 0.6622687861323356\n",
      "Epoch 8/200: Loss: 0.7986580669879914\n",
      "Epoch 9/200: Loss: 0.6679573997855186\n",
      "Epoch 10/200: Loss: 0.6492496442049742\n",
      "Epoch 11/200: Loss: 0.5861618515104056\n",
      "Epoch 12/200: Loss: 0.43694065809249877\n",
      "Epoch 13/200: Loss: 0.42714272551238536\n",
      "Epoch 14/200: Loss: 0.34686064533889294\n",
      "Epoch 15/200: Loss: 0.3930271058343351\n",
      "Epoch 16/200: Loss: 0.20712324138730764\n",
      "Epoch 17/200: Loss: 0.3263548522256315\n",
      "Epoch 18/200: Loss: 0.15020667854696512\n",
      "Epoch 19/200: Loss: 0.0961429059971124\n",
      "Epoch 20/200: Loss: 0.0194086072522623\n",
      "Epoch 21/200: Loss: 0.004666424462448049\n",
      "Epoch 22/200: Loss: 0.0011340318164002384\n",
      "Epoch 23/200: Loss: 0.06053875838406384\n",
      "Epoch 24/200: Loss: 0.006200339581118897\n",
      "Epoch 25/200: Loss: 0.0022473765391623603\n",
      "Epoch 26/200: Loss: 0.0006765264493878931\n",
      "Epoch 27/200: Loss: 0.0016613777333986945\n",
      "Epoch 28/200: Loss: 0.004196282918564975\n",
      "Epoch 29/200: Loss: 0.0010212809473159723\n",
      "Epoch 30/200: Loss: 0.038047176993677566\n",
      "Epoch 31/200: Loss: 0.01036698763809909\n",
      "Epoch 32/200: Loss: 0.0014256980532081798\n",
      "Epoch 33/200: Loss: 0.0007223781369248173\n",
      "Epoch 34/200: Loss: 0.00035836567294609265\n",
      "Epoch 35/200: Loss: 0.0003592257177842839\n",
      "Epoch 36/200: Loss: 0.00043617304472718386\n",
      "Epoch 37/200: Loss: 0.00028829276488977483\n",
      "Epoch 38/200: Loss: 0.00045817472769158483\n",
      "Epoch 39/200: Loss: 0.0003787287338127499\n",
      "Epoch 40/200: Loss: 0.0002910860092015355\n",
      "Epoch 41/200: Loss: 0.00023240903464625263\n",
      "Epoch 42/200: Loss: 0.0003934978140023304\n",
      "Epoch 43/200: Loss: 0.0004121669693631702\n",
      "Epoch 44/200: Loss: 0.0004937691550367162\n",
      "Epoch 45/200: Loss: 0.00037159964676902746\n",
      "Epoch 46/200: Loss: 0.00030710557866768794\n",
      "Epoch 47/200: Loss: 0.0003585075968658202\n",
      "Epoch 48/200: Loss: 0.0002942921746580396\n",
      "Epoch 49/200: Loss: 0.0003835137489659246\n",
      "Epoch 50/200: Loss: 0.0003266355769483198\n",
      "Epoch 51/200: Loss: 0.0004522001730947522\n",
      "Epoch 52/200: Loss: 0.0003854130403851741\n",
      "Epoch 53/200: Loss: 0.0004728462314233184\n",
      "Epoch 54/200: Loss: 0.0003063560746795702\n",
      "Epoch 55/200: Loss: 0.0004166780947798543\n",
      "Epoch 56/200: Loss: 0.00030542981694452463\n",
      "Epoch 57/200: Loss: 0.00044707545694109283\n",
      "Epoch 58/200: Loss: 0.000253577703369956\n",
      "Epoch 59/200: Loss: 0.00046061368957452944\n",
      "Epoch 60/200: Loss: 0.00033863936041598206\n",
      "Epoch 61/200: Loss: 0.0003846162338049908\n",
      "Epoch 62/200: Loss: 0.0002806306277307158\n",
      "Epoch 63/200: Loss: 0.00021854532751603984\n",
      "Epoch 64/200: Loss: 0.0003138183481496526\n",
      "Epoch 65/200: Loss: 0.0003829355996913364\n",
      "Epoch 66/200: Loss: 0.0003784373582675471\n",
      "Epoch 67/200: Loss: 0.00029535883604694393\n",
      "Epoch 68/200: Loss: 0.00028565144348249307\n",
      "Epoch 69/200: Loss: 0.0004706183732196223\n",
      "Epoch 70/200: Loss: 0.00023835434321881622\n",
      "Epoch 71/200: Loss: 0.0003241408321628114\n",
      "Epoch 72/200: Loss: 0.0003629630078648916\n",
      "Epoch 73/200: Loss: 0.0004974374429821182\n",
      "Epoch 74/200: Loss: 0.0003576699048608134\n",
      "Epoch 75/200: Loss: 0.0004386142682051286\n",
      "Epoch 76/200: Loss: 0.0003298952267414279\n",
      "Epoch 77/200: Loss: 0.0002843659114660113\n",
      "Epoch 78/200: Loss: 0.0003637644591435674\n",
      "Epoch 79/200: Loss: 0.00047016197923994697\n",
      "Epoch 80/200: Loss: 0.0002298679716204788\n",
      "Epoch 81/200: Loss: 0.00030810553562332643\n",
      "Epoch 82/200: Loss: 0.00027712255068763626\n",
      "Epoch 83/200: Loss: 0.0003064732196435216\n",
      "Epoch 84/200: Loss: 0.0002747017030742427\n",
      "Epoch 85/200: Loss: 0.00029917491137894104\n",
      "Epoch 86/200: Loss: 0.00031720745703296417\n",
      "Epoch 87/200: Loss: 0.00037375632309704087\n",
      "Epoch 88/200: Loss: 0.0002717747212955146\n",
      "Epoch 89/200: Loss: 0.0002620632347316132\n",
      "Epoch 90/200: Loss: 0.0002791284889099188\n",
      "Epoch 91/200: Loss: 0.0003363042656928883\n",
      "Epoch 92/200: Loss: 0.00029367749111770537\n",
      "Epoch 93/200: Loss: 0.000430623993406698\n",
      "Epoch 94/200: Loss: 0.0003630501792940777\n",
      "Epoch 95/200: Loss: 0.000564633024532668\n",
      "Epoch 96/200: Loss: 0.00031714946726424387\n",
      "Epoch 97/200: Loss: 0.00040273685754073086\n",
      "Epoch 98/200: Loss: 0.0003200369790647528\n",
      "Epoch 99/200: Loss: 0.00031912989788907\n",
      "Epoch 100/200: Loss: 0.00035451553630991837\n",
      "Epoch 101/200: Loss: 0.0003663340733965015\n",
      "Epoch 102/200: Loss: 0.0002760040601970104\n",
      "Epoch 103/200: Loss: 0.0003556690095138038\n",
      "Epoch 104/200: Loss: 0.00031466879499930656\n",
      "Epoch 105/200: Loss: 0.00029243909480101136\n",
      "Epoch 106/200: Loss: 0.0004259327173485872\n",
      "Epoch 107/200: Loss: 0.00038448041705123613\n",
      "Epoch 108/200: Loss: 0.0005093343130283756\n",
      "Epoch 109/200: Loss: 0.0004761649441206828\n",
      "Epoch 110/200: Loss: 0.00042088221016456373\n",
      "Epoch 111/200: Loss: 0.00034882830404967533\n",
      "Epoch 112/200: Loss: 0.00036972349080315324\n",
      "Epoch 113/200: Loss: 0.00024607368868601043\n",
      "Epoch 114/200: Loss: 0.0005841900250743492\n",
      "Epoch 115/200: Loss: 0.0004466207845325698\n",
      "Epoch 116/200: Loss: 0.0003499502647173358\n",
      "Epoch 117/200: Loss: 0.0003323656197608216\n",
      "Epoch 118/200: Loss: 0.00024324990190507378\n",
      "Epoch 119/200: Loss: 0.0004265792380465427\n",
      "Epoch 120/200: Loss: 0.0003840702942397911\n",
      "Epoch 121/200: Loss: 0.00033681518580124247\n",
      "Epoch 122/200: Loss: 0.0006857061223854543\n",
      "Epoch 123/200: Loss: 0.0004666881031880621\n",
      "Epoch 124/200: Loss: 0.0004358230085927062\n",
      "Epoch 125/200: Loss: 0.0004848543267144123\n",
      "Epoch 126/200: Loss: 0.00045931243839731907\n",
      "Epoch 127/200: Loss: 0.0004706417320448963\n",
      "Epoch 128/200: Loss: 0.0004468202121643117\n",
      "Epoch 129/200: Loss: 0.000401715761381638\n",
      "Epoch 130/200: Loss: 0.000393149439150875\n",
      "Epoch 131/200: Loss: 0.00047038282673383947\n",
      "Epoch 132/200: Loss: 0.00035473014559102013\n",
      "Epoch 133/200: Loss: 0.0005221469476964557\n",
      "Epoch 134/200: Loss: 0.0005584849726801622\n",
      "Epoch 135/200: Loss: 0.00038567281926589204\n",
      "Epoch 136/200: Loss: 0.00040750479529378937\n",
      "Epoch 137/200: Loss: 0.0005374457388825249\n",
      "Epoch 138/200: Loss: 0.00067666039294636\n",
      "Epoch 139/200: Loss: 0.000470222779767937\n",
      "Epoch 140/200: Loss: 0.00023228202744576265\n",
      "Epoch 141/200: Loss: 0.0003550285953679122\n",
      "Epoch 142/200: Loss: 0.0002829597873642342\n",
      "Epoch 143/200: Loss: 0.0003961171360060689\n",
      "Epoch 144/200: Loss: 0.0005140505261806538\n",
      "Epoch 145/200: Loss: 0.0006794105693188612\n",
      "Epoch 146/200: Loss: 0.0004707677327132842\n",
      "Epoch 147/200: Loss: 0.0004151459690547199\n",
      "Epoch 148/200: Loss: 0.00033492453330836726\n",
      "Epoch 149/200: Loss: 0.0005789421875306289\n",
      "Epoch 150/200: Loss: 0.0005854480747075286\n",
      "Epoch 151/200: Loss: 0.0004095012704055989\n",
      "Epoch 152/200: Loss: 0.0005842528295033844\n",
      "Epoch 153/200: Loss: 0.000760798153351061\n",
      "Epoch 154/200: Loss: 0.0006422073813155294\n",
      "Epoch 155/200: Loss: 0.0005600878344921512\n",
      "Epoch 156/200: Loss: 0.0004991905923816375\n",
      "Epoch 157/200: Loss: 0.00040117878852470313\n",
      "Epoch 158/200: Loss: 0.0003981330892202095\n",
      "Epoch 159/200: Loss: 0.0004145495755437878\n",
      "Epoch 160/200: Loss: 0.0005031289780163206\n",
      "Epoch 161/200: Loss: 0.0004178119213975151\n",
      "Epoch 162/200: Loss: 0.000581442884504213\n",
      "Epoch 163/200: Loss: 0.00042184382227787863\n",
      "Epoch 164/200: Loss: 0.0006144594539364334\n",
      "Epoch 165/200: Loss: 0.00030695870646013645\n",
      "Epoch 166/200: Loss: 0.0006051646269042976\n",
      "Epoch 167/200: Loss: 0.0003394895975361578\n",
      "Epoch 168/200: Loss: 0.0004151043065576232\n",
      "Epoch 169/200: Loss: 0.0005963053507002769\n",
      "Epoch 170/200: Loss: 0.0006152027512143831\n",
      "Epoch 171/200: Loss: 0.000488271567337506\n",
      "Epoch 172/200: Loss: 0.00047955897443898723\n",
      "Epoch 173/200: Loss: 0.0006648030595897581\n",
      "Epoch 174/200: Loss: 0.0005822836254083085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/200: Loss: 0.0005353501976060216\n",
      "Epoch 176/200: Loss: 0.0005559881759836571\n",
      "Epoch 177/200: Loss: 0.0003482542788333376\n",
      "Epoch 178/200: Loss: 0.0006623328459681943\n",
      "Epoch 179/200: Loss: 0.00041406910513615004\n",
      "Epoch 180/200: Loss: 0.0005136764237249735\n",
      "Epoch 181/200: Loss: 0.0004727544941488304\n",
      "Epoch 182/200: Loss: 0.0005545823329157429\n",
      "Epoch 183/200: Loss: 0.0004522704486589646\n",
      "Epoch 184/200: Loss: 0.0005701808478988824\n",
      "Epoch 185/200: Loss: 0.0005375615943194134\n",
      "Epoch 186/200: Loss: 0.0005358155332942261\n",
      "Epoch 187/200: Loss: 0.0006465780010330491\n",
      "Epoch 188/200: Loss: 0.0007204514962722896\n",
      "Epoch 189/200: Loss: 0.0005388877762015909\n",
      "Epoch 190/200: Loss: 0.0009091096413612832\n",
      "Epoch 191/200: Loss: 0.000966593790508341\n",
      "Epoch 192/200: Loss: 0.0005931335836066864\n",
      "Epoch 193/200: Loss: 0.0005732777099183295\n",
      "Epoch 194/200: Loss: 0.0005073465657915221\n",
      "Epoch 195/200: Loss: 0.0005848936103575397\n",
      "Epoch 196/200: Loss: 0.0008544989490474108\n",
      "Epoch 197/200: Loss: 0.0006265640964556951\n",
      "Epoch 198/200: Loss: 0.0006112903702160111\n",
      "Epoch 199/200: Loss: 0.0005555052048293874\n",
      "Epoch 200/200: Loss: 0.0006193106248247205\n",
      "time costs: 1437.0106356143951\n",
      "accuarcy: 98.90666904399095\n",
      "epoch: 5\n",
      "Epoch 1/200: Loss: 1.083230558037758\n",
      "Epoch 2/200: Loss: 1.0335462242364883\n",
      "Epoch 3/200: Loss: 0.9968551427125931\n",
      "Epoch 4/200: Loss: 0.9681745588779449\n",
      "Epoch 5/200: Loss: 0.9543277055025101\n",
      "Epoch 6/200: Loss: 0.8854541897773742\n",
      "Epoch 7/200: Loss: 0.7988509982824326\n",
      "Epoch 8/200: Loss: 0.8163129478693009\n",
      "Epoch 9/200: Loss: 0.6358540818095207\n",
      "Epoch 10/200: Loss: 0.6139583677053452\n",
      "Epoch 11/200: Loss: 0.609628115594387\n",
      "Epoch 12/200: Loss: 0.49182748347520827\n",
      "Epoch 13/200: Loss: 0.4241484463214874\n",
      "Epoch 14/200: Loss: 0.33981346562504766\n",
      "Epoch 15/200: Loss: 0.297812283039093\n",
      "Epoch 16/200: Loss: 0.21546749472618104\n",
      "Epoch 17/200: Loss: 0.15416080877184868\n",
      "Epoch 18/200: Loss: 0.062243686569854614\n",
      "Epoch 19/200: Loss: 0.04746219483204186\n",
      "Epoch 20/200: Loss: 0.02858384479768574\n",
      "Epoch 21/200: Loss: 0.014051441196352243\n",
      "Epoch 22/200: Loss: 0.00292398890160257\n",
      "Epoch 23/200: Loss: 0.002557175418769475\n",
      "Epoch 24/200: Loss: 0.0011549043272680137\n",
      "Epoch 25/200: Loss: 0.030284943537117213\n",
      "Epoch 26/200: Loss: 0.04290365323540755\n",
      "Epoch 27/200: Loss: 0.016431835060939193\n",
      "Epoch 28/200: Loss: 0.01338550403015688\n",
      "Epoch 29/200: Loss: 0.0046635728922410635\n",
      "Epoch 30/200: Loss: 0.002583532347853179\n",
      "Epoch 31/200: Loss: 0.0008893454738426954\n",
      "Epoch 32/200: Loss: 0.0005762641460023588\n",
      "Epoch 33/200: Loss: 0.0027298922257614324\n",
      "Epoch 34/200: Loss: 0.0025743030812009236\n",
      "Epoch 35/200: Loss: 0.001183221690280334\n",
      "Epoch 36/200: Loss: 0.0005727220501285047\n",
      "Epoch 37/200: Loss: 0.00036875693322144796\n",
      "Epoch 38/200: Loss: 0.11280652763634862\n",
      "Epoch 39/200: Loss: 0.028189774352358653\n",
      "Epoch 40/200: Loss: 0.008180252120655495\n",
      "Epoch 41/200: Loss: 0.0019401805020606844\n",
      "Epoch 42/200: Loss: 0.0007451950690665399\n",
      "Epoch 43/200: Loss: 0.00045598377091664587\n",
      "Epoch 44/200: Loss: 0.0006037291919710697\n",
      "Epoch 45/200: Loss: 0.0005208163539464295\n",
      "Epoch 46/200: Loss: 0.0004159118307143217\n",
      "Epoch 47/200: Loss: 0.000495741793383786\n",
      "Epoch 48/200: Loss: 0.00031447695200768064\n",
      "Epoch 49/200: Loss: 0.00030373544591384417\n",
      "Epoch 50/200: Loss: 0.00030009557740413585\n",
      "Epoch 51/200: Loss: 0.00032250303961518513\n",
      "Epoch 52/200: Loss: 0.00034151114987253096\n",
      "Epoch 53/200: Loss: 0.0004886071875148445\n",
      "Epoch 54/200: Loss: 0.00042196333561150823\n",
      "Epoch 55/200: Loss: 0.002355452840311045\n",
      "Epoch 56/200: Loss: 0.0012554683395137544\n",
      "Epoch 57/200: Loss: 0.0007327868632273748\n",
      "Epoch 58/200: Loss: 0.00037763893087685576\n",
      "Epoch 59/200: Loss: 0.0005187899861994083\n",
      "Epoch 60/200: Loss: 0.0002753457694325334\n",
      "Epoch 61/200: Loss: 0.0004382584587347083\n",
      "Epoch 62/200: Loss: 0.000800995332747334\n",
      "Epoch 63/200: Loss: 0.0005223320653385599\n",
      "Epoch 64/200: Loss: 0.0005942305553617189\n",
      "Epoch 65/200: Loss: 0.0003591320950363297\n",
      "Epoch 66/200: Loss: 0.0003900909558069543\n",
      "Epoch 67/200: Loss: 0.00033508176184113834\n",
      "Epoch 68/200: Loss: 0.0004786183017131407\n",
      "Epoch 69/200: Loss: 0.0003160929443765781\n",
      "Epoch 70/200: Loss: 0.0003062109167331073\n",
      "Epoch 71/200: Loss: 0.0002949042148429726\n",
      "Epoch 72/200: Loss: 0.0005201111483984277\n",
      "Epoch 73/200: Loss: 0.0006016758390614996\n",
      "Epoch 74/200: Loss: 0.0004865019356657285\n",
      "Epoch 75/200: Loss: 0.0004302550405554939\n",
      "Epoch 76/200: Loss: 0.00031257208765964604\n",
      "Epoch 77/200: Loss: 0.00030198562230907553\n",
      "Epoch 78/200: Loss: 0.00041901702134055084\n",
      "Epoch 79/200: Loss: 0.00045784922513121274\n",
      "Epoch 80/200: Loss: 0.0004729616410259041\n",
      "Epoch 81/200: Loss: 0.00039247997283382575\n",
      "Epoch 82/200: Loss: 0.00042572128641040765\n",
      "Epoch 83/200: Loss: 0.00036670088829851013\n",
      "Epoch 84/200: Loss: 0.0003414085287658963\n",
      "Epoch 85/200: Loss: 0.0007092068994097644\n",
      "Epoch 86/200: Loss: 0.00040314371144631875\n",
      "Epoch 87/200: Loss: 0.0004922435755361221\n",
      "Epoch 88/200: Loss: 0.00029381479303083327\n",
      "Epoch 89/200: Loss: 0.00041763856306715753\n",
      "Epoch 90/200: Loss: 0.0003367060318851145\n",
      "Epoch 91/200: Loss: 0.0004546756284526055\n",
      "Epoch 92/200: Loss: 0.0005739357968195691\n",
      "Epoch 93/200: Loss: 0.0004354251585027669\n",
      "Epoch 94/200: Loss: 0.00044592099329747724\n",
      "Epoch 95/200: Loss: 0.0005577378382440656\n",
      "Epoch 96/200: Loss: 0.000506388054418494\n",
      "Epoch 97/200: Loss: 0.0005050739402577165\n",
      "Epoch 98/200: Loss: 0.0005431069317637594\n",
      "Epoch 99/200: Loss: 0.00039918082184158265\n",
      "Epoch 100/200: Loss: 0.0004801032584509812\n",
      "Epoch 101/200: Loss: 0.0005754726589657366\n",
      "Epoch 102/200: Loss: 0.0005572325862885918\n",
      "Epoch 103/200: Loss: 0.00033350859048368874\n",
      "Epoch 104/200: Loss: 0.0005013114592657075\n",
      "Epoch 105/200: Loss: 0.0004935005161087247\n",
      "Epoch 106/200: Loss: 0.0004000176830231794\n",
      "Epoch 107/200: Loss: 0.0005821946910145925\n",
      "Epoch 108/200: Loss: 0.000367911580906366\n",
      "Epoch 109/200: Loss: 0.0003938790983738727\n",
      "Epoch 110/200: Loss: 0.0005360900324376417\n",
      "Epoch 111/200: Loss: 0.0002657787012253721\n",
      "Epoch 112/200: Loss: 0.0003706777838488051\n",
      "Epoch 113/200: Loss: 0.0004153371661232086\n",
      "Epoch 114/200: Loss: 0.00050178315359517\n",
      "Epoch 115/200: Loss: 0.000508589597302489\n",
      "Epoch 116/200: Loss: 0.0004914539080346003\n",
      "Epoch 117/200: Loss: 0.00048216494542430155\n",
      "Epoch 118/200: Loss: 0.000391973568684989\n",
      "Epoch 119/200: Loss: 0.000493821460804611\n",
      "Epoch 120/200: Loss: 0.0005057746540842345\n",
      "Epoch 121/200: Loss: 0.0004504394795731059\n",
      "Epoch 122/200: Loss: 0.0006118419008998899\n",
      "Epoch 123/200: Loss: 0.0006572254897037055\n",
      "Epoch 124/200: Loss: 0.0005280625331579358\n",
      "Epoch 125/200: Loss: 0.0004086821019882336\n",
      "Epoch 126/200: Loss: 0.0003300962805042218\n",
      "Epoch 127/200: Loss: 0.00044330376349535074\n",
      "Epoch 128/200: Loss: 0.0004797893205250148\n",
      "Epoch 129/200: Loss: 0.00033004765045916427\n",
      "Epoch 130/200: Loss: 0.00048284237673215105\n",
      "Epoch 131/200: Loss: 0.000293043021702033\n",
      "Epoch 132/200: Loss: 0.0006819281652497011\n",
      "Epoch 133/200: Loss: 0.00035885618435713693\n",
      "Epoch 134/200: Loss: 0.00031108808934732224\n",
      "Epoch 135/200: Loss: 0.0005717820644349558\n",
      "Epoch 136/200: Loss: 0.0004440174512637896\n",
      "Epoch 137/200: Loss: 0.0007442444682965288\n",
      "Epoch 138/200: Loss: 0.0005374365343413956\n",
      "Epoch 139/200: Loss: 0.0003962185897762538\n",
      "Epoch 140/200: Loss: 0.00034117860850528813\n",
      "Epoch 141/200: Loss: 0.0005433593571069651\n",
      "Epoch 142/200: Loss: 0.0008391044830204919\n",
      "Epoch 143/200: Loss: 0.0006940193889022339\n",
      "Epoch 144/200: Loss: 0.0006396715492883231\n",
      "Epoch 145/200: Loss: 0.0007967172199641936\n",
      "Epoch 146/200: Loss: 0.0008320895996803302\n",
      "Epoch 147/200: Loss: 0.0006055136660506833\n",
      "Epoch 148/200: Loss: 0.0005543576700802078\n",
      "Epoch 149/200: Loss: 0.0004956350110660423\n",
      "Epoch 150/200: Loss: 0.0007072169662933447\n",
      "Epoch 151/200: Loss: 0.0007989442485268228\n",
      "Epoch 152/200: Loss: 0.0007839508642064174\n",
      "Epoch 153/200: Loss: 0.0004251248614309588\n",
      "Epoch 154/200: Loss: 0.0005754851110395976\n",
      "Epoch 155/200: Loss: 0.0005660209430061513\n",
      "Epoch 156/200: Loss: 0.0004178378942015115\n",
      "Epoch 157/200: Loss: 0.0004762270829814952\n",
      "Epoch 158/200: Loss: 0.0009273536976252217\n",
      "Epoch 159/200: Loss: 0.0005525540712369547\n",
      "Epoch 160/200: Loss: 0.0006159198746900074\n",
      "Epoch 161/200: Loss: 0.0006931497133336961\n",
      "Epoch 162/200: Loss: 0.0005999621189403115\n",
      "Epoch 163/200: Loss: 0.0005542935417906847\n",
      "Epoch 164/200: Loss: 0.0006412835566152353\n",
      "Epoch 165/200: Loss: 0.0009400672559422673\n",
      "Epoch 166/200: Loss: 0.0007984586765815039\n",
      "Epoch 167/200: Loss: 0.0008673606538650347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/200: Loss: 0.0005035384619986871\n",
      "Epoch 169/200: Loss: 0.0004707600441179238\n",
      "Epoch 170/200: Loss: 0.00048115439094544856\n",
      "Epoch 171/200: Loss: 0.0006051069567547529\n",
      "Epoch 172/200: Loss: 0.000428209957863146\n",
      "Epoch 173/200: Loss: 0.0006997918770139222\n",
      "Epoch 174/200: Loss: 0.0006614827469093143\n",
      "Epoch 175/200: Loss: 0.0004550674206257099\n",
      "Epoch 176/200: Loss: 0.0006732549278240185\n",
      "Epoch 177/200: Loss: 0.0008037110594159458\n",
      "Epoch 178/200: Loss: 0.0007863788000577188\n",
      "Epoch 179/200: Loss: 0.00038094377605375486\n",
      "Epoch 180/200: Loss: 0.0006726847967911453\n",
      "Epoch 181/200: Loss: 0.0006680542393951328\n",
      "Epoch 182/200: Loss: 0.0005217389578319853\n",
      "Epoch 183/200: Loss: 0.0011990620052529267\n",
      "Epoch 184/200: Loss: 0.0015624811574525665\n",
      "Epoch 185/200: Loss: 0.0009439954657864292\n",
      "Epoch 186/200: Loss: 0.0007474540707335109\n",
      "Epoch 187/200: Loss: 0.000739319080093992\n",
      "Epoch 188/200: Loss: 0.000614546891711143\n",
      "Epoch 189/200: Loss: 0.0007767332464482024\n",
      "Epoch 190/200: Loss: 0.0006316520761174615\n",
      "Epoch 191/200: Loss: 0.0006243904666007438\n",
      "Epoch 192/200: Loss: 0.000391202153514314\n",
      "Epoch 193/200: Loss: 0.00045776520946674284\n",
      "Epoch 194/200: Loss: 0.0004543056209513452\n",
      "Epoch 195/200: Loss: 0.0006636129186517792\n",
      "Epoch 196/200: Loss: 0.0004953492345521227\n",
      "Epoch 197/200: Loss: 0.00046665058816870443\n",
      "Epoch 198/200: Loss: 0.0006593632180738496\n",
      "Epoch 199/200: Loss: 0.0006181208351335954\n",
      "Epoch 200/200: Loss: 0.0007878833530412521\n",
      "time costs: 1436.0304856300354\n",
      "accuarcy: 98.90793200175979\n",
      "epoch: 6\n",
      "Epoch 1/200: Loss: 0.9977306246757507\n",
      "Epoch 2/200: Loss: 0.9861128866672516\n",
      "Epoch 3/200: Loss: 0.9102952778339386\n",
      "Epoch 4/200: Loss: 0.8674350440502167\n",
      "Epoch 5/200: Loss: 0.9052042588591576\n",
      "Epoch 6/200: Loss: 0.7491924539208412\n",
      "Epoch 7/200: Loss: 0.7411072954535485\n",
      "Epoch 8/200: Loss: 0.6210078671574593\n",
      "Epoch 9/200: Loss: 0.6615130305290222\n",
      "Epoch 10/200: Loss: 0.5856042400002479\n",
      "Epoch 11/200: Loss: 0.48219302073121073\n",
      "Epoch 12/200: Loss: 0.516887241601944\n",
      "Epoch 13/200: Loss: 0.4615382444113493\n",
      "Epoch 14/200: Loss: 0.417234493046999\n",
      "Epoch 15/200: Loss: 0.3594735131599009\n",
      "Epoch 16/200: Loss: 0.28051622593775394\n",
      "Epoch 17/200: Loss: 0.24395448192954064\n",
      "Epoch 18/200: Loss: 0.3396518550813198\n",
      "Epoch 19/200: Loss: 0.28036345520522443\n",
      "Epoch 20/200: Loss: 0.11408583428710699\n",
      "Epoch 21/200: Loss: 0.046278472524136305\n",
      "Epoch 22/200: Loss: 0.04077689556579571\n",
      "Epoch 23/200: Loss: 0.022170514524623285\n",
      "Epoch 24/200: Loss: 0.008541623974451795\n",
      "Epoch 25/200: Loss: 0.005165963411855046\n",
      "Epoch 26/200: Loss: 0.007940778142074122\n",
      "Epoch 27/200: Loss: 0.0033898704481543974\n",
      "Epoch 28/200: Loss: 0.0015472711675101891\n",
      "Epoch 29/200: Loss: 0.002357362843758892\n",
      "Epoch 30/200: Loss: 0.0012779033051629086\n",
      "Epoch 31/200: Loss: 0.00044738601354765706\n",
      "Epoch 32/200: Loss: 0.0005189704479562351\n",
      "Epoch 33/200: Loss: 0.0015212777099804953\n",
      "Epoch 34/200: Loss: 0.000653709140169667\n",
      "Epoch 35/200: Loss: 0.0003184187019542151\n",
      "Epoch 36/200: Loss: 0.0004731194785563275\n",
      "Epoch 37/200: Loss: 0.0015024668482510606\n",
      "Epoch 38/200: Loss: 0.0029474123890395275\n",
      "Epoch 39/200: Loss: 0.0007632148256561777\n",
      "Epoch 40/200: Loss: 0.0005462312561576255\n",
      "Epoch 41/200: Loss: 0.0003680111822177423\n",
      "Epoch 42/200: Loss: 0.00030589951911679235\n",
      "Epoch 43/200: Loss: 0.0003446251425430091\n",
      "Epoch 44/200: Loss: 0.0005376927359975525\n",
      "Epoch 45/200: Loss: 0.0004325869090280321\n",
      "Epoch 46/200: Loss: 0.0004052124269946944\n",
      "Epoch 47/200: Loss: 0.00040173784909711683\n",
      "Epoch 48/200: Loss: 0.0003755842147711519\n",
      "Epoch 49/200: Loss: 0.0002879086219763849\n",
      "Epoch 50/200: Loss: 0.0003428451475883776\n",
      "Epoch 51/200: Loss: 0.0004165390539128566\n",
      "Epoch 52/200: Loss: 0.0003035904972421122\n",
      "Epoch 53/200: Loss: 0.00034313031974306794\n",
      "Epoch 54/200: Loss: 0.0004374688496682211\n",
      "Epoch 55/200: Loss: 0.00031412044700118713\n",
      "Epoch 56/200: Loss: 0.0004277146839740453\n",
      "Epoch 57/200: Loss: 0.00046928172178013484\n",
      "Epoch 58/200: Loss: 0.0003204549042493454\n",
      "Epoch 59/200: Loss: 0.00028854100419266614\n",
      "Epoch 60/200: Loss: 0.0003751376520085614\n",
      "Epoch 61/200: Loss: 0.0004956688778293028\n",
      "Epoch 62/200: Loss: 0.00047375500107591504\n",
      "Epoch 63/200: Loss: 0.0006031839136994676\n",
      "Epoch 64/200: Loss: 0.0005323751684500166\n",
      "Epoch 65/200: Loss: 0.00033048619188775775\n",
      "Epoch 66/200: Loss: 0.0005533612878934946\n",
      "Epoch 67/200: Loss: 0.0004776891389383309\n",
      "Epoch 68/200: Loss: 0.0005643454132950865\n",
      "Epoch 69/200: Loss: 0.0003935992815968348\n",
      "Epoch 70/200: Loss: 0.00043930457177339123\n",
      "Epoch 71/200: Loss: 0.00039581754426762925\n",
      "Epoch 72/200: Loss: 0.0004780525476235198\n",
      "Epoch 73/200: Loss: 0.0002769147563867591\n",
      "Epoch 74/200: Loss: 0.00034247044095536694\n",
      "Epoch 75/200: Loss: 0.0007348286853812169\n",
      "Epoch 76/200: Loss: 0.000667895399783447\n",
      "Epoch 77/200: Loss: 0.0004749808818814927\n",
      "Epoch 78/200: Loss: 0.0004354145244178653\n",
      "Epoch 79/200: Loss: 0.00031232946712407285\n",
      "Epoch 80/200: Loss: 0.00039005349226499674\n",
      "Epoch 81/200: Loss: 0.00043739226857724133\n",
      "Epoch 82/200: Loss: 0.0003918833235729835\n",
      "Epoch 83/200: Loss: 0.0006835070063971216\n",
      "Epoch 84/200: Loss: 0.00039853126527304993\n",
      "Epoch 85/200: Loss: 0.00032309106081811477\n",
      "Epoch 86/200: Loss: 0.0006512745789223118\n",
      "Epoch 87/200: Loss: 0.0005836465428728843\n",
      "Epoch 88/200: Loss: 0.0005260585956420982\n",
      "Epoch 89/200: Loss: 0.00039813634866732174\n",
      "Epoch 90/200: Loss: 0.0004667925666581141\n",
      "Epoch 91/200: Loss: 0.0005934303143476427\n",
      "Epoch 92/200: Loss: 0.0006693027342407731\n",
      "Epoch 93/200: Loss: 0.0005532156911613129\n",
      "Epoch 94/200: Loss: 0.00045730028996331383\n",
      "Epoch 95/200: Loss: 0.00047200700137182137\n",
      "Epoch 96/200: Loss: 0.000561234505403263\n",
      "Epoch 97/200: Loss: 0.0004677103283029282\n",
      "Epoch 98/200: Loss: 0.00037137645558686926\n",
      "Epoch 99/200: Loss: 0.0004362205616416759\n",
      "Epoch 100/200: Loss: 0.0003193809172444162\n",
      "Epoch 101/200: Loss: 0.0005402986402259557\n",
      "Epoch 102/200: Loss: 0.0005486030378961005\n",
      "Epoch 103/200: Loss: 0.0003281472068920266\n",
      "Epoch 104/200: Loss: 0.00045067712799209405\n",
      "Epoch 105/200: Loss: 0.00029745285646640693\n",
      "Epoch 106/200: Loss: 0.0005453783523989841\n",
      "Epoch 107/200: Loss: 0.0006144860417407472\n",
      "Epoch 108/200: Loss: 0.0006058677812688984\n",
      "Epoch 109/200: Loss: 0.00043069926141470205\n",
      "Epoch 110/200: Loss: 0.0004243733772455016\n",
      "Epoch 111/200: Loss: 0.00040301233329955723\n",
      "Epoch 112/200: Loss: 0.0004141793471717392\n",
      "Epoch 113/200: Loss: 0.0004794714513991494\n",
      "Epoch 114/200: Loss: 0.0004469488212635042\n",
      "Epoch 115/200: Loss: 0.0006488795531367942\n",
      "Epoch 116/200: Loss: 0.000556296202194062\n",
      "Epoch 117/200: Loss: 0.0004902436033262347\n",
      "Epoch 118/200: Loss: 0.0004849387831200147\n",
      "Epoch 119/200: Loss: 0.0004359658411431155\n",
      "Epoch 120/200: Loss: 0.0003709542606884497\n",
      "Epoch 121/200: Loss: 0.000546994776595966\n",
      "Epoch 122/200: Loss: 0.0007301914954950916\n",
      "Epoch 123/200: Loss: 0.0007873889939219225\n",
      "Epoch 124/200: Loss: 0.00047304014551627913\n",
      "Epoch 125/200: Loss: 0.00031498125645157415\n",
      "Epoch 126/200: Loss: 0.0003926699693693081\n",
      "Epoch 127/200: Loss: 0.0005660494060066412\n",
      "Epoch 128/200: Loss: 0.0005031755767959112\n",
      "Epoch 129/200: Loss: 0.0005114813482578028\n",
      "Epoch 130/200: Loss: 0.0005101772285343031\n",
      "Epoch 131/200: Loss: 0.0004744849545204488\n",
      "Epoch 132/200: Loss: 0.0006462631921749562\n",
      "Epoch 133/200: Loss: 0.0004985652685718378\n",
      "Epoch 134/200: Loss: 0.0005949547430645907\n",
      "Epoch 135/200: Loss: 0.0007876532661612145\n",
      "Epoch 136/200: Loss: 0.0005199953597184503\n",
      "Epoch 137/200: Loss: 0.0008484974665407208\n",
      "Epoch 138/200: Loss: 0.0007054628549667541\n",
      "Epoch 139/200: Loss: 0.0006404151545211789\n",
      "Epoch 140/200: Loss: 0.0005549451630940894\n",
      "Epoch 141/200: Loss: 0.000745737400256985\n",
      "Epoch 142/200: Loss: 0.0008948031405452639\n",
      "Epoch 143/200: Loss: 0.0006168716159663745\n",
      "Epoch 144/200: Loss: 0.0007744740145426476\n",
      "Epoch 145/200: Loss: 0.0003403452821657993\n",
      "Epoch 146/200: Loss: 0.0005924008139118087\n",
      "Epoch 147/200: Loss: 0.0005447651452413993\n",
      "Epoch 148/200: Loss: 0.0005668639991199597\n",
      "Epoch 149/200: Loss: 0.0003889260719006415\n",
      "Epoch 150/200: Loss: 0.0005044140571953904\n",
      "Epoch 151/200: Loss: 0.00043305453236826\n",
      "Epoch 152/200: Loss: 0.00038219493708311345\n",
      "Epoch 153/200: Loss: 0.0006104148531449027\n",
      "Epoch 154/200: Loss: 0.0005029338748499867\n",
      "Epoch 155/200: Loss: 0.0006327294697257458\n",
      "Epoch 156/200: Loss: 0.0005676117914845235\n",
      "Epoch 157/200: Loss: 0.000658517102147016\n",
      "Epoch 158/200: Loss: 0.0004796379791514482\n",
      "Epoch 159/200: Loss: 0.0005951106621068902\n",
      "Epoch 160/200: Loss: 0.00041258602623202024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/200: Loss: 0.0005012165157495474\n",
      "Epoch 162/200: Loss: 0.0005064556342404102\n",
      "Epoch 163/200: Loss: 0.0003818165550910635\n",
      "Epoch 164/200: Loss: 0.0005241455701252562\n",
      "Epoch 165/200: Loss: 0.000769928209047066\n",
      "Epoch 166/200: Loss: 0.0005546901489651646\n",
      "Epoch 167/200: Loss: 0.0002994975267938571\n",
      "Epoch 168/200: Loss: 0.0004713191219707369\n",
      "Epoch 169/200: Loss: 0.0005876133782294346\n",
      "Epoch 170/200: Loss: 0.0005920376899211987\n",
      "Epoch 171/200: Loss: 0.00042883661390078485\n",
      "Epoch 172/200: Loss: 0.0005811838094359701\n",
      "Epoch 173/200: Loss: 0.00048528153420193123\n",
      "Epoch 174/200: Loss: 0.0007784713707224\n",
      "Epoch 175/200: Loss: 0.0005677784918589168\n",
      "Epoch 176/200: Loss: 0.0006557933978911023\n",
      "Epoch 177/200: Loss: 0.0009628211526433005\n",
      "Epoch 178/200: Loss: 0.000727611191541655\n",
      "Epoch 179/200: Loss: 0.0008640901123726508\n",
      "Epoch 180/200: Loss: 0.0006038283581801807\n",
      "Epoch 181/200: Loss: 0.0009079500596271828\n",
      "Epoch 182/200: Loss: 0.0010931458265986294\n",
      "Epoch 183/200: Loss: 0.0007510148070650757\n",
      "Epoch 184/200: Loss: 0.0007540237647390314\n",
      "Epoch 185/200: Loss: 0.0004216820776491659\n",
      "Epoch 186/200: Loss: 0.00048396119018434547\n",
      "Epoch 187/200: Loss: 0.000529158728022594\n",
      "Epoch 188/200: Loss: 0.0004713109286967665\n",
      "Epoch 189/200: Loss: 0.00036086724412598414\n",
      "Epoch 190/200: Loss: 0.00037187378820817684\n",
      "Epoch 191/200: Loss: 0.0008385387365706265\n",
      "Epoch 192/200: Loss: 0.0008765916776610538\n",
      "Epoch 193/200: Loss: 0.0006715832627378404\n",
      "Epoch 194/200: Loss: 0.00071463445055997\n",
      "Epoch 195/200: Loss: 0.0005091851348879572\n",
      "Epoch 196/200: Loss: 0.00048527589788136536\n",
      "Epoch 197/200: Loss: 0.0004030729897749552\n",
      "Epoch 198/200: Loss: 0.0005223285448664683\n",
      "Epoch 199/200: Loss: 0.0006325053192995256\n",
      "Epoch 200/200: Loss: 0.0008661949459565221\n",
      "time costs: 1431.5917868614197\n",
      "accuarcy: 98.77865785265804\n",
      "epoch: 7\n",
      "Epoch 1/200: Loss: 1.0253080040216447\n",
      "Epoch 2/200: Loss: 1.0272441327571868\n",
      "Epoch 3/200: Loss: 0.9772034406661987\n",
      "Epoch 4/200: Loss: 0.9690607845783233\n",
      "Epoch 5/200: Loss: 0.9253245830535889\n",
      "Epoch 6/200: Loss: 0.8621800184249878\n",
      "Epoch 7/200: Loss: 0.7135461524128914\n",
      "Epoch 8/200: Loss: 0.6601391851902008\n",
      "Epoch 9/200: Loss: 0.5673479452729225\n",
      "Epoch 10/200: Loss: 0.45461678579449655\n",
      "Epoch 11/200: Loss: 0.4443288378417492\n",
      "Epoch 12/200: Loss: 0.3720934554934502\n",
      "Epoch 13/200: Loss: 0.3300776592455804\n",
      "Epoch 14/200: Loss: 0.2125521381618455\n",
      "Epoch 15/200: Loss: 0.40807536588981747\n",
      "Epoch 16/200: Loss: 0.24868598860921337\n",
      "Epoch 17/200: Loss: 0.35088845482096076\n",
      "Epoch 18/200: Loss: 0.302453179215081\n",
      "Epoch 19/200: Loss: 0.3160650715231895\n",
      "Epoch 20/200: Loss: 0.13711546291597188\n",
      "Epoch 21/200: Loss: 0.16748611919465475\n",
      "Epoch 22/200: Loss: 0.1425768803426763\n",
      "Epoch 23/200: Loss: 0.1455012898426503\n",
      "Epoch 24/200: Loss: 0.15476047331467271\n",
      "Epoch 25/200: Loss: 0.09035943450871856\n",
      "Epoch 26/200: Loss: 0.058550073771039025\n",
      "Epoch 27/200: Loss: 0.018360698089236394\n",
      "Epoch 28/200: Loss: 0.0405709649756318\n",
      "Epoch 29/200: Loss: 0.01913193636573851\n",
      "Epoch 30/200: Loss: 0.00945357042583055\n",
      "Epoch 31/200: Loss: 0.0033265592719544657\n",
      "Epoch 32/200: Loss: 0.0028923328585733544\n",
      "Epoch 33/200: Loss: 0.001637135933560785\n",
      "Epoch 34/200: Loss: 0.0012191973655717447\n",
      "Epoch 35/200: Loss: 0.0006847669435956049\n",
      "Epoch 36/200: Loss: 0.0031400103427586144\n",
      "Epoch 37/200: Loss: 0.000865401927876519\n",
      "Epoch 38/200: Loss: 0.0009388414335262496\n",
      "Epoch 39/200: Loss: 0.0008898204348952276\n",
      "Epoch 40/200: Loss: 0.000701367728470359\n",
      "Epoch 41/200: Loss: 0.005544803282100474\n",
      "Epoch 42/200: Loss: 0.005446299478353467\n",
      "Epoch 43/200: Loss: 0.0007006885418377351\n",
      "Epoch 44/200: Loss: 0.0006461508593929466\n",
      "Epoch 45/200: Loss: 0.00061783588871549\n",
      "Epoch 46/200: Loss: 0.0006570767636731034\n",
      "Epoch 47/200: Loss: 0.0005294530994433444\n",
      "Epoch 48/200: Loss: 0.0007393022442556685\n",
      "Epoch 49/200: Loss: 0.0005523644267668715\n",
      "Epoch 50/200: Loss: 0.0005283094462356531\n",
      "Epoch 51/200: Loss: 0.00042319514614064246\n",
      "Epoch 52/200: Loss: 0.00038973648224782664\n",
      "Epoch 53/200: Loss: 0.00075514974255384\n",
      "Epoch 54/200: Loss: 0.0006588544030819321\n",
      "Epoch 55/200: Loss: 0.0012387991329887882\n",
      "Epoch 56/200: Loss: 0.0011275200122327078\n",
      "Epoch 57/200: Loss: 0.0010438515884743538\n",
      "Epoch 58/200: Loss: 0.0006289624681812711\n",
      "Epoch 59/200: Loss: 0.0009403248674061615\n",
      "Epoch 60/200: Loss: 0.0008409733991356916\n",
      "Epoch 61/200: Loss: 0.0009414380640009768\n",
      "Epoch 62/200: Loss: 0.0005195916169213888\n",
      "Epoch 63/200: Loss: 0.0006992646922299173\n",
      "Epoch 64/200: Loss: 0.00043969615908281414\n",
      "Epoch 65/200: Loss: 0.0006935163631169416\n",
      "Epoch 66/200: Loss: 0.0005941180272202474\n",
      "Epoch 67/200: Loss: 0.0006192210632434581\n",
      "Epoch 68/200: Loss: 0.0005454701680719154\n",
      "Epoch 69/200: Loss: 0.0004660569044062868\n",
      "Epoch 70/200: Loss: 0.0004529264559096191\n",
      "Epoch 71/200: Loss: 0.000793916793554672\n",
      "Epoch 72/200: Loss: 0.0006596845349122304\n",
      "Epoch 73/200: Loss: 0.0004409192915773019\n",
      "Epoch 74/200: Loss: 0.0006015888416186499\n",
      "Epoch 75/200: Loss: 0.0007819155027391389\n",
      "Epoch 76/200: Loss: 0.0003262804326368496\n",
      "Epoch 77/200: Loss: 0.0004809866044524824\n",
      "Epoch 78/200: Loss: 0.0006839391975518083\n",
      "Epoch 79/200: Loss: 0.0003288238949608058\n",
      "Epoch 80/200: Loss: 0.0006080101549741812\n",
      "Epoch 81/200: Loss: 0.0007687949695537099\n",
      "Epoch 82/200: Loss: 0.000868849319886067\n",
      "Epoch 83/200: Loss: 0.0006091609728173352\n",
      "Epoch 84/200: Loss: 0.0008980397484265268\n",
      "Epoch 85/200: Loss: 0.0006964527845411794\n",
      "Epoch 86/200: Loss: 0.0006758644608453324\n",
      "Epoch 87/200: Loss: 0.0006467051864092355\n",
      "Epoch 88/200: Loss: 0.0010400578466942534\n",
      "Epoch 89/200: Loss: 0.0005033096824263339\n",
      "Epoch 90/200: Loss: 0.0004366701183244004\n",
      "Epoch 91/200: Loss: 0.00046016713031349354\n",
      "Epoch 92/200: Loss: 0.0009625669932574965\n",
      "Epoch 93/200: Loss: 0.0005105014835862676\n",
      "Epoch 94/200: Loss: 0.0005692995850040461\n",
      "Epoch 95/200: Loss: 0.0010518291350308574\n",
      "Epoch 96/200: Loss: 0.0007569385295937536\n",
      "Epoch 97/200: Loss: 0.0005061187090177555\n",
      "Epoch 98/200: Loss: 0.0006329332982204505\n",
      "Epoch 99/200: Loss: 0.0007035544509562897\n",
      "Epoch 100/200: Loss: 0.00045700343707721913\n",
      "Epoch 101/200: Loss: 0.0006653080603427952\n",
      "Epoch 102/200: Loss: 0.0004943643412843812\n",
      "Epoch 103/200: Loss: 0.0007002285327871504\n",
      "Epoch 104/200: Loss: 0.0005440003502485524\n",
      "Epoch 105/200: Loss: 0.0005634107772493735\n",
      "Epoch 106/200: Loss: 0.0005925184925217764\n",
      "Epoch 107/200: Loss: 0.0006449889955547405\n",
      "Epoch 108/200: Loss: 0.0008228964367845038\n",
      "Epoch 109/200: Loss: 0.0006980737788921942\n",
      "Epoch 110/200: Loss: 0.0003486776864519925\n",
      "Epoch 111/200: Loss: 0.0006415724634280196\n",
      "Epoch 112/200: Loss: 0.0006523991898575332\n",
      "Epoch 113/200: Loss: 0.0005732176188757876\n",
      "Epoch 114/200: Loss: 0.0005061580945039168\n",
      "Epoch 115/200: Loss: 0.0004643749469323666\n",
      "Epoch 116/200: Loss: 0.0007103777483280282\n",
      "Epoch 117/200: Loss: 0.0006988262188315275\n",
      "Epoch 118/200: Loss: 0.0007738911813248706\n",
      "Epoch 119/200: Loss: 0.0005733115252951393\n",
      "Epoch 120/200: Loss: 0.0007755712125799618\n",
      "Epoch 121/200: Loss: 0.0014173969773764838\n",
      "Epoch 122/200: Loss: 0.0007858856183702301\n",
      "Epoch 123/200: Loss: 0.0008247058050983469\n",
      "Epoch 124/200: Loss: 0.0006425457846944482\n",
      "Epoch 125/200: Loss: 0.00029522679906222036\n",
      "Epoch 126/200: Loss: 0.00046637530431326013\n",
      "Epoch 127/200: Loss: 0.0004235895336023532\n",
      "Epoch 128/200: Loss: 0.0007449421845194592\n",
      "Epoch 129/200: Loss: 0.0007170213747485832\n",
      "Epoch 130/200: Loss: 0.0007378328758932184\n",
      "Epoch 131/200: Loss: 0.0007011416055320296\n",
      "Epoch 132/200: Loss: 0.0009958940327123855\n",
      "Epoch 133/200: Loss: 0.0007910659331173519\n",
      "Epoch 134/200: Loss: 0.0004669865264077089\n",
      "Epoch 135/200: Loss: 0.0006327368832899083\n",
      "Epoch 136/200: Loss: 0.0006216411173227243\n",
      "Epoch 137/200: Loss: 0.000926966257247841\n",
      "Epoch 138/200: Loss: 0.000871121758245863\n",
      "Epoch 139/200: Loss: 0.00047335912931885106\n",
      "Epoch 140/200: Loss: 0.0005982422026136192\n",
      "Epoch 141/200: Loss: 0.000777202859899262\n",
      "Epoch 142/200: Loss: 0.0005795482669782359\n",
      "Epoch 143/200: Loss: 0.0010794527377584017\n",
      "Epoch 144/200: Loss: 0.0008812697560642846\n",
      "Epoch 145/200: Loss: 0.0008738136129977647\n",
      "Epoch 146/200: Loss: 0.0005409726121797575\n",
      "Epoch 147/200: Loss: 0.0006580352703167592\n",
      "Epoch 148/200: Loss: 0.0005972119975922397\n",
      "Epoch 149/200: Loss: 0.000492707856119523\n",
      "Epoch 150/200: Loss: 0.00042487263608563807\n",
      "Epoch 151/200: Loss: 0.0007866219879360869\n",
      "Epoch 152/200: Loss: 0.0006100299436184286\n",
      "Epoch 153/200: Loss: 0.0007742546785266313\n",
      "Epoch 154/200: Loss: 0.0007142713240682497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/200: Loss: 0.0005305807051627198\n",
      "Epoch 156/200: Loss: 0.0007171596746047726\n",
      "Epoch 157/200: Loss: 0.0007290454375834087\n",
      "Epoch 158/200: Loss: 0.0006223681710253004\n",
      "Epoch 159/200: Loss: 0.0007560063701930631\n",
      "Epoch 160/200: Loss: 0.0004945785562085802\n",
      "Epoch 161/200: Loss: 0.0007294627415831201\n",
      "Epoch 162/200: Loss: 0.000984907847850991\n",
      "Epoch 163/200: Loss: 0.0005425352675956674\n",
      "Epoch 164/200: Loss: 0.000577144189082901\n",
      "Epoch 165/200: Loss: 0.000587145276585943\n",
      "Epoch 166/200: Loss: 0.0003724960537510924\n",
      "Epoch 167/200: Loss: 0.0006534886852023192\n",
      "Epoch 168/200: Loss: 0.0008502691523972316\n",
      "Epoch 169/200: Loss: 0.0008021287430892698\n",
      "Epoch 170/200: Loss: 0.0006800699593441095\n",
      "Epoch 171/200: Loss: 0.0010063692949188407\n",
      "Epoch 172/200: Loss: 0.0011107483645901085\n",
      "Epoch 173/200: Loss: 0.000744960402516881\n",
      "Epoch 174/200: Loss: 0.0008026774370136991\n",
      "Epoch 175/200: Loss: 0.0008262052195277647\n",
      "Epoch 176/200: Loss: 0.0009663420845754444\n",
      "Epoch 177/200: Loss: 0.00045614701994054483\n",
      "Epoch 178/200: Loss: 0.0005639216396957636\n",
      "Epoch 179/200: Loss: 0.0007636726691998774\n",
      "Epoch 180/200: Loss: 0.0012725811120617436\n",
      "Epoch 181/200: Loss: 0.0009629716951167211\n",
      "Epoch 182/200: Loss: 0.0005911952901442418\n",
      "Epoch 183/200: Loss: 0.0007186560105765238\n",
      "Epoch 184/200: Loss: 0.0011575271775654983\n",
      "Epoch 185/200: Loss: 0.0010134436774023926\n",
      "Epoch 186/200: Loss: 0.0009221898701071041\n",
      "Epoch 187/200: Loss: 0.0011689311373629606\n",
      "Epoch 188/200: Loss: 0.0008527260648406809\n",
      "Epoch 189/200: Loss: 0.0005573892982283724\n",
      "Epoch 190/200: Loss: 0.0006318427243968472\n",
      "Epoch 191/200: Loss: 0.0007688738751312485\n",
      "Epoch 192/200: Loss: 0.0006948524740437278\n",
      "Epoch 193/200: Loss: 0.0006911427659360925\n",
      "Epoch 194/200: Loss: 0.0008401539034821326\n",
      "Epoch 195/200: Loss: 0.0010807778260641498\n",
      "Epoch 196/200: Loss: 0.0008427846718404908\n",
      "Epoch 197/200: Loss: 0.0009140101792581845\n",
      "Epoch 198/200: Loss: 0.0024747226176259573\n",
      "Epoch 199/200: Loss: 0.000985942278202856\n",
      "Epoch 200/200: Loss: 0.0007506304697017186\n",
      "time costs: 1433.9591400623322\n",
      "accuarcy: 97.6428650197723\n",
      "epoch: 8\n",
      "Epoch 1/200: Loss: 1.013406452536583\n",
      "Epoch 2/200: Loss: 0.9444956481456757\n",
      "Epoch 3/200: Loss: 0.9472325891256332\n",
      "Epoch 4/200: Loss: 0.9346474856138229\n",
      "Epoch 5/200: Loss: 0.8599805623292923\n",
      "Epoch 6/200: Loss: 0.7601931154727936\n",
      "Epoch 7/200: Loss: 0.5628264442086219\n",
      "Epoch 8/200: Loss: 0.5372414141893387\n",
      "Epoch 9/200: Loss: 0.4691116765141487\n",
      "Epoch 10/200: Loss: 0.3527727209031582\n",
      "Epoch 11/200: Loss: 0.245865832362324\n",
      "Epoch 12/200: Loss: 0.22301330282352866\n",
      "Epoch 13/200: Loss: 0.10752133410423995\n",
      "Epoch 14/200: Loss: 0.04654769857006613\n",
      "Epoch 15/200: Loss: 0.06545401716139168\n",
      "Epoch 16/200: Loss: 0.03039374853760819\n",
      "Epoch 17/200: Loss: 0.11405822303495369\n",
      "Epoch 18/200: Loss: 0.04911387762986123\n",
      "Epoch 19/200: Loss: 0.04996435495559126\n",
      "Epoch 20/200: Loss: 0.011509127418685238\n",
      "Epoch 21/200: Loss: 0.0018988891504704952\n",
      "Epoch 22/200: Loss: 0.002671863133582519\n",
      "Epoch 23/200: Loss: 0.0012497987045207992\n",
      "Epoch 24/200: Loss: 0.000739455661187094\n",
      "Epoch 25/200: Loss: 0.001607358334149467\n",
      "Epoch 26/200: Loss: 0.0004307460793597784\n",
      "Epoch 27/200: Loss: 0.002005453282799863\n",
      "Epoch 28/200: Loss: 0.015481577953323722\n",
      "Epoch 29/200: Loss: 0.0073383428069064395\n",
      "Epoch 30/200: Loss: 0.0010846992532606236\n",
      "Epoch 31/200: Loss: 0.0005344354387489148\n",
      "Epoch 32/200: Loss: 0.0031665996541050843\n",
      "Epoch 33/200: Loss: 0.00592095473621157\n",
      "Epoch 34/200: Loss: 0.0012960231106262654\n",
      "Epoch 35/200: Loss: 0.0014286881931184325\n",
      "Epoch 36/200: Loss: 0.002186809584236471\n",
      "Epoch 37/200: Loss: 0.0006914622586918995\n",
      "Epoch 38/200: Loss: 0.00036821883950324266\n",
      "Epoch 39/200: Loss: 0.0007402285982607282\n",
      "Epoch 40/200: Loss: 0.0005822211588565551\n",
      "Epoch 41/200: Loss: 0.001251057240551745\n",
      "Epoch 42/200: Loss: 0.00321369533194229\n",
      "Epoch 43/200: Loss: 0.000611778980555755\n",
      "Epoch 44/200: Loss: 0.00030301547776616643\n",
      "Epoch 45/200: Loss: 0.000454212694967282\n",
      "Epoch 46/200: Loss: 0.0002644000868713192\n",
      "Epoch 47/200: Loss: 0.00045318902866711144\n",
      "Epoch 48/200: Loss: 0.0005948237198026618\n",
      "Epoch 49/200: Loss: 0.000385489697146113\n",
      "Epoch 50/200: Loss: 0.0004280860815924825\n",
      "Epoch 51/200: Loss: 0.0003546732017184695\n",
      "Epoch 52/200: Loss: 0.0004862615765887313\n",
      "Epoch 53/200: Loss: 0.00042823652038350704\n",
      "Epoch 54/200: Loss: 0.0006326816648652312\n",
      "Epoch 55/200: Loss: 0.00048477984682904207\n",
      "Epoch 56/200: Loss: 0.0003249217505072011\n",
      "Epoch 57/200: Loss: 0.0005891561469979934\n",
      "Epoch 58/200: Loss: 0.0004571882562231622\n",
      "Epoch 59/200: Loss: 0.0006081105101657158\n",
      "Epoch 60/200: Loss: 0.0006847757504147012\n",
      "Epoch 61/200: Loss: 0.00031536243404843843\n",
      "Epoch 62/200: Loss: 0.00046737937882426197\n",
      "Epoch 63/200: Loss: 0.0006339024064800469\n",
      "Epoch 64/200: Loss: 0.0005408075456216465\n",
      "Epoch 65/200: Loss: 0.000405441479961155\n",
      "Epoch 66/200: Loss: 0.00047655994367232777\n",
      "Epoch 67/200: Loss: 0.00035007529859285567\n",
      "Epoch 68/200: Loss: 0.0005110449999847333\n",
      "Epoch 69/200: Loss: 0.0004981751393643208\n",
      "Epoch 70/200: Loss: 0.0004210877012155834\n",
      "Epoch 71/200: Loss: 0.0005317907935022959\n",
      "Epoch 72/200: Loss: 0.0004948560901539168\n",
      "Epoch 73/200: Loss: 0.0005384967168538424\n",
      "Epoch 74/200: Loss: 0.0003943483763578115\n",
      "Epoch 75/200: Loss: 0.0005832026598000084\n",
      "Epoch 76/200: Loss: 0.0005674991862179013\n",
      "Epoch 77/200: Loss: 0.0006096877810705337\n",
      "Epoch 78/200: Loss: 0.0004243104631314054\n",
      "Epoch 79/200: Loss: 0.0005029709082009504\n",
      "Epoch 80/200: Loss: 0.0007354507317359094\n",
      "Epoch 81/200: Loss: 0.000526342792181822\n",
      "Epoch 82/200: Loss: 0.0006921710450114915\n",
      "Epoch 83/200: Loss: 0.0006066881569495308\n",
      "Epoch 84/200: Loss: 0.0006366166957377572\n",
      "Epoch 85/200: Loss: 0.0007071056686982046\n",
      "Epoch 86/200: Loss: 0.0006402359871117369\n",
      "Epoch 87/200: Loss: 0.0003531618165652617\n",
      "Epoch 88/200: Loss: 0.0005396896774982451\n",
      "Epoch 89/200: Loss: 0.0003485823222945328\n",
      "Epoch 90/200: Loss: 0.00034191519080195576\n",
      "Epoch 91/200: Loss: 0.0007704548717811122\n",
      "Epoch 92/200: Loss: 0.0005048461782280356\n",
      "Epoch 93/200: Loss: 0.0008089346782071515\n",
      "Epoch 94/200: Loss: 0.0005958033474598778\n",
      "Epoch 95/200: Loss: 0.0006168028648971813\n",
      "Epoch 96/200: Loss: 0.00041054056127904915\n",
      "Epoch 97/200: Loss: 0.0005862665173481218\n",
      "Epoch 98/200: Loss: 0.0004040108249682817\n",
      "Epoch 99/200: Loss: 0.00040837252222445384\n",
      "Epoch 100/200: Loss: 0.00048085480484587605\n",
      "Epoch 101/200: Loss: 0.0006632280375924893\n",
      "Epoch 102/200: Loss: 0.0006157617390272208\n",
      "Epoch 103/200: Loss: 0.0009501312461907219\n",
      "Epoch 104/200: Loss: 0.0009254889573639957\n",
      "Epoch 105/200: Loss: 0.0006383202260622056\n",
      "Epoch 106/200: Loss: 0.00034442194482835474\n",
      "Epoch 107/200: Loss: 0.0007289648250662139\n",
      "Epoch 108/200: Loss: 0.0005434637794678565\n",
      "Epoch 109/200: Loss: 0.0003677760166283406\n",
      "Epoch 110/200: Loss: 0.00047111931489780546\n",
      "Epoch 111/200: Loss: 0.0006589641654500156\n",
      "Epoch 112/200: Loss: 0.0005987390344671439\n",
      "Epoch 113/200: Loss: 0.0005443281012503576\n",
      "Epoch 114/200: Loss: 0.0005806186094559962\n",
      "Epoch 115/200: Loss: 0.0010380780942796264\n",
      "Epoch 116/200: Loss: 0.0007657158945221454\n",
      "Epoch 117/200: Loss: 0.0007493800301745068\n",
      "Epoch 118/200: Loss: 0.0005611955006315838\n",
      "Epoch 119/200: Loss: 0.0005374089389079018\n",
      "Epoch 120/200: Loss: 0.0004659075609197316\n",
      "Epoch 121/200: Loss: 0.0004238081372932356\n",
      "Epoch 122/200: Loss: 0.0004917231803119648\n",
      "Epoch 123/200: Loss: 0.0007599708702855423\n",
      "Epoch 124/200: Loss: 0.0006765416632333654\n",
      "Epoch 125/200: Loss: 0.00047919502949298474\n",
      "Epoch 126/200: Loss: 0.0004161948085311451\n",
      "Epoch 127/200: Loss: 0.0004987164964404655\n",
      "Epoch 128/200: Loss: 0.00044625946484302406\n",
      "Epoch 129/200: Loss: 0.0006226602985407226\n",
      "Epoch 130/200: Loss: 0.00033242605934447056\n",
      "Epoch 131/200: Loss: 0.0004050660238135606\n",
      "Epoch 132/200: Loss: 0.0005634156646920019\n",
      "Epoch 133/200: Loss: 0.0003074314794503152\n",
      "Epoch 134/200: Loss: 0.0005354046183128957\n",
      "Epoch 135/200: Loss: 0.0005915242940318421\n",
      "Epoch 136/200: Loss: 0.0009019119454023894\n",
      "Epoch 137/200: Loss: 0.0004986346802979824\n",
      "Epoch 138/200: Loss: 0.0004681375139625743\n",
      "Epoch 139/200: Loss: 0.0005846214866323862\n",
      "Epoch 140/200: Loss: 0.0005377814090024913\n",
      "Epoch 141/200: Loss: 0.0008447365147731034\n",
      "Epoch 142/200: Loss: 0.000565552788975765\n",
      "Epoch 143/200: Loss: 0.0008262195726274513\n",
      "Epoch 144/200: Loss: 0.0005850105198987876\n",
      "Epoch 145/200: Loss: 0.0006007489548210288\n",
      "Epoch 146/200: Loss: 0.0008230241932324134\n",
      "Epoch 147/200: Loss: 0.000941468370911025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/200: Loss: 0.000987426019855775\n",
      "Epoch 149/200: Loss: 0.00084408324873948\n",
      "Epoch 150/200: Loss: 0.0003095401105383644\n",
      "Epoch 151/200: Loss: 0.0006040069027221761\n",
      "Epoch 152/200: Loss: 0.0006687883193080779\n",
      "Epoch 153/200: Loss: 0.0009591157686372753\n",
      "Epoch 154/200: Loss: 0.0006372512070811354\n",
      "Epoch 155/200: Loss: 0.0005811882663692813\n",
      "Epoch 156/200: Loss: 0.0005064436565589859\n",
      "Epoch 157/200: Loss: 0.0007572647386950848\n",
      "Epoch 158/200: Loss: 0.0004808475330719375\n",
      "Epoch 159/200: Loss: 0.0006168439693283289\n",
      "Epoch 160/200: Loss: 0.000522893625748111\n",
      "Epoch 161/200: Loss: 0.00041059337863771363\n",
      "Epoch 162/200: Loss: 0.0007922630429675337\n",
      "Epoch 163/200: Loss: 0.0010883009766985197\n",
      "Epoch 164/200: Loss: 0.0009150283312010288\n",
      "Epoch 165/200: Loss: 0.0006672451090707909\n",
      "Epoch 166/200: Loss: 0.0005367869231122313\n",
      "Epoch 167/200: Loss: 0.0006802782518207096\n",
      "Epoch 168/200: Loss: 0.0007433961705828551\n",
      "Epoch 169/200: Loss: 0.0006120456279859355\n",
      "Epoch 170/200: Loss: 0.000738589842512738\n",
      "Epoch 171/200: Loss: 0.000781644020753447\n",
      "Epoch 172/200: Loss: 0.000511472314974526\n",
      "Epoch 173/200: Loss: 0.00048268278956129505\n",
      "Epoch 174/200: Loss: 0.0005085146622150205\n",
      "Epoch 175/200: Loss: 0.0010134944612218532\n",
      "Epoch 176/200: Loss: 0.0007423060895234812\n",
      "Epoch 177/200: Loss: 0.0006641106327151646\n",
      "Epoch 178/200: Loss: 0.0007294985629414441\n",
      "Epoch 179/200: Loss: 0.0007818811362085398\n",
      "Epoch 180/200: Loss: 0.0006288368349487428\n",
      "Epoch 181/200: Loss: 0.00036214339379512237\n",
      "Epoch 182/200: Loss: 0.0007338044648349751\n",
      "Epoch 183/200: Loss: 0.00104850034294941\n",
      "Epoch 184/200: Loss: 0.0008315587163451709\n",
      "Epoch 185/200: Loss: 0.001141645817551762\n",
      "Epoch 186/200: Loss: 0.0008545776079699862\n",
      "Epoch 187/200: Loss: 0.0006463680454544373\n",
      "Epoch 188/200: Loss: 0.0004342327889389708\n",
      "Epoch 189/200: Loss: 0.0007298764350707643\n",
      "Epoch 190/200: Loss: 0.0008925568918130011\n",
      "Epoch 191/200: Loss: 0.0012816131576983026\n",
      "Epoch 192/200: Loss: 0.0008539940710761585\n",
      "Epoch 193/200: Loss: 0.0008504391691531055\n",
      "Epoch 194/200: Loss: 0.0012730424110486637\n",
      "Epoch 195/200: Loss: 0.0009045710281498032\n",
      "Epoch 196/200: Loss: 0.0006873416276903299\n",
      "Epoch 197/200: Loss: 0.0008497505335981259\n",
      "Epoch 198/200: Loss: 0.0003667598732135957\n",
      "Epoch 199/200: Loss: 0.000587540770357009\n",
      "Epoch 200/200: Loss: 0.0006442085898015649\n",
      "time costs: 1432.9550726413727\n",
      "accuarcy: 98.21615160671877\n",
      "epoch: 9\n",
      "Epoch 1/200: Loss: 0.9907233566045761\n",
      "Epoch 2/200: Loss: 0.9521644085645675\n",
      "Epoch 3/200: Loss: 0.9377946436405182\n",
      "Epoch 4/200: Loss: 0.889412009716034\n",
      "Epoch 5/200: Loss: 0.7563856422901154\n",
      "Epoch 6/200: Loss: 0.5273780688643456\n",
      "Epoch 7/200: Loss: 0.4790841432288289\n",
      "Epoch 8/200: Loss: 0.36516812406480315\n",
      "Epoch 9/200: Loss: 0.2883858444169164\n",
      "Epoch 10/200: Loss: 0.21132951015606521\n",
      "Epoch 11/200: Loss: 0.20886267251335083\n",
      "Epoch 12/200: Loss: 0.1215105994604528\n",
      "Epoch 13/200: Loss: 0.05465770603623241\n",
      "Epoch 14/200: Loss: 0.030970680981408805\n",
      "Epoch 15/200: Loss: 0.009737868525553495\n",
      "Epoch 16/200: Loss: 0.0074030990101164205\n",
      "Epoch 17/200: Loss: 0.0025569665696821174\n",
      "Epoch 18/200: Loss: 0.002729080050630728\n",
      "Epoch 19/200: Loss: 0.0028771722500096075\n",
      "Epoch 20/200: Loss: 0.0018058616216876544\n",
      "Epoch 21/200: Loss: 0.0004309023985115346\n",
      "Epoch 22/200: Loss: 0.000508736527444853\n",
      "Epoch 23/200: Loss: 0.0004082126786670415\n",
      "Epoch 24/200: Loss: 0.03130091949751659\n",
      "Epoch 25/200: Loss: 0.01891406629001722\n",
      "Epoch 26/200: Loss: 0.03061735167284496\n",
      "Epoch 27/200: Loss: 0.003094852820504457\n",
      "Epoch 28/200: Loss: 0.0011448573273810324\n",
      "Epoch 29/200: Loss: 0.0005557117252465105\n",
      "Epoch 30/200: Loss: 0.0005004281847504899\n",
      "Epoch 31/200: Loss: 0.0013297268173118938\n",
      "Epoch 32/200: Loss: 0.0011407639454773743\n",
      "Epoch 33/200: Loss: 0.0005074895962025039\n",
      "Epoch 34/200: Loss: 0.008103511736408109\n",
      "Epoch 35/200: Loss: 0.0013286411442095414\n",
      "Epoch 36/200: Loss: 0.0004285354687453946\n",
      "Epoch 37/200: Loss: 0.00044466065719461767\n",
      "Epoch 38/200: Loss: 0.0004996453280909918\n",
      "Epoch 39/200: Loss: 0.0006071624549804256\n",
      "Epoch 40/200: Loss: 0.00033282267249887807\n",
      "Epoch 41/200: Loss: 0.0005058380087575642\n",
      "Epoch 42/200: Loss: 0.0002867075138055952\n",
      "Epoch 43/200: Loss: 0.0008640632125207048\n",
      "Epoch 44/200: Loss: 0.000912128017807845\n",
      "Epoch 45/200: Loss: 0.0005363861667774472\n",
      "Epoch 46/200: Loss: 0.0003684116230942891\n",
      "Epoch 47/200: Loss: 0.0004389179073768901\n",
      "Epoch 48/200: Loss: 0.0005154768316060654\n",
      "Epoch 49/200: Loss: 0.00039412402220477815\n",
      "Epoch 50/200: Loss: 0.00047961483542167114\n",
      "Epoch 51/200: Loss: 0.0005182762297863519\n",
      "Epoch 52/200: Loss: 0.0003180156216330943\n",
      "Epoch 53/200: Loss: 0.0006765039863239509\n",
      "Epoch 54/200: Loss: 0.0005320210699210293\n",
      "Epoch 55/200: Loss: 0.0003258705814005225\n",
      "Epoch 56/200: Loss: 0.0005670791164448019\n",
      "Epoch 57/200: Loss: 0.00037739947765658144\n",
      "Epoch 58/200: Loss: 0.00046545708864869083\n",
      "Epoch 59/200: Loss: 0.00037753556498500986\n",
      "Epoch 60/200: Loss: 0.00038537889304279815\n",
      "Epoch 61/200: Loss: 0.0004815505522856256\n",
      "Epoch 62/200: Loss: 0.0005796578177978517\n",
      "Epoch 63/200: Loss: 0.0005274055560221313\n",
      "Epoch 64/200: Loss: 0.0005711892543331487\n",
      "Epoch 65/200: Loss: 0.00047007834818941775\n",
      "Epoch 66/200: Loss: 0.0004137238197927218\n",
      "Epoch 67/200: Loss: 0.00046808294355287216\n",
      "Epoch 68/200: Loss: 0.00042340671770944025\n",
      "Epoch 69/200: Loss: 0.00030887121665728045\n",
      "Epoch 70/200: Loss: 0.00032052187152658006\n",
      "Epoch 71/200: Loss: 0.0003877435545291519\n",
      "Epoch 72/200: Loss: 0.0002242421594928601\n",
      "Epoch 73/200: Loss: 0.0004048898755172559\n",
      "Epoch 74/200: Loss: 0.00047520420630462466\n",
      "Epoch 75/200: Loss: 0.0005885804384888615\n",
      "Epoch 76/200: Loss: 0.00036725488062074876\n",
      "Epoch 77/200: Loss: 0.0004733699786811485\n",
      "Epoch 78/200: Loss: 0.0005023143033213273\n",
      "Epoch 79/200: Loss: 0.0006541794555232627\n",
      "Epoch 80/200: Loss: 0.000652600502326095\n",
      "Epoch 81/200: Loss: 0.00042034027610498014\n",
      "Epoch 82/200: Loss: 0.00047574251270816604\n",
      "Epoch 83/200: Loss: 0.000622303096770338\n",
      "Epoch 84/200: Loss: 0.0005413291706645396\n",
      "Epoch 85/200: Loss: 0.0008163449409039458\n",
      "Epoch 86/200: Loss: 0.0004993935050151777\n",
      "Epoch 87/200: Loss: 0.0006005787599406176\n",
      "Epoch 88/200: Loss: 0.000463060578385921\n",
      "Epoch 89/200: Loss: 0.00041976268348662413\n",
      "Epoch 90/200: Loss: 0.0006704133880703012\n",
      "Epoch 91/200: Loss: 0.0006943673039131682\n",
      "Epoch 92/200: Loss: 0.00045222827429824974\n",
      "Epoch 93/200: Loss: 0.0005396397122240159\n",
      "Epoch 94/200: Loss: 0.0007587759933812777\n",
      "Epoch 95/200: Loss: 0.0006216950740054016\n",
      "Epoch 96/200: Loss: 0.00043831404800585003\n",
      "Epoch 97/200: Loss: 0.0004980031913873973\n",
      "Epoch 98/200: Loss: 0.0004471993350307457\n",
      "Epoch 99/200: Loss: 0.00039337899379461306\n",
      "Epoch 100/200: Loss: 0.0004430713217516313\n",
      "Epoch 101/200: Loss: 0.0004263485956471413\n",
      "Epoch 102/200: Loss: 0.0004512933963042087\n",
      "Epoch 103/200: Loss: 0.0006367188788317435\n",
      "Epoch 104/200: Loss: 0.0003331498896841367\n",
      "Epoch 105/200: Loss: 0.0003893169487128034\n",
      "Epoch 106/200: Loss: 0.0005162076437045471\n",
      "Epoch 107/200: Loss: 0.00033619316618569426\n",
      "Epoch 108/200: Loss: 0.0004402305286930641\n",
      "Epoch 109/200: Loss: 0.0006501651709186262\n",
      "Epoch 110/200: Loss: 0.0005290015080390731\n",
      "Epoch 111/200: Loss: 0.0007660462836611259\n",
      "Epoch 112/200: Loss: 0.00045855680982640477\n",
      "Epoch 113/200: Loss: 0.000547213477511832\n",
      "Epoch 114/200: Loss: 0.0005911059877689695\n",
      "Epoch 115/200: Loss: 0.00045510722447943407\n",
      "Epoch 116/200: Loss: 0.0004846580666708178\n",
      "Epoch 117/200: Loss: 0.0005983831593766809\n",
      "Epoch 118/200: Loss: 0.0006938475298738922\n",
      "Epoch 119/200: Loss: 0.0005375565873691812\n",
      "Epoch 120/200: Loss: 0.0004304948150092969\n",
      "Epoch 121/200: Loss: 0.0008165481664036634\n",
      "Epoch 122/200: Loss: 0.0009360108517284971\n",
      "Epoch 123/200: Loss: 0.0007987027573108207\n",
      "Epoch 124/200: Loss: 0.0006873928057757439\n",
      "Epoch 125/200: Loss: 0.000769394702729187\n",
      "Epoch 126/200: Loss: 0.0006084115910198306\n",
      "Epoch 127/200: Loss: 0.0006339572222714196\n",
      "Epoch 128/200: Loss: 0.0005287920914724964\n",
      "Epoch 129/200: Loss: 0.00047157474800769706\n",
      "Epoch 130/200: Loss: 0.0005584622414971819\n",
      "Epoch 131/200: Loss: 0.0006818932589794713\n",
      "Epoch 132/200: Loss: 0.0007197581437139889\n",
      "Epoch 133/200: Loss: 0.0005610795610664354\n",
      "Epoch 134/200: Loss: 0.0007273166240338469\n",
      "Epoch 135/200: Loss: 0.0006695804353512358\n",
      "Epoch 136/200: Loss: 0.0006381392278854037\n",
      "Epoch 137/200: Loss: 0.00052743130700037\n",
      "Epoch 138/200: Loss: 0.0006045713436833467\n",
      "Epoch 139/200: Loss: 0.0008061765991442371\n",
      "Epoch 140/200: Loss: 0.0005264137413178105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/200: Loss: 0.0007250246573221375\n",
      "Epoch 142/200: Loss: 0.0007389809377855272\n",
      "Epoch 143/200: Loss: 0.0006896819468238391\n",
      "Epoch 144/200: Loss: 0.0007692438928643241\n",
      "Epoch 145/200: Loss: 0.0005511553123142221\n",
      "Epoch 146/200: Loss: 0.0005400068800554437\n",
      "Epoch 147/200: Loss: 0.0005139700202562381\n",
      "Epoch 148/200: Loss: 0.0006074936789900676\n",
      "Epoch 149/200: Loss: 0.0005649958039612102\n",
      "Epoch 150/200: Loss: 0.0006719483608321752\n",
      "Epoch 151/200: Loss: 0.0008232074236730113\n",
      "Epoch 152/200: Loss: 0.0013830103263899219\n",
      "Epoch 153/200: Loss: 0.0011441641410783632\n",
      "Epoch 154/200: Loss: 0.0009859500623861095\n",
      "Epoch 155/200: Loss: 0.0006239369926333893\n",
      "Epoch 156/200: Loss: 0.0005793662463474903\n",
      "Epoch 157/200: Loss: 0.0006523460728203645\n",
      "Epoch 158/200: Loss: 0.0005969252553768456\n",
      "Epoch 159/200: Loss: 0.0008241204544901848\n",
      "Epoch 160/200: Loss: 0.0006632774562604027\n",
      "Epoch 161/200: Loss: 0.0007740627486782614\n",
      "Epoch 162/200: Loss: 0.0012009511796350126\n",
      "Epoch 163/200: Loss: 0.000744705761098885\n",
      "Epoch 164/200: Loss: 0.00043042712877650047\n",
      "Epoch 165/200: Loss: 0.00065675521382218\n",
      "Epoch 166/200: Loss: 0.00036366312924656085\n",
      "Epoch 167/200: Loss: 0.0005577543375693495\n",
      "Epoch 168/200: Loss: 0.0006307190502411686\n",
      "Epoch 169/200: Loss: 0.0007824535518011544\n",
      "Epoch 170/200: Loss: 0.0008452446458250051\n",
      "Epoch 171/200: Loss: 0.0005980771906251902\n",
      "Epoch 172/200: Loss: 0.0007107471308700041\n",
      "Epoch 173/200: Loss: 0.0005085528657218675\n",
      "Epoch 174/200: Loss: 0.0007508715396397747\n",
      "Epoch 175/200: Loss: 0.0005114920854794036\n",
      "Epoch 176/200: Loss: 0.0005994665065372828\n",
      "Epoch 177/200: Loss: 0.0009838573460001498\n",
      "Epoch 178/200: Loss: 0.0006928790040547028\n",
      "Epoch 179/200: Loss: 0.0005297098909068154\n",
      "Epoch 180/200: Loss: 0.0006097604420574499\n",
      "Epoch 181/200: Loss: 0.0006891557753988309\n",
      "Epoch 182/200: Loss: 0.0006801713563618251\n",
      "Epoch 183/200: Loss: 0.0009184587448544335\n",
      "Epoch 184/200: Loss: 0.000795058673247695\n",
      "Epoch 185/200: Loss: 0.0007578877994092181\n",
      "Epoch 186/200: Loss: 0.0007849360574255116\n",
      "Epoch 187/200: Loss: 0.0004689250607043505\n",
      "Epoch 188/200: Loss: 0.0006185649424878647\n",
      "Epoch 189/200: Loss: 0.00045120915856387\n",
      "Epoch 190/200: Loss: 0.00045470191180356777\n",
      "Epoch 191/200: Loss: 0.000676889842361561\n",
      "Epoch 192/200: Loss: 0.0005499062441231217\n",
      "Epoch 193/200: Loss: 0.0006546827503370878\n",
      "Epoch 194/200: Loss: 0.000758147253509378\n",
      "Epoch 195/200: Loss: 0.0008162799073033966\n",
      "Epoch 196/200: Loss: 0.0007180195045293658\n",
      "Epoch 197/200: Loss: 0.0006328723444312345\n",
      "Epoch 198/200: Loss: 0.0007451118639437482\n",
      "Epoch 199/200: Loss: 0.0005913008320931112\n",
      "Epoch 200/200: Loss: 0.0005411648685367254\n",
      "time costs: 1439.0781154632568\n",
      "accuarcy: 98.74691213071065\n",
      "epoch: 10\n",
      "Epoch 1/200: Loss: 1.032604667544365\n",
      "Epoch 2/200: Loss: 0.9899494141340256\n",
      "Epoch 3/200: Loss: 1.0017196148633958\n",
      "Epoch 4/200: Loss: 0.9699002265930176\n",
      "Epoch 5/200: Loss: 0.889980036020279\n",
      "Epoch 6/200: Loss: 0.8673554003238678\n",
      "Epoch 7/200: Loss: 0.8021662995219231\n",
      "Epoch 8/200: Loss: 0.695849384367466\n",
      "Epoch 9/200: Loss: 0.5237915925681591\n",
      "Epoch 10/200: Loss: 0.44038080237805843\n",
      "Epoch 11/200: Loss: 0.46620797365903854\n",
      "Epoch 12/200: Loss: 0.3386039514094591\n",
      "Epoch 13/200: Loss: 0.34038008507341144\n",
      "Epoch 14/200: Loss: 0.3400701183825731\n",
      "Epoch 15/200: Loss: 0.2539576440118253\n",
      "Epoch 16/200: Loss: 0.19970950225833803\n",
      "Epoch 17/200: Loss: 0.18001779181649907\n",
      "Epoch 18/200: Loss: 0.12678828296484426\n",
      "Epoch 19/200: Loss: 0.18127579719293863\n",
      "Epoch 20/200: Loss: 0.14529722454026342\n",
      "Epoch 21/200: Loss: 0.06661409330554306\n",
      "Epoch 22/200: Loss: 0.0313222689088434\n",
      "Epoch 23/200: Loss: 0.023398281782283446\n",
      "Epoch 24/200: Loss: 0.033618545762146824\n",
      "Epoch 25/200: Loss: 0.017051677394192666\n",
      "Epoch 26/200: Loss: 0.019221046916209163\n",
      "Epoch 27/200: Loss: 0.018590134952683003\n",
      "Epoch 28/200: Loss: 0.006669193564448506\n",
      "Epoch 29/200: Loss: 0.006535124994115904\n",
      "Epoch 30/200: Loss: 0.0014888861442159396\n",
      "Epoch 31/200: Loss: 0.0009287006156228017\n",
      "Epoch 32/200: Loss: 0.0007767216586216819\n",
      "Epoch 33/200: Loss: 0.0009075026653590612\n",
      "Epoch 34/200: Loss: 0.0007465954877261538\n",
      "Epoch 35/200: Loss: 0.0005277416717945016\n",
      "Epoch 36/200: Loss: 0.0005639630886435043\n",
      "Epoch 37/200: Loss: 0.0006176352244438021\n",
      "Epoch 38/200: Loss: 0.0006927249000000302\n",
      "Epoch 39/200: Loss: 0.0005365451855141145\n",
      "Epoch 40/200: Loss: 0.00041629330326031777\n",
      "Epoch 41/200: Loss: 0.000530313146100525\n",
      "Epoch 42/200: Loss: 0.0003734239962795982\n",
      "Epoch 43/200: Loss: 0.0003566353329006233\n",
      "Epoch 44/200: Loss: 0.00046242716666711203\n",
      "Epoch 45/200: Loss: 0.0007614726742758649\n",
      "Epoch 46/200: Loss: 0.00041325858765048904\n",
      "Epoch 47/200: Loss: 0.000493977817495761\n",
      "Epoch 48/200: Loss: 0.0008006436724826927\n",
      "Epoch 49/200: Loss: 0.0005762853937994805\n",
      "Epoch 50/200: Loss: 0.0004091024939043564\n",
      "Epoch 51/200: Loss: 0.0006157247384180664\n",
      "Epoch 52/200: Loss: 0.0007007483294728445\n",
      "Epoch 53/200: Loss: 0.0005030244297813624\n",
      "Epoch 54/200: Loss: 0.0007575408957563923\n",
      "Epoch 55/200: Loss: 0.0004988664986740332\n",
      "Epoch 56/200: Loss: 0.0007301327317691175\n",
      "Epoch 57/200: Loss: 0.001009621640696423\n",
      "Epoch 58/200: Loss: 0.0006207822601936641\n",
      "Epoch 59/200: Loss: 0.00040898078004829584\n",
      "Epoch 60/200: Loss: 0.0005755501862722667\n",
      "Epoch 61/200: Loss: 0.0005285811617795844\n",
      "Epoch 62/200: Loss: 0.00039844069488026435\n",
      "Epoch 63/200: Loss: 0.0007901721251982963\n",
      "Epoch 64/200: Loss: 0.00032143592798092866\n",
      "Epoch 65/200: Loss: 0.0007889256781709264\n",
      "Epoch 66/200: Loss: 0.0008357327131307101\n",
      "Epoch 67/200: Loss: 0.0008106519018838299\n",
      "Epoch 68/200: Loss: 0.0002773435258859536\n",
      "Epoch 69/200: Loss: 0.0007132891583751189\n",
      "Epoch 70/200: Loss: 0.00042038076135213486\n",
      "Epoch 71/200: Loss: 0.0005027872790378751\n",
      "Epoch 72/200: Loss: 0.00044272464670029875\n",
      "Epoch 73/200: Loss: 0.00047588880315743156\n",
      "Epoch 74/200: Loss: 0.0005866324757562325\n",
      "Epoch 75/200: Loss: 0.0005191562864183652\n",
      "Epoch 76/200: Loss: 0.0005265766722004628\n",
      "Epoch 77/200: Loss: 0.0006629346437875938\n",
      "Epoch 78/200: Loss: 0.0006556367579833022\n",
      "Epoch 79/200: Loss: 0.0004521780276263598\n",
      "Epoch 80/200: Loss: 0.0007147060307033826\n",
      "Epoch 81/200: Loss: 0.00045974677359481573\n",
      "Epoch 82/200: Loss: 0.0006110902868385892\n",
      "Epoch 83/200: Loss: 0.000795858480341849\n",
      "Epoch 84/200: Loss: 0.0006441087494749809\n",
      "Epoch 85/200: Loss: 0.0004648544309020508\n",
      "Epoch 86/200: Loss: 0.0005396116570409504\n",
      "Epoch 87/200: Loss: 0.0008146116131683811\n",
      "Epoch 88/200: Loss: 0.00034831028069675084\n",
      "Epoch 89/200: Loss: 0.0006215556582901627\n",
      "Epoch 90/200: Loss: 0.0009654470801251591\n",
      "Epoch 91/200: Loss: 0.0007527594509156188\n",
      "Epoch 92/200: Loss: 0.00045825698107364586\n",
      "Epoch 93/200: Loss: 0.0006449386462918482\n",
      "Epoch 94/200: Loss: 0.0006921928927113186\n",
      "Epoch 95/200: Loss: 0.0005930312352575128\n",
      "Epoch 96/200: Loss: 0.0005996937228701426\n",
      "Epoch 97/200: Loss: 0.0005372390367028856\n",
      "Epoch 98/200: Loss: 0.0006902150831592735\n",
      "Epoch 99/200: Loss: 0.0007016319315880537\n",
      "Epoch 100/200: Loss: 0.000557618698803708\n",
      "Epoch 101/200: Loss: 0.0007308503449166892\n",
      "Epoch 102/200: Loss: 0.0006447991113418539\n",
      "Epoch 103/200: Loss: 0.0008324024154717335\n",
      "Epoch 104/200: Loss: 0.0010552649218880106\n",
      "Epoch 105/200: Loss: 0.000701823648341815\n",
      "Epoch 106/200: Loss: 0.001029169189496315\n",
      "Epoch 107/200: Loss: 0.0005723014828618034\n",
      "Epoch 108/200: Loss: 0.0006074856522900518\n",
      "Epoch 109/200: Loss: 0.000713405112401233\n",
      "Epoch 110/200: Loss: 0.0006561838541529142\n",
      "Epoch 111/200: Loss: 0.001123363186707138\n",
      "Epoch 112/200: Loss: 0.0005948771600742475\n",
      "Epoch 113/200: Loss: 0.0005104884457978187\n",
      "Epoch 114/200: Loss: 0.0007900892676843795\n",
      "Epoch 115/200: Loss: 0.00044898482929056627\n",
      "Epoch 116/200: Loss: 0.0006038523090865055\n",
      "Epoch 117/200: Loss: 0.0005684208783350187\n",
      "Epoch 118/200: Loss: 0.0005495748851899407\n",
      "Epoch 119/200: Loss: 0.00047570470014761667\n",
      "Epoch 120/200: Loss: 0.0007490937023248989\n",
      "Epoch 121/200: Loss: 0.0006093509678976261\n",
      "Epoch 122/200: Loss: 0.001016983280715067\n",
      "Epoch 123/200: Loss: 0.0006347748590997071\n",
      "Epoch 124/200: Loss: 0.0008821863282719277\n",
      "Epoch 125/200: Loss: 0.00046013837536520443\n",
      "Epoch 126/200: Loss: 0.0008159789031196851\n",
      "Epoch 127/200: Loss: 0.000470091607348877\n",
      "Epoch 128/200: Loss: 0.0006779892988561187\n",
      "Epoch 129/200: Loss: 0.00043343049364921173\n",
      "Epoch 130/200: Loss: 0.0008473491692711832\n",
      "Epoch 131/200: Loss: 0.0005278113258100348\n",
      "Epoch 132/200: Loss: 0.0008178154239431024\n",
      "Epoch 133/200: Loss: 0.0005706957828806481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/200: Loss: 0.0006353067898999143\n",
      "Epoch 135/200: Loss: 0.0008360990226719877\n",
      "Epoch 136/200: Loss: 0.0006522555890114745\n",
      "Epoch 137/200: Loss: 0.001197233152197441\n",
      "Epoch 138/200: Loss: 0.0007153781207307475\n",
      "Epoch 139/200: Loss: 0.0015224650014715735\n",
      "Epoch 140/200: Loss: 0.0007066917831252794\n",
      "Epoch 141/200: Loss: 0.0008936003579037788\n",
      "Epoch 142/200: Loss: 0.0009709396234029554\n",
      "Epoch 143/200: Loss: 0.0008516053676430602\n",
      "Epoch 144/200: Loss: 0.0010088444861821699\n",
      "Epoch 145/200: Loss: 0.0010405331497167936\n",
      "Epoch 146/200: Loss: 0.0007263876283104765\n",
      "Epoch 147/200: Loss: 0.0010539714430706226\n",
      "Epoch 148/200: Loss: 0.0007651308293134207\n",
      "Epoch 149/200: Loss: 0.0006701546576550754\n",
      "Epoch 150/200: Loss: 0.0005328432304395392\n",
      "Epoch 151/200: Loss: 0.0006759237272490282\n",
      "Epoch 152/200: Loss: 0.0005863821976163308\n",
      "Epoch 153/200: Loss: 0.0005165574138118245\n",
      "Epoch 154/200: Loss: 0.0006272287970205071\n",
      "Epoch 155/200: Loss: 0.0009856971280896687\n",
      "Epoch 156/200: Loss: 0.000660978216183139\n",
      "Epoch 157/200: Loss: 0.001067537144990638\n",
      "Epoch 158/200: Loss: 0.0007922753209641086\n",
      "Epoch 159/200: Loss: 0.0007254072750583873\n",
      "Epoch 160/200: Loss: 0.0009151766147624585\n",
      "Epoch 161/200: Loss: 0.0005399910236519645\n",
      "Epoch 162/200: Loss: 0.0006275106403336395\n",
      "Epoch 163/200: Loss: 0.0008986359292975976\n",
      "Epoch 164/200: Loss: 0.0009542991061607609\n",
      "Epoch 165/200: Loss: 0.0010581556480246945\n",
      "Epoch 166/200: Loss: 0.0007030096558082732\n",
      "Epoch 167/200: Loss: 0.0007194134916062467\n",
      "Epoch 168/200: Loss: 0.0008411794316998567\n",
      "Epoch 169/200: Loss: 0.001112983305574744\n",
      "Epoch 170/200: Loss: 0.0008686900073371362\n",
      "Epoch 171/200: Loss: 0.0015180573893303518\n",
      "Epoch 172/200: Loss: 0.0018564074249297845\n",
      "Epoch 173/200: Loss: 0.0010872049271711149\n",
      "Epoch 174/200: Loss: 0.0011975722711213166\n",
      "Epoch 175/200: Loss: 0.0004877549867160269\n",
      "Epoch 176/200: Loss: 0.0009387052007241437\n",
      "Epoch 177/200: Loss: 0.0008967634807049762\n",
      "Epoch 178/200: Loss: 0.0010466153446031968\n",
      "Epoch 179/200: Loss: 0.0008602374597103335\n",
      "Epoch 180/200: Loss: 0.0007338781488215318\n",
      "Epoch 181/200: Loss: 0.0011573619794944534\n",
      "Epoch 182/200: Loss: 0.0011909926121006719\n",
      "Epoch 183/200: Loss: 0.0011181530629983173\n",
      "Epoch 184/200: Loss: 0.0009277859513531439\n",
      "Epoch 185/200: Loss: 0.0008186400619706547\n",
      "Epoch 186/200: Loss: 0.0009110886931011919\n",
      "Epoch 187/200: Loss: 0.0007423096954880748\n",
      "Epoch 188/200: Loss: 0.0006470899006671971\n",
      "Epoch 189/200: Loss: 0.0005371572995500173\n",
      "Epoch 190/200: Loss: 0.000438521676005621\n",
      "Epoch 191/200: Loss: 0.0009542118401441258\n",
      "Epoch 192/200: Loss: 0.0005527101751795271\n",
      "Epoch 193/200: Loss: 0.0005993040867906529\n",
      "Epoch 194/200: Loss: 0.0009944154068875833\n",
      "Epoch 195/200: Loss: 0.0011634358568699099\n",
      "Epoch 196/200: Loss: 0.0007341809206991457\n",
      "Epoch 197/200: Loss: 0.0008587685451857396\n",
      "Epoch 198/200: Loss: 0.0006237393665287528\n",
      "Epoch 199/200: Loss: 0.0007195323632913642\n",
      "Epoch 200/200: Loss: 0.0007930582408334885\n",
      "time costs: 1438.6325709819794\n",
      "accuarcy: 97.84785969340504\n",
      "mean times:1433.7043896198272\n",
      "mean accuarcy:98.31385705827356\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "accuarcies = []\n",
    "models = []\n",
    "mode = 'angles'\n",
    "for i in range(10):\n",
    "    print(f'epoch: {i + 1}')\n",
    "    qmodel = QModel(input_size, hidden_size, num_output, \n",
    "                    num_layers = num_layers, ctx = ctx, mode=\"classical\", dropout_rate = 0.001)\n",
    "    optimizer = torch.optim.Adam(qmodel.parameters(), lr = 0.0035)\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    start = time.time()\n",
    "    losses = train_model(qmodel, train_data, batch_size=20,          \n",
    "                   loss_func = loss_func, optimizer = optimizer, epoch = 200)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'time costs: {end - start}')\n",
    "    times.append(end - start)\n",
    "    \n",
    "    accuarcy = calculate_accuarcy(qmodel, X_test, y_test)[0]\n",
    "    print(f'accuarcy: {accuarcy}')\n",
    "    accuarcies.append(accuarcy)\n",
    "    \n",
    "    with open(f'loss/{mode}/loss_{mode}_{i+1}.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(losses, pkl_file)\n",
    "        \n",
    "    models.append(qmodel)\n",
    "    torch.save(qmodel.state_dict(), f\"model/{mode}/model_{mode}_{i+1}.pt\")\n",
    "\n",
    "print(f\"mean times:{np.mean(times)}\")\n",
    "print(f\"mean accuarcy:{np.mean(accuarcies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1679077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "acc, rmse, mse, mae, mape = 0, 0, 0, 0, 0\n",
    "for i in range(epoch):\n",
    "    dropout_lock = True\n",
    "    qmodel = QModel(input_size, hidden_size, num_output, \n",
    "                    num_layers = num_layers, ctx = ctx, mode='classical', dropout_rate = 0.001)\n",
    "    optimizer = torch.optim.Adam(qmodel.parameters(), lr = 0.0015)\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    start = time.time()\n",
    "    losses = train_model(qmodel, train_data, batch_size=20,          \n",
    "                   loss_func = loss_func, optimizer = optimizer, epoch = 200, early_stop=True)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'time costs: {end - start}')\n",
    "\n",
    "    dropout_lock = False\n",
    "    results = calculate_accuarcy(qmodel, X_test, y_test)\n",
    "    acc += results[0]\n",
    "    rmse += results[1]\n",
    "    mse += results[2]\n",
    "    mae += results[3]\n",
    "    mape += results[4]\n",
    "\n",
    "    print(results)\n",
    "\n",
    "print('mean acc: ', acc / epoch)\n",
    "print('mean rmse: ', rmse / epoch)\n",
    "print('mean mse: ', mse / epoch)\n",
    "print('mean mae: ', mae / epoch)\n",
    "print('mean mape: ', mape / epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109228a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean acc:  98.77708771229041\n",
    "# mean rmse:  8.19976289999141\n",
    "# mean mse:  68.07838621765883\n",
    "# mean mae:  6.338886569730907\n",
    "# mean mape:  0.3011534649602976"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d86de",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "acc, rmse, mse, mae, mape = 0, 0, 0, 0, 0\n",
    "for i in range(epoch):\n",
    "    dropout_lock = True\n",
    "    qmodel = QModel(input_size, hidden_size, num_output, \n",
    "                    num_layers = num_layers, ctx = ctx, mode='classical', dropout_rate = 0.005)\n",
    "    optimizer = torch.optim.Adam(qmodel.parameters(), lr = 0.0015)\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    start = time.time()\n",
    "    losses = train_model(qmodel, train_data, batch_size=20,          \n",
    "                   loss_func = loss_func, optimizer = optimizer, epoch = 200, early_stop=True)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'time costs: {end - start}')\n",
    "\n",
    "    dropout_lock = False\n",
    "    results = calculate_accuarcy(qmodel, X_test, y_test)\n",
    "    acc += results[0]\n",
    "    rmse += results[1]\n",
    "    mse += results[2]\n",
    "    mae += results[3]\n",
    "    mape += results[4]\n",
    "\n",
    "    print(results)\n",
    "\n",
    "print('mean acc: ', acc / epoch)\n",
    "print('mean rmse: ', rmse / epoch)\n",
    "print('mean mse: ', mse / epoch)\n",
    "print('mean mae: ', mae / epoch)\n",
    "print('mean mape: ', mape / epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1b9f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean acc:  98.68618230925745\n",
    "# mean rmse:  8.671875681743698\n",
    "# mean mse:  76.89663944446423\n",
    "# mean mae:  6.810088833534145\n",
    "# mean mape:  0.40398970738091106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses, color=\"#FF6666\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d880e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda82d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_lock = False\n",
    "calculate_accuarcy(qmodel, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd61c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# array([98.87070045,  7.63037941, 58.22268997,  5.85365099,  0.43295637])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df63d0a1",
   "metadata": {},
   "source": [
    "### - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b4072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('loss/angle/loss5.pkl', 'wb') as pkl_file:\n",
    "    pickle.dump(losses, pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c09b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_accuracy = np.mean([\n",
    "    0.9842, 0.9850, 0.9837, 0.9858, 0.9852\n",
    "])\n",
    "average_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f6c1ec",
   "metadata": {},
   "source": [
    "### - describe trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c8cdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3497e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable = 0\n",
    "for p in qmodel.parameters():\n",
    "    if p.requires_grad:\n",
    "        trainable += p.numel()\n",
    "print(f'total parameters: {trainable}')\n",
    "\n",
    "qlstm = QLSTMMap.get(qmodel.mode)[1]\n",
    "print(f'quantum paramers: {qlstm(1, 1, ctx = ctx).qparameters_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d33ae3",
   "metadata": {},
   "source": [
    "### - Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f1defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(qmodel.state_dict(), \"model/angle_adjusted.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e8d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i in range(0, len(X_test), features_size):\n",
    "    input_data = X_test[i:i+1]\n",
    "    result = qmodel(input_data)\n",
    "    result = test_scaler.inverse_transform(result.data)\n",
    "    \n",
    "    results.append(result[0])\n",
    "\n",
    "test1 = pd.DataFrame(test, columns=('meantemp',))\n",
    "test2 = pd.DataFrame(test, columns=('humidity',))\n",
    "test3 = pd.DataFrame(test, columns=('wind_speed',))\n",
    "test4 = pd.DataFrame(test, columns=('meanpressure',))\n",
    "results = pd.DataFrame(results)\n",
    "results.columns = ('meantemp', 'humidity', 'wind_speed', 'meanpressure')\n",
    "result1 = pd.DataFrame(results, columns=('meantemp', 'humidity', 'wind_speed'))\n",
    "result2 = pd.DataFrame(results, columns=('meanpressure',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41eb55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(test1, label=\"meantemp\")\n",
    "plt.plot(test2, label=\"humidity\")\n",
    "plt.plot(test3, label=\"wind_speed\")\n",
    "\n",
    "# plt.subplot(1,2,2)\n",
    "plt.ylim(0, 100)\n",
    "plt.plot(result1, color=\"#FF6666\", linestyle=\"--\")\n",
    "plt.legend()\n",
    "\n",
    "# plt.subplot(1,2,1)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(test4, label=\"meanpressure\", color=\"#9966CC\")\n",
    "plt.ylim(980, 1050)\n",
    "plt.plot(result2, color=\"#FF6666\", linestyle='--')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc45b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf9e5db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cdbe6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ed1102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
