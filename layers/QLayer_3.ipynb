{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6581af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94243045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff0ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyqpanda import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "455c7f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a618a3a",
   "metadata": {},
   "source": [
    "# 1. Prepare Dadaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c64faba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/sumanthvrao/daily-climate-time-series-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de627766",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './../data/DailyDelhiClimateTrain.csv'\n",
    "test_path = './../data/DailyDelhiClimateTest.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d0827fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [1,2,3,4]\n",
    "\n",
    "train = pd.read_csv(train_path, usecols=cols, engine=\"python\")\n",
    "test = pd.read_csv(test_path, usecols=cols, engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c3039c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train)=1462\n",
      "len(test)=114\n"
     ]
    }
   ],
   "source": [
    "print(f'len(train)={len(train)}')\n",
    "print(f'len(test)={len(test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b941db2e",
   "metadata": {},
   "source": [
    "## 1.1 Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e59fddc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove outliers num: 9\n"
     ]
    }
   ],
   "source": [
    "unnormal_num = 0\n",
    "for i in range(len(train)):\n",
    "    mp = train.iloc[i][3]\n",
    "    if mp > 1200 or mp < 950:\n",
    "        unnormal_num += 1\n",
    "        train.iloc[i][3] = train.iloc[i + 1][3]\n",
    "print(f'remove outliers num: {unnormal_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fefec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.iloc[0][3] = test.iloc[1][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6035297e",
   "metadata": {},
   "source": [
    "## 1.2 Transfer data to LSTM representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a1277fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "884bc13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(data, window_size, predict_size):\n",
    "    scaler = StandardScaler()\n",
    "    data = scaler.fit_transform(np.array(data).reshape(-1, 1))\n",
    "    \n",
    "    data_in = []\n",
    "    data_out = []\n",
    "    \n",
    "    for i in range(data.shape[0] - window_size - predict_size):\n",
    "        data_in.append(data[i:i + window_size].reshape(1, window_size)[0])\n",
    "        data_out.append(data[i + window_size:i + window_size + predict_size].reshape(1, predict_size)[0])\n",
    "        \n",
    "    data_in = np.array(data_in).reshape(-1, window_size)\n",
    "    data_out = np.array(data_out).reshape(-1, predict_size)\n",
    "    \n",
    "    data_process = {'datain': data_in, 'dataout': data_out}\n",
    "    \n",
    "    return data_process, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6517fe60",
   "metadata": {},
   "source": [
    "## 1.3 prepare train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d333c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_size = 4\n",
    "window_size = features_size * 3 # features num * time steps\n",
    "predict_size = features_size # features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ef548b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed, train_scaler = data_process(train, window_size, predict_size)\n",
    "X_train, y_train = train_processed['datain'], train_processed['dataout']\n",
    "\n",
    "test_processed, test_scaler = data_process(test, window_size, predict_size)\n",
    "X_test, y_test = test_processed['datain'], test_processed['dataout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f779325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dda516d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "\n",
    "train_data = Data.TensorDataset(X_train, y_train)\n",
    "test_data = Data.TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb132c",
   "metadata": {},
   "source": [
    "# 2. Quantum Enhanced LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d517f5f3",
   "metadata": {},
   "source": [
    "## 2.1 initiate quantum environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffc85d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitQMachine:\n",
    "    def __init__(self, qubitsCount, cbitsCount = 0, machineType = QMachineType.CPU):\n",
    "        self.machine = init_quantum_machine(machineType)\n",
    "        \n",
    "        self.qubits = self.machine.qAlloc_many(qubitsCount)\n",
    "        self.cbits = self.machine.cAlloc_many(cbitsCount)\n",
    "        \n",
    "        print(f'Init Quantum Machine with qubits:[{qubitsCount}] / cbits:[{cbitsCount}] Successfully')\n",
    "    \n",
    "    def __del__(self):\n",
    "        destroy_quantum_machine(self.machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "741a6a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Quantum Machine with qubits:[4] / cbits:[0] Successfully\n"
     ]
    }
   ],
   "source": [
    "# maximum qubits size\n",
    "ctx = InitQMachine(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a39cf3",
   "metadata": {},
   "source": [
    "## 2.2 Quantum Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b029d",
   "metadata": {},
   "source": [
    "### 2.2.1 Quantum layer base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5570e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f9cfd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayerBase(nn.Module):\n",
    "    def __init__(self, input_size, output_size, *, n_qubits, n_layers = 1, ctx = None):\n",
    "        super(QuantumLayerBase, self).__init__()\n",
    "        \n",
    "        self.data = None # need to input during forward\n",
    "    \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size # hidden size, not n_qubits\n",
    "        \n",
    "        # quantum infos\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.ctx = ctx\n",
    "        self.qubits = ctx.qubits\n",
    "        self.machine = ctx.machine\n",
    "        \n",
    "        # convert quantum input/output to match classical computation\n",
    "        self.qin = nn.Linear(self.input_size, self.n_qubits)\n",
    "        self.qout = nn.Linear(self.n_qubits, self.output_size)\n",
    "        \n",
    "    @property\n",
    "    def circuit(self):\n",
    "        raise NotImplementedError('Should init circuit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94c5bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(self):\n",
    "    HamiZ = [ PauliOperator({f'Z{i}': 1}) for i in range(len(self.qubits)) ]\n",
    "    res = [ eval(qop(self.circuit, Hami, self.machine, self.qubits))[0,0] for Hami in HamiZ ]\n",
    "    \n",
    "    return Parameter(Tensor(res[:self.n_qubits]))\n",
    "\n",
    "QuantumLayerBase.measure = measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4341341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, inputs):\n",
    "    y_t = self.qin(Parameter(inputs))\n",
    "    self.data = y_t[0]\n",
    "    \n",
    "    return self.qout(self.measure())\n",
    "\n",
    "QuantumLayerBase.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4f794b",
   "metadata": {},
   "source": [
    "### 2.2.2 Quantum layer design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b98a8984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayer(QuantumLayerBase):\n",
    "    def __init__(self, input_size, output_size, *, n_qubits, degree = 1, n_layers = 1, ctx = None):\n",
    "        super(QuantumLayer, self).__init__(input_size, output_size, \n",
    "                                         n_qubits = n_qubits, n_layers = n_layers, ctx = ctx)\n",
    "        \n",
    "        self.degree = degree\n",
    "        self.angles = Parameter(torch.rand(n_layers * 4, degree, self.n_qubits))\n",
    "        \n",
    "    @property\n",
    "    def qparameters_size(self):\n",
    "        return self.angles.flatten().size()[0]\n",
    "        \n",
    "    @property\n",
    "    def circuit(self):\n",
    "        if self.data == None:\n",
    "            raise ValueError('Need to feed a input data!')\n",
    "        \n",
    "        n = self.n_qubits\n",
    "        q = self.qubits\n",
    "        x = self.data\n",
    "        p = self.angles\n",
    "        degree = self.degree\n",
    "        \n",
    "        h = VariationalQuantumGate_H\n",
    "        ry = VariationalQuantumGate_RY\n",
    "        cz = VariationalQuantumGate_CZ\n",
    "        crz = VariationalQuantumGate_CRZ\n",
    "        \n",
    "        # init variational quantum circuit\n",
    "        vqc = VariationalQuantumCircuit()\n",
    "\n",
    "        # encoding layer\n",
    "        [ vqc.insert( h(q[i]) ) for i in range(n) ]\n",
    "        [ vqc.insert( ry(q[i], var(x[i] * torch.pi / 2)) ) for i in range(n) ]\n",
    "        \n",
    "        # variational layer\n",
    "        [ vqc.insert( ry(q[i], var(p[0][0][i]) )) for i in range(n) ]\n",
    "        \n",
    "        vqc.insert(crz(q[0], q[3], var(p[1][0][0])))\n",
    "        vqc.insert(crz(q[3], q[2], var(p[1][0][1])))\n",
    "        vqc.insert(crz(q[2], q[1], var(p[1][0][2])))\n",
    "        vqc.insert(crz(q[1], q[0], var(p[1][0][3])))\n",
    "        \n",
    "        [ vqc.insert( ry(q[i], var(p[2][0][i]) )) for i in range(n) ]\n",
    "        \n",
    "        vqc.insert(crz(q[2], q[3], var(p[3][0][0])))\n",
    "        vqc.insert(crz(q[3], q[0], var(p[3][0][1])))\n",
    "        vqc.insert(crz(q[0], q[1], var(p[3][0][2])))\n",
    "        vqc.insert(crz(q[1], q[2], var(p[3][0][3])))\n",
    "        \n",
    "        return vqc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766692de",
   "metadata": {},
   "source": [
    "### 2.2.3 Plot Quantum Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88ff286b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyqpanda.pyQPanda.QProg at 0x1c7f3cef030>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Tensor([[0.1, 0.2, 0.3, 0.4]])\n",
    "layer = QuantumLayer(4, 4, n_qubits=4, n_layers=1, degree=3, ctx=ctx)\n",
    "layer.data = data[0]\n",
    "vqc = layer.circuit\n",
    "prog = create_empty_qprog()\n",
    "prog.insert(vqc.feed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fd364d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'null'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_qprog(prog, 'pic', filename=f'pic/layer3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50742459",
   "metadata": {},
   "source": [
    "## 2.3 Quantum-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e53ae0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLSTMBase(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, *, ctx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ctx = ctx\n",
    "        \n",
    "    @property\n",
    "    def qparameters_size(self):\n",
    "        num = 0\n",
    "        for attr in dir(self):\n",
    "            if attr.endswith('_circuit'):\n",
    "                num += getattr(self, attr).qparameters_size\n",
    "        return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "582b17af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, inputs, init_states = None):\n",
    "    sequence_size, batch_size, _ = inputs.size()\n",
    "    hidden_sequence = []\n",
    "    \n",
    "    if init_states == None:\n",
    "        h_t, c_t = (\n",
    "            torch.zeros(1, batch_size, self.hidden_size).to(inputs.device),\n",
    "            torch.zeros(1, batch_size, self.hidden_size).to(inputs.device),\n",
    "        )\n",
    "    else:\n",
    "        h_t, c_t = init_states\n",
    "    \n",
    "    return hidden_sequence, (h_t, c_t)\n",
    "\n",
    "QLSTMBase.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d6b0af",
   "metadata": {},
   "source": [
    "## - classical quatum enhanced LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adc8fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLSTM(QLSTMBase):\n",
    "    def __init__(self, input_size, hidden_size, *, ctx):\n",
    "        super().__init__(input_size, hidden_size, ctx = ctx)\n",
    "    \n",
    "        # input gates\n",
    "        self.input_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                        n_qubits = 4, ctx = ctx) # 15\n",
    "        # forget gates\n",
    "        self.forget_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                         n_qubits = 4, ctx = ctx) # 15\n",
    "        # candidate\n",
    "        self.candidate_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                       n_qubits = 4, ctx = ctx) # 15\n",
    "        # output gates\n",
    "        self.output_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                         n_qubits = 4, ctx = ctx) # 15\n",
    "        \n",
    "    def forward(self, inputs, init_states = None):\n",
    "        hidden_sequence, (h_t, c_t) = super(QLSTM, self).forward(inputs, init_states)\n",
    "\n",
    "        for t in range(inputs.size()[0]):\n",
    "            x_t = inputs[t, :, :]\n",
    "            v_t = torch.cat((h_t[0], x_t), dim = 1)\n",
    "\n",
    "            # input gates\n",
    "            i_t = torch.sigmoid(self.input_circuit(v_t))\n",
    "            # forget gates\n",
    "            f_t = torch.sigmoid(self.forget_circuit(v_t))\n",
    "            # candidate for cell state update\n",
    "            g_t = torch.tanh(self.candidate_circuit(v_t))\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "\n",
    "            # output gates\n",
    "            o_t = torch.sigmoid(self.output_circuit(v_t))\n",
    "            # update output ht\n",
    "            h_t = o_t * (torch.tanh(c_t))\n",
    "\n",
    "            hidden_sequence.append(h_t)\n",
    "\n",
    "        # reshape hidden_seq p/ retornar\n",
    "        #\n",
    "        # [tensor([[[0.0444, ...]]] => tensor([[[0.0444, ...]]]\n",
    "        # \n",
    "        hidden_sequence = torch.cat(hidden_sequence, dim = 0)\n",
    "\n",
    "        return hidden_sequence, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28836475",
   "metadata": {},
   "source": [
    "## 2.4 Stacked QLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ec06c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class StackedQLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, *, num_layers = 1, ctx = None, mode = 'classical'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.qlstms = nn.Sequential(OrderedDict([\n",
    "            (f'QLSTM {i + 1}', QLSTM(input_size if i == 0 else hidden_size , hidden_size, ctx = ctx)) \n",
    "                for i in range(num_layers)\n",
    "        ]))\n",
    "\n",
    "    def forward(self, inputs, parameters = None):\n",
    "        outputs = None\n",
    "        \n",
    "        for i, qlstm in enumerate(self.qlstms):\n",
    "            if i != 0:\n",
    "                inputs = outputs\n",
    "            \n",
    "            outputs, parameters = qlstm(inputs, parameters)\n",
    "        \n",
    "        return outputs, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0389535c",
   "metadata": {},
   "source": [
    "# 3. Quantum Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "413150bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_output, *, num_layers = 1, ctx = None, mode = 'classical'):\n",
    "        super(QModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.qlstm = StackedQLSTM(input_size, hidden_size, \n",
    "                                  num_layers = num_layers, ctx = ctx, mode = mode)\n",
    "        self.predict = nn.Linear(hidden_size, num_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        # sequence lenth , batch_size, features length\n",
    "        # \n",
    "        h0 = torch.zeros(1, x.size(1), self.hidden_size)\n",
    "        c0 = torch.zeros(1, x.size(1), self.hidden_size)\n",
    "        \n",
    "        out, _ = self.qlstm(x, (h0, c0))\n",
    "        out = self.predict(out[0])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111aebe4",
   "metadata": {},
   "source": [
    "## 3.1 train QModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc1d1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "def train_model(model, datas, batch_size, *, loss_func, optimizer, epoch = 50):\n",
    "    losses = []\n",
    "    sampler = RandomSampler(datas, num_samples = batch_size)\n",
    "    \n",
    "    for step in range(epoch):\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for index in sampler:\n",
    "            batch_x, batch_y = datas[index][0], datas[index][1]\n",
    "            b_x = batch_x.unsqueeze(0)\n",
    "            b_y = batch_y.unsqueeze(0)\n",
    "            \n",
    "            output = model(b_x)\n",
    "\n",
    "            loss = loss_func(output, b_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {step + 1}/{epoch}: Loss: {train_loss / batch_size}')\n",
    "        losses.append(train_loss / batch_size)\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef07934f",
   "metadata": {},
   "source": [
    "## 3.2 Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57e9b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "def MAE_naive(actuals, predicteds):\n",
    "    n = len(actuals)\n",
    "    err = 0.0\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        err += np.abs(actuals[i] - actuals[i - 1])\n",
    "    return err / (n - 1)\n",
    "\n",
    "def calculate_accuarcy(model, X_test, y_test, scaler=test_scaler):\n",
    "    n = len(X_test)\n",
    "    \n",
    "    actuals = []\n",
    "    predicteds = []\n",
    "    \n",
    "    for i in range(0, n, predict_size):\n",
    "        actual = scaler.inverse_transform(y_test[i:i+1].data)\n",
    "        actuals.append(np.array(actual[0]))\n",
    "        predicted = scaler.inverse_transform(model(X_test[i:i+1]).data)\n",
    "        predicteds.append(np.array(predicted[0]))\n",
    "    \n",
    "    actuals = np.array(actuals)\n",
    "    predicteds = np.array(predicteds)\n",
    "    \n",
    "    mae = mean_absolute_error(actuals, predicteds)\n",
    "    mase = mae / MAE_naive(actuals.flatten(), predicteds.flatten())\n",
    "    mape = mean_absolute_percentage_error(actuals, predicteds)\n",
    "    mse = mean_squared_error(actuals, predicteds)\n",
    "    rmse = mse ** 0.5\n",
    "    \n",
    "    return np.array([(1 - mase) * 100, rmse, mse, mae, mape])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6407f969",
   "metadata": {},
   "source": [
    "## 3.3 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "305c9471",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_size = 4\n",
    "window_size = features_size * 3 # \n",
    "predict_size = features_size # features\n",
    "\n",
    "input_size = window_size\n",
    "num_output = predict_size\n",
    "\n",
    "hidden_size = 32\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50472565",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: 1\n",
      "Epoch 1/100: Loss: 1.027185821533203\n",
      "Epoch 2/100: Loss: 1.029902121424675\n",
      "Epoch 3/100: Loss: 1.0064326584339143\n",
      "Epoch 4/100: Loss: 1.004819032549858\n",
      "Epoch 5/100: Loss: 0.9891128182411194\n",
      "Epoch 6/100: Loss: 0.9734294027090072\n",
      "Epoch 7/100: Loss: 0.967221736907959\n",
      "Epoch 8/100: Loss: 0.9852250128984451\n",
      "Epoch 9/100: Loss: 0.9482600450515747\n",
      "Epoch 10/100: Loss: 0.9987450212240219\n",
      "Epoch 11/100: Loss: 0.9654740124940873\n",
      "Epoch 12/100: Loss: 0.9547377616167069\n",
      "Epoch 13/100: Loss: 0.934275022149086\n",
      "Epoch 14/100: Loss: 0.9460938811302185\n",
      "Epoch 15/100: Loss: 0.941115927696228\n",
      "Epoch 16/100: Loss: 0.9187463879585266\n",
      "Epoch 17/100: Loss: 0.9003453642129898\n",
      "Epoch 18/100: Loss: 0.9304892599582673\n",
      "Epoch 19/100: Loss: 0.8717064946889878\n",
      "Epoch 20/100: Loss: 0.9167912662029266\n",
      "Epoch 21/100: Loss: 0.8450482249259949\n",
      "Epoch 22/100: Loss: 0.8692410290241241\n",
      "Epoch 23/100: Loss: 0.8067771762609481\n",
      "Epoch 24/100: Loss: 0.8257603377103806\n",
      "Epoch 25/100: Loss: 0.7418175727128983\n",
      "Epoch 26/100: Loss: 0.8043049842119216\n",
      "Epoch 27/100: Loss: 0.7416755855083466\n",
      "Epoch 28/100: Loss: 0.7576986849308014\n",
      "Epoch 29/100: Loss: 0.6808635771274567\n",
      "Epoch 30/100: Loss: 0.7097278192639351\n",
      "Epoch 31/100: Loss: 0.7343796014785766\n",
      "Epoch 32/100: Loss: 0.6615000516176224\n",
      "Epoch 33/100: Loss: 0.6570228680968284\n",
      "Epoch 34/100: Loss: 0.6249296709895134\n",
      "Epoch 35/100: Loss: 0.6438242390751838\n",
      "Epoch 36/100: Loss: 0.6400704950094223\n",
      "Epoch 37/100: Loss: 0.6612648740410805\n",
      "Epoch 38/100: Loss: 0.6200839355587959\n",
      "Epoch 39/100: Loss: 0.6158215716481209\n",
      "Epoch 40/100: Loss: 0.5851379618048668\n",
      "Epoch 41/100: Loss: 0.5518781453371048\n",
      "Epoch 42/100: Loss: 0.5910621583461761\n",
      "Epoch 43/100: Loss: 0.5888509184122086\n",
      "Epoch 44/100: Loss: 0.5600388586521149\n",
      "Epoch 45/100: Loss: 0.5920987248420715\n",
      "Epoch 46/100: Loss: 0.5713072463870048\n",
      "Epoch 47/100: Loss: 0.525194376707077\n",
      "Epoch 48/100: Loss: 0.6094604313373566\n",
      "Epoch 49/100: Loss: 0.5344074428081512\n",
      "Epoch 50/100: Loss: 0.5039474681019783\n",
      "Epoch 51/100: Loss: 0.5772229552268981\n",
      "Epoch 52/100: Loss: 0.4988469138741493\n",
      "Epoch 53/100: Loss: 0.5377767264842988\n",
      "Epoch 54/100: Loss: 0.46526374369859697\n",
      "Epoch 55/100: Loss: 0.4915131449699402\n",
      "Epoch 56/100: Loss: 0.5226265728473664\n",
      "Epoch 57/100: Loss: 0.5027252644300461\n",
      "Epoch 58/100: Loss: 0.4902261883020401\n",
      "Epoch 59/100: Loss: 0.45409712195396423\n",
      "Epoch 60/100: Loss: 0.4395814135670662\n",
      "Epoch 61/100: Loss: 0.46208198815584184\n",
      "Epoch 62/100: Loss: 0.4344965651631355\n",
      "Epoch 63/100: Loss: 0.4425674632191658\n",
      "Epoch 64/100: Loss: 0.3993418589234352\n",
      "Epoch 65/100: Loss: 0.3705970861017704\n",
      "Epoch 66/100: Loss: 0.3962286993861198\n",
      "Epoch 67/100: Loss: 0.3941086336970329\n",
      "Epoch 68/100: Loss: 0.41604757159948347\n",
      "Epoch 69/100: Loss: 0.3688930496573448\n",
      "Epoch 70/100: Loss: 0.3961118035018444\n",
      "Epoch 71/100: Loss: 0.33565944582223894\n",
      "Epoch 72/100: Loss: 0.33627417087554934\n",
      "Epoch 73/100: Loss: 0.33567531779408455\n",
      "Epoch 74/100: Loss: 0.3169333845376968\n",
      "Epoch 75/100: Loss: 0.29722755700349807\n",
      "Epoch 76/100: Loss: 0.3156005822122097\n",
      "Epoch 77/100: Loss: 0.3100178107619286\n",
      "Epoch 78/100: Loss: 0.26979699060320855\n",
      "Epoch 79/100: Loss: 0.32459058724343776\n",
      "Epoch 80/100: Loss: 0.30600012131035326\n",
      "Epoch 81/100: Loss: 0.244559733197093\n",
      "Epoch 82/100: Loss: 0.2113271027803421\n",
      "Epoch 83/100: Loss: 0.2800257844850421\n",
      "Epoch 84/100: Loss: 0.2553298886865377\n",
      "Epoch 85/100: Loss: 0.26232128888368605\n",
      "Epoch 86/100: Loss: 0.2633662551641464\n",
      "Epoch 87/100: Loss: 0.2243101954460144\n",
      "Epoch 88/100: Loss: 0.24362498000264168\n",
      "Epoch 89/100: Loss: 0.234660604596138\n",
      "Epoch 90/100: Loss: 0.2092808224260807\n",
      "Epoch 91/100: Loss: 0.21689979620277883\n",
      "Epoch 92/100: Loss: 0.19689662456512452\n",
      "Epoch 93/100: Loss: 0.1852516397833824\n",
      "Epoch 94/100: Loss: 0.18647229820489883\n",
      "Epoch 95/100: Loss: 0.1714835248887539\n",
      "Epoch 96/100: Loss: 0.16729574091732502\n",
      "Epoch 97/100: Loss: 0.15928943939507006\n",
      "Epoch 98/100: Loss: 0.16378762684762477\n",
      "Epoch 99/100: Loss: 0.1727406196296215\n",
      "Epoch 100/100: Loss: 0.15333740487694741\n",
      "time costs: 390.75919342041016\n",
      "--------------------\n",
      "training epoch: 2\n",
      "Epoch 1/100: Loss: 1.121148544549942\n",
      "Epoch 2/100: Loss: 0.9917785435914993\n",
      "Epoch 3/100: Loss: 1.0160781294107437\n",
      "Epoch 4/100: Loss: 1.0605640411376953\n",
      "Epoch 5/100: Loss: 1.04115948677063\n",
      "Epoch 6/100: Loss: 0.9629709392786026\n",
      "Epoch 7/100: Loss: 0.957212170958519\n",
      "Epoch 8/100: Loss: 0.9555651247501373\n",
      "Epoch 9/100: Loss: 0.9107451766729355\n",
      "Epoch 10/100: Loss: 0.9587379693984985\n",
      "Epoch 11/100: Loss: 0.9766143530607223\n",
      "Epoch 12/100: Loss: 0.9590283304452896\n",
      "Epoch 13/100: Loss: 0.9473853290081025\n",
      "Epoch 14/100: Loss: 0.9301879167556762\n",
      "Epoch 15/100: Loss: 0.8894975751638412\n",
      "Epoch 16/100: Loss: 0.8046562671661377\n",
      "Epoch 17/100: Loss: 0.7447483763098717\n",
      "Epoch 18/100: Loss: 0.8234017953276634\n",
      "Epoch 19/100: Loss: 0.7469011262059212\n",
      "Epoch 20/100: Loss: 0.6975559264421463\n",
      "Epoch 21/100: Loss: 0.6297051265835762\n",
      "Epoch 22/100: Loss: 0.7879651308059692\n",
      "Epoch 23/100: Loss: 0.7152776837348938\n",
      "Epoch 24/100: Loss: 0.6057365521788597\n",
      "Epoch 25/100: Loss: 0.6500733364373446\n",
      "Epoch 26/100: Loss: 0.5743788134306669\n",
      "Epoch 27/100: Loss: 0.6513241060078144\n",
      "Epoch 28/100: Loss: 0.6385003121569752\n",
      "Epoch 29/100: Loss: 0.6289179246872664\n",
      "Epoch 30/100: Loss: 0.46725828098133204\n",
      "Epoch 31/100: Loss: 0.476996012032032\n",
      "Epoch 32/100: Loss: 0.4099352132063359\n",
      "Epoch 33/100: Loss: 0.5170524810906499\n",
      "Epoch 34/100: Loss: 0.520500058028847\n",
      "Epoch 35/100: Loss: 0.6308991319965571\n",
      "Epoch 36/100: Loss: 0.5582706866785884\n",
      "Epoch 37/100: Loss: 0.509050402767025\n",
      "Epoch 38/100: Loss: 0.5325829475652426\n",
      "Epoch 39/100: Loss: 0.4591692868852988\n",
      "Epoch 40/100: Loss: 0.48000761882867665\n",
      "Epoch 41/100: Loss: 0.41955775253009053\n",
      "Epoch 42/100: Loss: 0.5398841945454478\n",
      "Epoch 43/100: Loss: 0.39731027621310205\n",
      "Epoch 44/100: Loss: 0.397504154429771\n",
      "Epoch 45/100: Loss: 0.5619627404725179\n",
      "Epoch 46/100: Loss: 0.37383610422257335\n",
      "Epoch 47/100: Loss: 0.4249589893035591\n",
      "Epoch 48/100: Loss: 0.5047727591590956\n",
      "Epoch 49/100: Loss: 0.3989155464340001\n",
      "Epoch 50/100: Loss: 0.33017374309711156\n",
      "Epoch 51/100: Loss: 0.21783663530368358\n",
      "Epoch 52/100: Loss: 0.3917942397063598\n",
      "Epoch 53/100: Loss: 0.4071640098001808\n",
      "Epoch 54/100: Loss: 0.34988158256746826\n",
      "Epoch 55/100: Loss: 0.2773160141194239\n",
      "Epoch 56/100: Loss: 0.3404980498133227\n",
      "Epoch 57/100: Loss: 0.2874193815048784\n",
      "Epoch 58/100: Loss: 0.284905743971467\n",
      "Epoch 59/100: Loss: 0.2664149151649326\n",
      "Epoch 60/100: Loss: 0.2447449746541679\n",
      "Epoch 61/100: Loss: 0.26610680487938226\n",
      "Epoch 62/100: Loss: 0.24167590592987837\n",
      "Epoch 63/100: Loss: 0.24696115446276962\n",
      "Epoch 64/100: Loss: 0.1992608940578066\n",
      "Epoch 65/100: Loss: 0.1318853096337989\n",
      "Epoch 66/100: Loss: 0.23217349331243894\n",
      "Epoch 67/100: Loss: 0.1771577873150818\n",
      "Epoch 68/100: Loss: 0.1870185352046974\n",
      "Epoch 69/100: Loss: 0.06764746170956641\n",
      "Epoch 70/100: Loss: 0.23627630047121784\n",
      "Epoch 71/100: Loss: 0.16922994439955802\n",
      "Epoch 72/100: Loss: 0.07188451079418882\n",
      "Epoch 73/100: Loss: 0.15368345048482296\n",
      "Epoch 74/100: Loss: 0.19360857363790274\n",
      "Epoch 75/100: Loss: 0.1331785020418465\n",
      "Epoch 76/100: Loss: 0.12256986933061853\n",
      "Epoch 77/100: Loss: 0.07621668991050683\n",
      "Epoch 78/100: Loss: 0.09902967597008683\n",
      "Epoch 79/100: Loss: 0.07435015887022019\n",
      "Epoch 80/100: Loss: 0.07745684882975183\n",
      "Epoch 81/100: Loss: 0.09670762366440613\n",
      "Epoch 82/100: Loss: 0.06313489708409178\n",
      "Epoch 83/100: Loss: 0.05443077240452112\n",
      "Epoch 84/100: Loss: 0.07192863140298869\n",
      "Epoch 85/100: Loss: 0.07072180737595772\n",
      "Epoch 86/100: Loss: 0.07342596672242507\n",
      "Epoch 87/100: Loss: 0.057375607942231\n",
      "Epoch 88/100: Loss: 0.03892493736493634\n",
      "Epoch 89/100: Loss: 0.05323224011808634\n",
      "Epoch 90/100: Loss: 0.055058516011195026\n",
      "Epoch 91/100: Loss: 0.055446382123045625\n",
      "Epoch 92/100: Loss: 0.05868453253060579\n",
      "Epoch 93/100: Loss: 0.0314942508310196\n",
      "Epoch 94/100: Loss: 0.04479934346381924\n",
      "Epoch 95/100: Loss: 0.029393555666320025\n",
      "Epoch 96/100: Loss: 0.043145957356318834\n",
      "Epoch 97/100: Loss: 0.023671054298756645\n",
      "Epoch 98/100: Loss: 0.02287485989909328\n",
      "Epoch 99/100: Loss: 0.025095846758631524\n",
      "Epoch 100/100: Loss: 0.017615482398832684\n",
      "time costs: 383.7832329273224\n",
      "--------------------\n",
      "training epoch: 3\n",
      "Epoch 1/100: Loss: 1.0070564895868301\n",
      "Epoch 2/100: Loss: 0.9951937913894653\n",
      "Epoch 3/100: Loss: 0.9697044014930725\n",
      "Epoch 4/100: Loss: 0.9754993766546249\n",
      "Epoch 5/100: Loss: 0.9661432653665543\n",
      "Epoch 6/100: Loss: 0.9519728153944016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: Loss: 0.9368378490209579\n",
      "Epoch 8/100: Loss: 0.9028856575489044\n",
      "Epoch 9/100: Loss: 0.9052019089460372\n",
      "Epoch 10/100: Loss: 0.8581693917512894\n",
      "Epoch 11/100: Loss: 0.816168487071991\n",
      "Epoch 12/100: Loss: 0.7901678085327148\n",
      "Epoch 13/100: Loss: 0.7620234280824661\n",
      "Epoch 14/100: Loss: 0.6975029349327088\n",
      "Epoch 15/100: Loss: 0.6746934548020362\n",
      "Epoch 16/100: Loss: 0.6181622460484505\n",
      "Epoch 17/100: Loss: 0.5495435252785683\n",
      "Epoch 18/100: Loss: 0.559938321262598\n",
      "Epoch 19/100: Loss: 0.49921054765582085\n",
      "Epoch 20/100: Loss: 0.4422291971743107\n",
      "Epoch 21/100: Loss: 0.39481549635529517\n",
      "Epoch 22/100: Loss: 0.22210195567458868\n",
      "Epoch 23/100: Loss: 0.4089119527488947\n",
      "Epoch 24/100: Loss: 0.45762230586260555\n",
      "Epoch 25/100: Loss: 0.35729703593533485\n",
      "Epoch 26/100: Loss: 0.34886995239648966\n",
      "Epoch 27/100: Loss: 0.264566476206528\n",
      "Epoch 28/100: Loss: 0.3068162021110766\n",
      "Epoch 29/100: Loss: 0.28258765226928517\n",
      "Epoch 30/100: Loss: 0.28125182845105884\n",
      "Epoch 31/100: Loss: 0.30981059168698266\n",
      "Epoch 32/100: Loss: 0.37211510239867496\n",
      "Epoch 33/100: Loss: 0.24648011680401397\n",
      "Epoch 34/100: Loss: 0.34832106085159464\n",
      "Epoch 35/100: Loss: 0.19562500734027707\n",
      "Epoch 36/100: Loss: 0.27581316758296454\n",
      "Epoch 37/100: Loss: 0.2890662923193304\n",
      "Epoch 38/100: Loss: 0.3015907866196358\n",
      "Epoch 39/100: Loss: 0.19726210092649127\n",
      "Epoch 40/100: Loss: 0.20283165083528729\n",
      "Epoch 41/100: Loss: 0.3050588048372447\n",
      "Epoch 42/100: Loss: 0.241489630954311\n",
      "Epoch 43/100: Loss: 0.2249602556519676\n",
      "Epoch 44/100: Loss: 0.27992409944999963\n",
      "Epoch 45/100: Loss: 0.300251454820318\n",
      "Epoch 46/100: Loss: 0.29734029889950764\n",
      "Epoch 47/100: Loss: 0.3184240777831292\n",
      "Epoch 48/100: Loss: 0.2673964428598993\n",
      "Epoch 49/100: Loss: 0.18493457697913981\n",
      "Epoch 50/100: Loss: 0.41257204236753753\n",
      "Epoch 51/100: Loss: 0.29165870499000446\n",
      "Epoch 52/100: Loss: 0.18945057257951703\n",
      "Epoch 53/100: Loss: 0.23956658940005582\n",
      "Epoch 54/100: Loss: 0.25217729676223827\n",
      "Epoch 55/100: Loss: 0.1896831183345057\n",
      "Epoch 56/100: Loss: 0.29072238872904566\n",
      "Epoch 57/100: Loss: 0.18643099219189024\n",
      "Epoch 58/100: Loss: 0.19375138595642055\n",
      "Epoch 59/100: Loss: 0.3107815411145566\n",
      "Epoch 60/100: Loss: 0.2376450828742236\n",
      "Epoch 61/100: Loss: 0.2808347377274913\n",
      "Epoch 62/100: Loss: 0.19194841674461713\n",
      "Epoch 63/100: Loss: 0.2700091768267157\n",
      "Epoch 64/100: Loss: 0.2815272832463961\n",
      "Epoch 65/100: Loss: 0.15357162594154944\n",
      "Epoch 66/100: Loss: 0.18461709397743106\n",
      "Epoch 67/100: Loss: 0.2261288631510979\n",
      "Epoch 68/100: Loss: 0.23034060629142913\n",
      "Epoch 69/100: Loss: 0.16481381233315914\n",
      "Epoch 70/100: Loss: 0.1689646005484974\n",
      "Epoch 71/100: Loss: 0.1363144236507651\n",
      "Epoch 72/100: Loss: 0.14485366993048956\n",
      "Epoch 73/100: Loss: 0.12018440428291796\n",
      "Epoch 74/100: Loss: 0.1431367570650764\n",
      "Epoch 75/100: Loss: 0.14323014105902984\n",
      "Epoch 76/100: Loss: 0.15532067732783617\n",
      "Epoch 77/100: Loss: 0.132535541134348\n",
      "Epoch 78/100: Loss: 0.09566528903669677\n",
      "Epoch 79/100: Loss: 0.10787097146167071\n",
      "Epoch 80/100: Loss: 0.11541150516713969\n",
      "Epoch 81/100: Loss: 0.1048990071285516\n",
      "Epoch 82/100: Loss: 0.05639342640060931\n",
      "Epoch 83/100: Loss: 0.043131199567142176\n",
      "Epoch 84/100: Loss: 0.12253763875814912\n",
      "Epoch 85/100: Loss: 0.08561101300874725\n",
      "Epoch 86/100: Loss: 0.0949398046839633\n",
      "Epoch 87/100: Loss: 0.07809825493604876\n",
      "Epoch 88/100: Loss: 0.05161296533733548\n",
      "Epoch 89/100: Loss: 0.06872634775500046\n",
      "Epoch 90/100: Loss: 0.09131629576877458\n",
      "Epoch 91/100: Loss: 0.049019742747077544\n",
      "Epoch 92/100: Loss: 0.04465057572815567\n",
      "Epoch 93/100: Loss: 0.07405135189401335\n",
      "Epoch 94/100: Loss: 0.03617397728812648\n",
      "Epoch 95/100: Loss: 0.05094956145694596\n",
      "Epoch 96/100: Loss: 0.05951485991408845\n",
      "Epoch 97/100: Loss: 0.03827783013985027\n",
      "Epoch 98/100: Loss: 0.050612480135896476\n",
      "Epoch 99/100: Loss: 0.024502370151458307\n",
      "Epoch 100/100: Loss: 0.028151463974791114\n",
      "time costs: 434.0707402229309\n",
      "--------------------\n",
      "training epoch: 4\n",
      "Epoch 1/100: Loss: 1.009349262714386\n",
      "Epoch 2/100: Loss: 1.009384623169899\n",
      "Epoch 3/100: Loss: 0.9744310498237609\n",
      "Epoch 4/100: Loss: 1.0044085144996644\n",
      "Epoch 5/100: Loss: 0.9892959982156754\n",
      "Epoch 6/100: Loss: 1.0074415355920792\n",
      "Epoch 7/100: Loss: 0.9910290271043778\n",
      "Epoch 8/100: Loss: 0.9894184112548828\n",
      "Epoch 9/100: Loss: 0.9522444248199463\n",
      "Epoch 10/100: Loss: 0.966798746585846\n",
      "Epoch 11/100: Loss: 0.8980692684650421\n",
      "Epoch 12/100: Loss: 0.9949198335409164\n",
      "Epoch 13/100: Loss: 0.924266254901886\n",
      "Epoch 14/100: Loss: 0.9415736019611358\n",
      "Epoch 15/100: Loss: 0.9231190592050552\n",
      "Epoch 16/100: Loss: 0.9095251977443695\n",
      "Epoch 17/100: Loss: 0.8407214701175689\n",
      "Epoch 18/100: Loss: 0.8589864164590836\n",
      "Epoch 19/100: Loss: 0.8209214359521866\n",
      "Epoch 20/100: Loss: 0.7326004117727279\n",
      "Epoch 21/100: Loss: 0.8207940846681595\n",
      "Epoch 22/100: Loss: 0.7345679223537445\n",
      "Epoch 23/100: Loss: 0.7149580329656601\n",
      "Epoch 24/100: Loss: 0.6498977780342102\n",
      "Epoch 25/100: Loss: 0.587642426788807\n",
      "Epoch 26/100: Loss: 0.5994289457798004\n",
      "Epoch 27/100: Loss: 0.5698075398802758\n",
      "Epoch 28/100: Loss: 0.5120899707078934\n",
      "Epoch 29/100: Loss: 0.488361531496048\n",
      "Epoch 30/100: Loss: 0.4530873388051987\n",
      "Epoch 31/100: Loss: 0.4327681377530098\n",
      "Epoch 32/100: Loss: 0.39279425591230394\n",
      "Epoch 33/100: Loss: 0.36070890724658966\n",
      "Epoch 34/100: Loss: 0.3738008961081505\n",
      "Epoch 35/100: Loss: 0.34051495641469953\n",
      "Epoch 36/100: Loss: 0.31080715730786324\n",
      "Epoch 37/100: Loss: 0.32175266072154046\n",
      "Epoch 38/100: Loss: 0.33164525032043457\n",
      "Epoch 39/100: Loss: 0.34769785925745966\n",
      "Epoch 40/100: Loss: 0.26828166618943217\n",
      "Epoch 41/100: Loss: 0.2724997512996197\n",
      "Epoch 42/100: Loss: 0.31711945198476316\n",
      "Epoch 43/100: Loss: 0.3284046921879053\n",
      "Epoch 44/100: Loss: 0.28222086243331435\n",
      "Epoch 45/100: Loss: 0.2943213127553463\n",
      "Epoch 46/100: Loss: 0.31651083640754224\n",
      "Epoch 47/100: Loss: 0.32093286588788034\n",
      "Epoch 48/100: Loss: 0.2873217348009348\n",
      "Epoch 49/100: Loss: 0.3031539559364319\n",
      "Epoch 50/100: Loss: 0.27368972897529603\n",
      "Epoch 51/100: Loss: 0.27421669475734234\n",
      "Epoch 52/100: Loss: 0.27315999679267405\n",
      "Epoch 53/100: Loss: 0.2939468428492546\n",
      "Epoch 54/100: Loss: 0.269679769128561\n",
      "Epoch 55/100: Loss: 0.2777704194188118\n",
      "Epoch 56/100: Loss: 0.22289883196353913\n",
      "Epoch 57/100: Loss: 0.27260772138834\n",
      "Epoch 58/100: Loss: 0.23940562829375267\n",
      "Epoch 59/100: Loss: 0.27786301225423815\n",
      "Epoch 60/100: Loss: 0.20202690064907075\n",
      "Epoch 61/100: Loss: 0.20944880321621895\n",
      "Epoch 62/100: Loss: 0.15884713195264338\n",
      "Epoch 63/100: Loss: 0.24330158531665802\n",
      "Epoch 64/100: Loss: 0.25703737773001195\n",
      "Epoch 65/100: Loss: 0.2501877140253782\n",
      "Epoch 66/100: Loss: 0.25224049501121043\n",
      "Epoch 67/100: Loss: 0.2529319077730179\n",
      "Epoch 68/100: Loss: 0.16075061000883578\n",
      "Epoch 69/100: Loss: 0.21218187026679516\n",
      "Epoch 70/100: Loss: 0.24636110588908194\n",
      "Epoch 71/100: Loss: 0.18791296035051347\n",
      "Epoch 72/100: Loss: 0.17045160438865423\n",
      "Epoch 73/100: Loss: 0.14130935370922088\n",
      "Epoch 74/100: Loss: 0.18024621959775686\n",
      "Epoch 75/100: Loss: 0.21415572464466096\n",
      "Epoch 76/100: Loss: 0.17698141783475876\n",
      "Epoch 77/100: Loss: 0.18014603815972804\n",
      "Epoch 78/100: Loss: 0.14963572863489388\n",
      "Epoch 79/100: Loss: 0.20038973577320576\n",
      "Epoch 80/100: Loss: 0.16223943987861275\n",
      "Epoch 81/100: Loss: 0.14540746808052063\n",
      "Epoch 82/100: Loss: 0.1225721649825573\n",
      "Epoch 83/100: Loss: 0.09267556667327881\n",
      "Epoch 84/100: Loss: 0.10377462115138769\n",
      "Epoch 85/100: Loss: 0.13622650783509016\n",
      "Epoch 86/100: Loss: 0.11686248378828168\n",
      "Epoch 87/100: Loss: 0.10355254542082548\n",
      "Epoch 88/100: Loss: 0.1387484919279814\n",
      "Epoch 89/100: Loss: 0.12106017176993192\n",
      "Epoch 90/100: Loss: 0.11471432959660888\n",
      "Epoch 91/100: Loss: 0.08034586925059557\n",
      "Epoch 92/100: Loss: 0.1182230954989791\n",
      "Epoch 93/100: Loss: 0.07185293268412352\n",
      "Epoch 94/100: Loss: 0.07843095511198044\n",
      "Epoch 95/100: Loss: 0.0820172842592001\n",
      "Epoch 96/100: Loss: 0.0654922179877758\n",
      "Epoch 97/100: Loss: 0.0724441010504961\n",
      "Epoch 98/100: Loss: 0.062073244620114564\n",
      "Epoch 99/100: Loss: 0.05696709454059601\n",
      "Epoch 100/100: Loss: 0.05101361502893269\n",
      "time costs: 466.92758679389954\n",
      "--------------------\n",
      "training epoch: 5\n",
      "Epoch 1/100: Loss: 1.0346887290477753\n",
      "Epoch 2/100: Loss: 1.0653013169765473\n",
      "Epoch 3/100: Loss: 0.9877937316894532\n",
      "Epoch 4/100: Loss: 0.9707340806722641\n",
      "Epoch 5/100: Loss: 0.9890121668577194\n",
      "Epoch 6/100: Loss: 0.9709673821926117\n",
      "Epoch 7/100: Loss: 1.0000980585813521\n",
      "Epoch 8/100: Loss: 0.971578323841095\n",
      "Epoch 9/100: Loss: 1.0015351593494415\n",
      "Epoch 10/100: Loss: 0.9578947186470032\n",
      "Epoch 11/100: Loss: 0.9543146073818207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: Loss: 0.9687863975763321\n",
      "Epoch 13/100: Loss: 0.9343685001134873\n",
      "Epoch 14/100: Loss: 0.9257214486598968\n",
      "Epoch 15/100: Loss: 0.9486228942871093\n",
      "Epoch 16/100: Loss: 0.9169527947902679\n",
      "Epoch 17/100: Loss: 0.921863904595375\n",
      "Epoch 18/100: Loss: 0.8857253670692444\n",
      "Epoch 19/100: Loss: 0.8848346143960952\n",
      "Epoch 20/100: Loss: 0.8722957044839859\n",
      "Epoch 21/100: Loss: 0.8656993538141251\n",
      "Epoch 22/100: Loss: 0.8445719569921494\n",
      "Epoch 23/100: Loss: 0.8290314555168152\n",
      "Epoch 24/100: Loss: 0.8228325933218003\n",
      "Epoch 25/100: Loss: 0.775630521774292\n",
      "Epoch 26/100: Loss: 0.7442798435688018\n",
      "Epoch 27/100: Loss: 0.7025513142347336\n",
      "Epoch 28/100: Loss: 0.70205118060112\n",
      "Epoch 29/100: Loss: 0.7865899503231049\n",
      "Epoch 30/100: Loss: 0.6313681989908219\n",
      "Epoch 31/100: Loss: 0.5994090527296067\n",
      "Epoch 32/100: Loss: 0.6666366145014763\n",
      "Epoch 33/100: Loss: 0.6071855545043945\n",
      "Epoch 34/100: Loss: 0.5766999959945679\n",
      "Epoch 35/100: Loss: 0.6530733332037926\n",
      "Epoch 36/100: Loss: 0.6032345369458199\n",
      "Epoch 37/100: Loss: 0.5312596708536148\n",
      "Epoch 38/100: Loss: 0.540705481171608\n",
      "Epoch 39/100: Loss: 0.411403726041317\n",
      "Epoch 40/100: Loss: 0.5271536439657212\n",
      "Epoch 41/100: Loss: 0.47081983685493467\n",
      "Epoch 42/100: Loss: 0.48150583952665327\n",
      "Epoch 43/100: Loss: 0.4920272469520569\n",
      "Epoch 44/100: Loss: 0.4512954086065292\n",
      "Epoch 45/100: Loss: 0.4881278082728386\n",
      "Epoch 46/100: Loss: 0.5106869913637638\n",
      "Epoch 47/100: Loss: 0.43757355138659476\n",
      "Epoch 48/100: Loss: 0.3596247941255569\n",
      "Epoch 49/100: Loss: 0.3936707966029644\n",
      "Epoch 50/100: Loss: 0.41125010773539544\n",
      "Epoch 51/100: Loss: 0.3024234354496002\n",
      "Epoch 52/100: Loss: 0.330167480558157\n",
      "Epoch 53/100: Loss: 0.26387816965579985\n",
      "Epoch 54/100: Loss: 0.3425299473106861\n",
      "Epoch 55/100: Loss: 0.38294146917760374\n",
      "Epoch 56/100: Loss: 0.29751884527504446\n",
      "Epoch 57/100: Loss: 0.3106768287718296\n",
      "Epoch 58/100: Loss: 0.3509352721273899\n",
      "Epoch 59/100: Loss: 0.2872245516628027\n",
      "Epoch 60/100: Loss: 0.28209336176514627\n",
      "Epoch 61/100: Loss: 0.3158026909455657\n",
      "Epoch 62/100: Loss: 0.24305158732458948\n",
      "Epoch 63/100: Loss: 0.32676428779959676\n",
      "Epoch 64/100: Loss: 0.27192416563630106\n",
      "Epoch 65/100: Loss: 0.2919201094657183\n",
      "Epoch 66/100: Loss: 0.23213106011971832\n",
      "Epoch 67/100: Loss: 0.3012754928320646\n",
      "Epoch 68/100: Loss: 0.19327998971566557\n",
      "Epoch 69/100: Loss: 0.2695318817626685\n",
      "Epoch 70/100: Loss: 0.20350269181653857\n",
      "Epoch 71/100: Loss: 0.2301825638860464\n",
      "Epoch 72/100: Loss: 0.2957226913422346\n",
      "Epoch 73/100: Loss: 0.23447538891341538\n",
      "Epoch 74/100: Loss: 0.2466731980908662\n",
      "Epoch 75/100: Loss: 0.17129670199938118\n",
      "Epoch 76/100: Loss: 0.24560984442941844\n",
      "Epoch 77/100: Loss: 0.15119335980853066\n",
      "Epoch 78/100: Loss: 0.1658463693340309\n",
      "Epoch 79/100: Loss: 0.18898578095249832\n",
      "Epoch 80/100: Loss: 0.20861594246234744\n",
      "Epoch 81/100: Loss: 0.1895792795578018\n",
      "Epoch 82/100: Loss: 0.26186778396368027\n",
      "Epoch 83/100: Loss: 0.11783678776118904\n",
      "Epoch 84/100: Loss: 0.2365560087841004\n",
      "Epoch 85/100: Loss: 0.08565172436647117\n",
      "Epoch 86/100: Loss: 0.18290296355262398\n",
      "Epoch 87/100: Loss: 0.11547696492634714\n",
      "Epoch 88/100: Loss: 0.10103892632760107\n",
      "Epoch 89/100: Loss: 0.14658506708219649\n",
      "Epoch 90/100: Loss: 0.1131731196976034\n",
      "Epoch 91/100: Loss: 0.0949226387863746\n",
      "Epoch 92/100: Loss: 0.09925522914854809\n",
      "Epoch 93/100: Loss: 0.08157109192106873\n",
      "Epoch 94/100: Loss: 0.09909623092971742\n",
      "Epoch 95/100: Loss: 0.09092788958805613\n",
      "Epoch 96/100: Loss: 0.10306199867336545\n",
      "Epoch 97/100: Loss: 0.11203509550541639\n",
      "Epoch 98/100: Loss: 0.08377404028869932\n",
      "Epoch 99/100: Loss: 0.0807006797487702\n",
      "Epoch 100/100: Loss: 0.04807820663845632\n",
      "time costs: 451.30697298049927\n",
      "--------------------\n",
      "training epoch: 6\n",
      "Epoch 1/100: Loss: 1.0485079050064088\n",
      "Epoch 2/100: Loss: 0.993473681807518\n",
      "Epoch 3/100: Loss: 0.9981314301490783\n",
      "Epoch 4/100: Loss: 0.9832644641399384\n",
      "Epoch 5/100: Loss: 0.9410917609930038\n",
      "Epoch 6/100: Loss: 0.9489429324865342\n",
      "Epoch 7/100: Loss: 0.9272608637809754\n",
      "Epoch 8/100: Loss: 0.9091572523117065\n",
      "Epoch 9/100: Loss: 0.9254706472158432\n",
      "Epoch 10/100: Loss: 0.8602654844522476\n",
      "Epoch 11/100: Loss: 0.822899454832077\n",
      "Epoch 12/100: Loss: 0.780543115735054\n",
      "Epoch 13/100: Loss: 0.7551508247852325\n",
      "Epoch 14/100: Loss: 0.6919988244771957\n",
      "Epoch 15/100: Loss: 0.706500843167305\n",
      "Epoch 16/100: Loss: 0.6443316698074341\n",
      "Epoch 17/100: Loss: 0.6581671848893166\n",
      "Epoch 18/100: Loss: 0.5823506444692612\n",
      "Epoch 19/100: Loss: 0.4793290808796883\n",
      "Epoch 20/100: Loss: 0.5457757040858269\n",
      "Epoch 21/100: Loss: 0.6039571978151799\n",
      "Epoch 22/100: Loss: 0.567710043489933\n",
      "Epoch 23/100: Loss: 0.5982959605753422\n",
      "Epoch 24/100: Loss: 0.5668040804564953\n",
      "Epoch 25/100: Loss: 0.5653328880667686\n",
      "Epoch 26/100: Loss: 0.4901438675820827\n",
      "Epoch 27/100: Loss: 0.6024854525923728\n",
      "Epoch 28/100: Loss: 0.5092439323663711\n",
      "Epoch 29/100: Loss: 0.515902130305767\n",
      "Epoch 30/100: Loss: 0.47964743971824647\n",
      "Epoch 31/100: Loss: 0.5473290607333183\n",
      "Epoch 32/100: Loss: 0.47944328337907793\n",
      "Epoch 33/100: Loss: 0.4635101333260536\n",
      "Epoch 34/100: Loss: 0.43369793742895124\n",
      "Epoch 35/100: Loss: 0.48476689085364344\n",
      "Epoch 36/100: Loss: 0.36987819373607633\n",
      "Epoch 37/100: Loss: 0.47675304114818573\n",
      "Epoch 38/100: Loss: 0.45914302915334704\n",
      "Epoch 39/100: Loss: 0.44310553669929503\n",
      "Epoch 40/100: Loss: 0.3982497066259384\n",
      "Epoch 41/100: Loss: 0.3841328926384449\n",
      "Epoch 42/100: Loss: 0.40703864470124246\n",
      "Epoch 43/100: Loss: 0.32315568402409556\n",
      "Epoch 44/100: Loss: 0.31476296335458753\n",
      "Epoch 45/100: Loss: 0.34820234328508376\n",
      "Epoch 46/100: Loss: 0.36223027259111407\n",
      "Epoch 47/100: Loss: 0.3172789990901947\n",
      "Epoch 48/100: Loss: 0.3812929131090641\n",
      "Epoch 49/100: Loss: 0.26825002282857896\n",
      "Epoch 50/100: Loss: 0.2565513554960489\n",
      "Epoch 51/100: Loss: 0.3336351875215769\n",
      "Epoch 52/100: Loss: 0.3193540494889021\n",
      "Epoch 53/100: Loss: 0.3136461965739727\n",
      "Epoch 54/100: Loss: 0.2878524478524923\n",
      "Epoch 55/100: Loss: 0.3784244619309902\n",
      "Epoch 56/100: Loss: 0.32449189238250253\n",
      "Epoch 57/100: Loss: 0.26844968236982825\n",
      "Epoch 58/100: Loss: 0.31955373883247373\n",
      "Epoch 59/100: Loss: 0.26652696654200553\n",
      "Epoch 60/100: Loss: 0.2245661536231637\n",
      "Epoch 61/100: Loss: 0.2415991960093379\n",
      "Epoch 62/100: Loss: 0.2677416458725929\n",
      "Epoch 63/100: Loss: 0.23664678931236266\n",
      "Epoch 64/100: Loss: 0.286919466778636\n",
      "Epoch 65/100: Loss: 0.25840270724147557\n",
      "Epoch 66/100: Loss: 0.2199909733608365\n",
      "Epoch 67/100: Loss: 0.2927853113040328\n",
      "Epoch 68/100: Loss: 0.24771649185568095\n",
      "Epoch 69/100: Loss: 0.2100365338847041\n",
      "Epoch 70/100: Loss: 0.22397113237529992\n",
      "Epoch 71/100: Loss: 0.1825480113737285\n",
      "Epoch 72/100: Loss: 0.24104806073009968\n",
      "Epoch 73/100: Loss: 0.23589694928377866\n",
      "Epoch 74/100: Loss: 0.1846823249012232\n",
      "Epoch 75/100: Loss: 0.34437202718108895\n",
      "Epoch 76/100: Loss: 0.251815774384886\n",
      "Epoch 77/100: Loss: 0.21279636630788445\n",
      "Epoch 78/100: Loss: 0.1914601161144674\n",
      "Epoch 79/100: Loss: 0.23819442689418793\n",
      "Epoch 80/100: Loss: 0.23052297569811345\n",
      "Epoch 81/100: Loss: 0.23437933772802352\n",
      "Epoch 82/100: Loss: 0.23704415690153838\n",
      "Epoch 83/100: Loss: 0.21177481729537248\n",
      "Epoch 84/100: Loss: 0.21677087265998124\n",
      "Epoch 85/100: Loss: 0.22820714376866819\n",
      "Epoch 86/100: Loss: 0.16187123665586114\n",
      "Epoch 87/100: Loss: 0.18655620021745561\n",
      "Epoch 88/100: Loss: 0.17860836209729314\n",
      "Epoch 89/100: Loss: 0.18330124635249376\n",
      "Epoch 90/100: Loss: 0.20659725964069367\n",
      "Epoch 91/100: Loss: 0.1952032834291458\n",
      "Epoch 92/100: Loss: 0.1681011827662587\n",
      "Epoch 93/100: Loss: 0.17724389471113683\n",
      "Epoch 94/100: Loss: 0.17510828636586667\n",
      "Epoch 95/100: Loss: 0.2212195247411728\n",
      "Epoch 96/100: Loss: 0.16788885202258824\n",
      "Epoch 97/100: Loss: 0.1997978262603283\n",
      "Epoch 98/100: Loss: 0.15903999842703342\n",
      "Epoch 99/100: Loss: 0.16603668853640557\n",
      "Epoch 100/100: Loss: 0.151324375346303\n",
      "time costs: 410.5718104839325\n",
      "--------------------\n",
      "training epoch: 7\n",
      "Epoch 1/100: Loss: 1.028464013338089\n",
      "Epoch 2/100: Loss: 0.9949123054742813\n",
      "Epoch 3/100: Loss: 1.0098994493484497\n",
      "Epoch 4/100: Loss: 0.9843623071908951\n",
      "Epoch 5/100: Loss: 0.997042390704155\n",
      "Epoch 6/100: Loss: 0.998865681886673\n",
      "Epoch 7/100: Loss: 1.0278473764657974\n",
      "Epoch 8/100: Loss: 0.9937688142061234\n",
      "Epoch 9/100: Loss: 0.9884529769420624\n",
      "Epoch 10/100: Loss: 0.9826381802558899\n",
      "Epoch 11/100: Loss: 0.9735731899738311\n",
      "Epoch 12/100: Loss: 0.9677482366561889\n",
      "Epoch 13/100: Loss: 0.968475952744484\n",
      "Epoch 14/100: Loss: 0.9580939292907715\n",
      "Epoch 15/100: Loss: 0.9756876856088639\n",
      "Epoch 16/100: Loss: 0.9374127447605133\n",
      "Epoch 17/100: Loss: 0.9364502936601639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: Loss: 0.9309574335813522\n",
      "Epoch 19/100: Loss: 0.9225157022476196\n",
      "Epoch 20/100: Loss: 0.9155684590339661\n",
      "Epoch 21/100: Loss: 0.8849907755851746\n",
      "Epoch 22/100: Loss: 0.9045523285865784\n",
      "Epoch 23/100: Loss: 0.8521387666463852\n",
      "Epoch 24/100: Loss: 0.8366840153932571\n",
      "Epoch 25/100: Loss: 0.8238149672746659\n",
      "Epoch 26/100: Loss: 0.7924129456281662\n",
      "Epoch 27/100: Loss: 0.7525107502937317\n",
      "Epoch 28/100: Loss: 0.7516537755727768\n",
      "Epoch 29/100: Loss: 0.7706861048936844\n",
      "Epoch 30/100: Loss: 0.7967979818582535\n",
      "Epoch 31/100: Loss: 0.7047543674707413\n",
      "Epoch 32/100: Loss: 0.6882164970040321\n",
      "Epoch 33/100: Loss: 0.6233281880617142\n",
      "Epoch 34/100: Loss: 0.5033831760287285\n",
      "Epoch 35/100: Loss: 0.5184120588004589\n",
      "Epoch 36/100: Loss: 0.46446642503142355\n",
      "Epoch 37/100: Loss: 0.6201682724058628\n",
      "Epoch 38/100: Loss: 0.5018773667514325\n",
      "Epoch 39/100: Loss: 0.49761471524834633\n",
      "Epoch 40/100: Loss: 0.35324239209294317\n",
      "Epoch 41/100: Loss: 0.43494409807026385\n",
      "Epoch 42/100: Loss: 0.47332684211432935\n",
      "Epoch 43/100: Loss: 0.1995594918727875\n",
      "Epoch 44/100: Loss: 0.45246972143650055\n",
      "Epoch 45/100: Loss: 0.46312507167458533\n",
      "Epoch 46/100: Loss: 0.2696462616324425\n",
      "Epoch 47/100: Loss: 0.44048492778092624\n",
      "Epoch 48/100: Loss: 0.341453249938786\n",
      "Epoch 49/100: Loss: 0.40677080675959587\n",
      "Epoch 50/100: Loss: 0.3470741789788008\n",
      "Epoch 51/100: Loss: 0.22512060264125466\n",
      "Epoch 52/100: Loss: 0.29050060380250214\n",
      "Epoch 53/100: Loss: 0.21840118221007288\n",
      "Epoch 54/100: Loss: 0.3014293707557954\n",
      "Epoch 55/100: Loss: 0.24923955195117742\n",
      "Epoch 56/100: Loss: 0.26534327045083045\n",
      "Epoch 57/100: Loss: 0.31805132038425654\n",
      "Epoch 58/100: Loss: 0.26750019058818\n",
      "Epoch 59/100: Loss: 0.1651732907339465\n",
      "Epoch 60/100: Loss: 0.22908470987458712\n",
      "Epoch 61/100: Loss: 0.19800374588230624\n",
      "Epoch 62/100: Loss: 0.292755683332507\n",
      "Epoch 63/100: Loss: 0.2749063702765852\n",
      "Epoch 64/100: Loss: 0.33456873733666725\n",
      "Epoch 65/100: Loss: 0.23857503802282737\n",
      "Epoch 66/100: Loss: 0.11037960810936057\n",
      "Epoch 67/100: Loss: 0.3310418828041293\n",
      "Epoch 68/100: Loss: 0.26424629712710157\n",
      "Epoch 69/100: Loss: 0.18521642658479323\n",
      "Epoch 70/100: Loss: 0.2352332139678765\n",
      "Epoch 71/100: Loss: 0.22269673572154716\n",
      "Epoch 72/100: Loss: 0.20271309863819625\n",
      "Epoch 73/100: Loss: 0.2545782273315126\n",
      "Epoch 74/100: Loss: 0.21991864258015995\n",
      "Epoch 75/100: Loss: 0.18690282526804367\n",
      "Epoch 76/100: Loss: 0.23810565362509806\n",
      "Epoch 77/100: Loss: 0.17233192317653448\n",
      "Epoch 78/100: Loss: 0.18035202979517634\n",
      "Epoch 79/100: Loss: 0.22195142552554897\n",
      "Epoch 80/100: Loss: 0.14730308662401512\n",
      "Epoch 81/100: Loss: 0.16846398740672158\n",
      "Epoch 82/100: Loss: 0.22996534369958682\n",
      "Epoch 83/100: Loss: 0.19680090177280363\n",
      "Epoch 84/100: Loss: 0.09033613362626056\n",
      "Epoch 85/100: Loss: 0.1261951793821936\n",
      "Epoch 86/100: Loss: 0.1757904508995125\n",
      "Epoch 87/100: Loss: 0.16345448541178484\n",
      "Epoch 88/100: Loss: 0.13894088589804596\n",
      "Epoch 89/100: Loss: 0.20954821892082692\n",
      "Epoch 90/100: Loss: 0.16259551506227582\n",
      "Epoch 91/100: Loss: 0.13961083134054206\n",
      "Epoch 92/100: Loss: 0.14483640340622514\n",
      "Epoch 93/100: Loss: 0.11110480079078115\n",
      "Epoch 94/100: Loss: 0.1354612021423236\n",
      "Epoch 95/100: Loss: 0.16400381407001988\n",
      "Epoch 96/100: Loss: 0.06749093349608301\n",
      "Epoch 97/100: Loss: 0.1365484658832429\n",
      "Epoch 98/100: Loss: 0.14983314591227098\n",
      "Epoch 99/100: Loss: 0.1590144800429698\n",
      "Epoch 100/100: Loss: 0.09465626985183917\n",
      "time costs: 390.04934072494507\n",
      "--------------------\n",
      "training epoch: 8\n",
      "Epoch 1/100: Loss: 0.9642958581447602\n",
      "Epoch 2/100: Loss: 0.9475048631429672\n",
      "Epoch 3/100: Loss: 1.0630454063415526\n",
      "Epoch 4/100: Loss: 0.9581042617559433\n",
      "Epoch 5/100: Loss: 0.9668579429388047\n",
      "Epoch 6/100: Loss: 0.9887705981731415\n",
      "Epoch 7/100: Loss: 0.9920826435089112\n",
      "Epoch 8/100: Loss: 1.0310901254415512\n",
      "Epoch 9/100: Loss: 0.9919750958681106\n",
      "Epoch 10/100: Loss: 1.0253972560167313\n",
      "Epoch 11/100: Loss: 1.0173434346914292\n",
      "Epoch 12/100: Loss: 0.9702568471431732\n",
      "Epoch 13/100: Loss: 0.953986006975174\n",
      "Epoch 14/100: Loss: 1.0081606596708297\n",
      "Epoch 15/100: Loss: 0.9589496642351151\n",
      "Epoch 16/100: Loss: 0.9860385894775391\n",
      "Epoch 17/100: Loss: 0.962541276216507\n",
      "Epoch 18/100: Loss: 0.9723152905702591\n",
      "Epoch 19/100: Loss: 0.9490076720714569\n",
      "Epoch 20/100: Loss: 0.9321165293455124\n",
      "Epoch 21/100: Loss: 0.939533707499504\n",
      "Epoch 22/100: Loss: 0.9527944177389145\n",
      "Epoch 23/100: Loss: 0.9007624626159668\n",
      "Epoch 24/100: Loss: 0.9285054475069046\n",
      "Epoch 25/100: Loss: 0.8968235999345779\n",
      "Epoch 26/100: Loss: 0.8819592654705047\n",
      "Epoch 27/100: Loss: 0.8328702688217163\n",
      "Epoch 28/100: Loss: 0.8409333944320678\n",
      "Epoch 29/100: Loss: 0.8577953398227691\n",
      "Epoch 30/100: Loss: 0.8364040702581406\n",
      "Epoch 31/100: Loss: 0.7962705194950104\n",
      "Epoch 32/100: Loss: 0.8027833282947541\n",
      "Epoch 33/100: Loss: 0.7873436838388443\n",
      "Epoch 34/100: Loss: 0.7883061975240707\n",
      "Epoch 35/100: Loss: 0.7682395964860916\n",
      "Epoch 36/100: Loss: 0.7470057755708694\n",
      "Epoch 37/100: Loss: 0.7139610707759857\n",
      "Epoch 38/100: Loss: 0.6738903164863587\n",
      "Epoch 39/100: Loss: 0.7204588443040848\n",
      "Epoch 40/100: Loss: 0.6670078217983246\n",
      "Epoch 41/100: Loss: 0.6300728291273117\n",
      "Epoch 42/100: Loss: 0.6644722118973732\n",
      "Epoch 43/100: Loss: 0.6694108679890632\n",
      "Epoch 44/100: Loss: 0.5947400659322739\n",
      "Epoch 45/100: Loss: 0.6514230251312256\n",
      "Epoch 46/100: Loss: 0.6018738120794296\n",
      "Epoch 47/100: Loss: 0.6376609832048417\n",
      "Epoch 48/100: Loss: 0.6453353434801101\n",
      "Epoch 49/100: Loss: 0.5799158588051796\n",
      "Epoch 50/100: Loss: 0.5964912414550781\n",
      "Epoch 51/100: Loss: 0.6520976901054383\n",
      "Epoch 52/100: Loss: 0.619759663939476\n",
      "Epoch 53/100: Loss: 0.5778363198041916\n",
      "Epoch 54/100: Loss: 0.5380261734127998\n",
      "Epoch 55/100: Loss: 0.5534839689731598\n",
      "Epoch 56/100: Loss: 0.5986557185649872\n",
      "Epoch 57/100: Loss: 0.5462695509195328\n",
      "Epoch 58/100: Loss: 0.5996285140514374\n",
      "Epoch 59/100: Loss: 0.5691667586565018\n",
      "Epoch 60/100: Loss: 0.5295110359787941\n",
      "Epoch 61/100: Loss: 0.5468048974871635\n",
      "Epoch 62/100: Loss: 0.5847147554159164\n",
      "Epoch 63/100: Loss: 0.5266649559140205\n",
      "Epoch 64/100: Loss: 0.5785170391201973\n",
      "Epoch 65/100: Loss: 0.5373359069228172\n",
      "Epoch 66/100: Loss: 0.5607001140713692\n",
      "Epoch 67/100: Loss: 0.5870468869805336\n",
      "Epoch 68/100: Loss: 0.5350965484976768\n",
      "Epoch 69/100: Loss: 0.49023725241422655\n",
      "Epoch 70/100: Loss: 0.5537605553865432\n",
      "Epoch 71/100: Loss: 0.480630598962307\n",
      "Epoch 72/100: Loss: 0.4782426320016384\n",
      "Epoch 73/100: Loss: 0.46768788024783137\n",
      "Epoch 74/100: Loss: 0.5551036208868027\n",
      "Epoch 75/100: Loss: 0.42529002130031585\n",
      "Epoch 76/100: Loss: 0.47808932289481165\n",
      "Epoch 77/100: Loss: 0.47106800228357315\n",
      "Epoch 78/100: Loss: 0.4283947393298149\n",
      "Epoch 79/100: Loss: 0.4038303501904011\n",
      "Epoch 80/100: Loss: 0.48475185930728915\n",
      "Epoch 81/100: Loss: 0.45373350456357003\n",
      "Epoch 82/100: Loss: 0.4020296946167946\n",
      "Epoch 83/100: Loss: 0.4245707508176565\n",
      "Epoch 84/100: Loss: 0.37801344245672225\n",
      "Epoch 85/100: Loss: 0.4318394772708416\n",
      "Epoch 86/100: Loss: 0.4274416245520115\n",
      "Epoch 87/100: Loss: 0.42651014439761636\n",
      "Epoch 88/100: Loss: 0.3568734500557184\n",
      "Epoch 89/100: Loss: 0.40155733972787855\n",
      "Epoch 90/100: Loss: 0.3086589198559523\n",
      "Epoch 91/100: Loss: 0.27468458097428083\n",
      "Epoch 92/100: Loss: 0.40836196690797805\n",
      "Epoch 93/100: Loss: 0.4585028920322657\n",
      "Epoch 94/100: Loss: 0.35135077126324177\n",
      "Epoch 95/100: Loss: 0.27461399184539914\n",
      "Epoch 96/100: Loss: 0.2910708745010197\n",
      "Epoch 97/100: Loss: 0.39789262786507607\n",
      "Epoch 98/100: Loss: 0.2814131846651435\n",
      "Epoch 99/100: Loss: 0.3414499267935753\n",
      "Epoch 100/100: Loss: 0.2577883694320917\n",
      "time costs: 442.02608704566956\n",
      "--------------------\n",
      "training epoch: 9\n",
      "Epoch 1/100: Loss: 1.0197260141372682\n",
      "Epoch 2/100: Loss: 0.9996084958314896\n",
      "Epoch 3/100: Loss: 0.989922720193863\n",
      "Epoch 4/100: Loss: 0.9925287872552871\n",
      "Epoch 5/100: Loss: 1.0098641902208327\n",
      "Epoch 6/100: Loss: 0.9741786926984787\n",
      "Epoch 7/100: Loss: 1.0108608305454254\n",
      "Epoch 8/100: Loss: 1.003933745622635\n",
      "Epoch 9/100: Loss: 0.9791230291128159\n",
      "Epoch 10/100: Loss: 0.9856334239244461\n",
      "Epoch 11/100: Loss: 0.9820031940937042\n",
      "Epoch 12/100: Loss: 0.9831449240446091\n",
      "Epoch 13/100: Loss: 0.9677185148000718\n",
      "Epoch 14/100: Loss: 0.9831107676029205\n",
      "Epoch 15/100: Loss: 0.9519194960594177\n",
      "Epoch 16/100: Loss: 0.95561203956604\n",
      "Epoch 17/100: Loss: 0.9394882917404175\n",
      "Epoch 18/100: Loss: 0.9349158555269241\n",
      "Epoch 19/100: Loss: 0.9314135104417801\n",
      "Epoch 20/100: Loss: 0.9174224257469177\n",
      "Epoch 21/100: Loss: 0.9197847038507462\n",
      "Epoch 22/100: Loss: 0.8920094996690751\n",
      "Epoch 23/100: Loss: 0.8823737233877182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: Loss: 0.876542928814888\n",
      "Epoch 25/100: Loss: 0.8463895887136459\n",
      "Epoch 26/100: Loss: 0.8517550587654114\n",
      "Epoch 27/100: Loss: 0.8165343761444092\n",
      "Epoch 28/100: Loss: 0.8117587596178055\n",
      "Epoch 29/100: Loss: 0.7886178910732269\n",
      "Epoch 30/100: Loss: 0.7554219722747803\n",
      "Epoch 31/100: Loss: 0.7488974124193192\n",
      "Epoch 32/100: Loss: 0.7562190473079682\n",
      "Epoch 33/100: Loss: 0.7705389708280563\n",
      "Epoch 34/100: Loss: 0.7047011524438858\n",
      "Epoch 35/100: Loss: 0.6974719196557999\n",
      "Epoch 36/100: Loss: 0.7195059686899186\n",
      "Epoch 37/100: Loss: 0.6674066632986069\n",
      "Epoch 38/100: Loss: 0.6591157972812652\n",
      "Epoch 39/100: Loss: 0.6283283799886703\n",
      "Epoch 40/100: Loss: 0.5803967162966728\n",
      "Epoch 41/100: Loss: 0.6224093914031983\n",
      "Epoch 42/100: Loss: 0.613498330116272\n",
      "Epoch 43/100: Loss: 0.6033291414380073\n",
      "Epoch 44/100: Loss: 0.6258807346224785\n",
      "Epoch 45/100: Loss: 0.5247507363557815\n",
      "Epoch 46/100: Loss: 0.516208304464817\n",
      "Epoch 47/100: Loss: 0.504730124771595\n",
      "Epoch 48/100: Loss: 0.5480775624513626\n",
      "Epoch 49/100: Loss: 0.5300371021032333\n",
      "Epoch 50/100: Loss: 0.5126065671443939\n",
      "Epoch 51/100: Loss: 0.48597140833735464\n",
      "Epoch 52/100: Loss: 0.48360545188188553\n",
      "Epoch 53/100: Loss: 0.4696354538202286\n",
      "Epoch 54/100: Loss: 0.45122532844543456\n",
      "Epoch 55/100: Loss: 0.46386397406458857\n",
      "Epoch 56/100: Loss: 0.37368585988879205\n",
      "Epoch 57/100: Loss: 0.3585155352950096\n",
      "Epoch 58/100: Loss: 0.5120120976120234\n",
      "Epoch 59/100: Loss: 0.27864602506160735\n",
      "Epoch 60/100: Loss: 0.49898474253714087\n",
      "Epoch 61/100: Loss: 0.44209291562438013\n",
      "Epoch 62/100: Loss: 0.4001794129610062\n",
      "Epoch 63/100: Loss: 0.4387575939297676\n",
      "Epoch 64/100: Loss: 0.3704967826604843\n",
      "Epoch 65/100: Loss: 0.4057384442538023\n",
      "Epoch 66/100: Loss: 0.33872546143829824\n",
      "Epoch 67/100: Loss: 0.3165210569277406\n",
      "Epoch 68/100: Loss: 0.453588267788291\n",
      "Epoch 69/100: Loss: 0.345628878287971\n",
      "Epoch 70/100: Loss: 0.30982302594929934\n",
      "Epoch 71/100: Loss: 0.35448884647339585\n",
      "Epoch 72/100: Loss: 0.24524726178497075\n",
      "Epoch 73/100: Loss: 0.2674385640770197\n",
      "Epoch 74/100: Loss: 0.3953195885755122\n",
      "Epoch 75/100: Loss: 0.20000613071024417\n",
      "Epoch 76/100: Loss: 0.3660463555715978\n",
      "Epoch 77/100: Loss: 0.2628063020296395\n",
      "Epoch 78/100: Loss: 0.24042338021099569\n",
      "Epoch 79/100: Loss: 0.3143051981460303\n",
      "Epoch 80/100: Loss: 0.3809690023306757\n",
      "Epoch 81/100: Loss: 0.28835378992371263\n",
      "Epoch 82/100: Loss: 0.2556770752649754\n",
      "Epoch 83/100: Loss: 0.26116797315189616\n",
      "Epoch 84/100: Loss: 0.29100845529465\n",
      "Epoch 85/100: Loss: 0.18820196292363106\n",
      "Epoch 86/100: Loss: 0.2541930535342544\n",
      "Epoch 87/100: Loss: 0.23807884394191206\n",
      "Epoch 88/100: Loss: 0.3096977712353691\n",
      "Epoch 89/100: Loss: 0.2090700626547914\n",
      "Epoch 90/100: Loss: 0.3177090520504862\n",
      "Epoch 91/100: Loss: 0.3020528368651867\n",
      "Epoch 92/100: Loss: 0.19118994947057216\n",
      "Epoch 93/100: Loss: 0.356855126330629\n",
      "Epoch 94/100: Loss: 0.2649165185110178\n",
      "Epoch 95/100: Loss: 0.2513157508117729\n",
      "Epoch 96/100: Loss: 0.16750337611592842\n",
      "Epoch 97/100: Loss: 0.20272669750847855\n",
      "Epoch 98/100: Loss: 0.33688760251388883\n",
      "Epoch 99/100: Loss: 0.30224930862896143\n",
      "Epoch 100/100: Loss: 0.2039562536374433\n",
      "time costs: 635.5036809444427\n",
      "--------------------\n",
      "training epoch: 10\n",
      "Epoch 1/100: Loss: 0.9775971353054047\n",
      "Epoch 2/100: Loss: 0.994311335682869\n",
      "Epoch 3/100: Loss: 0.9798936247825623\n",
      "Epoch 4/100: Loss: 1.0365844428539277\n",
      "Epoch 5/100: Loss: 0.968213415145874\n",
      "Epoch 6/100: Loss: 1.0629570454359054\n",
      "Epoch 7/100: Loss: 0.9388745814561844\n",
      "Epoch 8/100: Loss: 0.9363165110349655\n",
      "Epoch 9/100: Loss: 0.9267139494419098\n",
      "Epoch 10/100: Loss: 1.0068576514720917\n",
      "Epoch 11/100: Loss: 0.9581874996423722\n",
      "Epoch 12/100: Loss: 0.9271105349063873\n",
      "Epoch 13/100: Loss: 0.8809588998556137\n",
      "Epoch 14/100: Loss: 0.8637793511152267\n",
      "Epoch 15/100: Loss: 0.8334350943565368\n",
      "Epoch 16/100: Loss: 0.804240271449089\n",
      "Epoch 17/100: Loss: 0.771682295203209\n",
      "Epoch 18/100: Loss: 0.7090929031372071\n",
      "Epoch 19/100: Loss: 0.6856981173157692\n",
      "Epoch 20/100: Loss: 0.6159230530261993\n",
      "Epoch 21/100: Loss: 0.6154549822211266\n",
      "Epoch 22/100: Loss: 0.5243775315582753\n",
      "Epoch 23/100: Loss: 0.5570035085082055\n",
      "Epoch 24/100: Loss: 0.47236606329679487\n",
      "Epoch 25/100: Loss: 0.5270093221217393\n",
      "Epoch 26/100: Loss: 0.4667883783578873\n",
      "Epoch 27/100: Loss: 0.4082775194197893\n",
      "Epoch 28/100: Loss: 0.4012109011411667\n",
      "Epoch 29/100: Loss: 0.36613231636583804\n",
      "Epoch 30/100: Loss: 0.3278399160131812\n",
      "Epoch 31/100: Loss: 0.44562998358160255\n",
      "Epoch 32/100: Loss: 0.1431604907847941\n",
      "Epoch 33/100: Loss: 0.35122093530371784\n",
      "Epoch 34/100: Loss: 0.3275457206182182\n",
      "Epoch 35/100: Loss: 0.44285146379843354\n",
      "Epoch 36/100: Loss: 0.1850501473993063\n",
      "Epoch 37/100: Loss: 0.2482277013361454\n",
      "Epoch 38/100: Loss: 0.2806216974975541\n",
      "Epoch 39/100: Loss: 0.29725375602720305\n",
      "Epoch 40/100: Loss: 0.3510274567117449\n",
      "Epoch 41/100: Loss: 0.36112904592882844\n",
      "Epoch 42/100: Loss: 0.3006669076276012\n",
      "Epoch 43/100: Loss: 0.28051635129377245\n",
      "Epoch 44/100: Loss: 0.2562295964104123\n",
      "Epoch 45/100: Loss: 0.4020229479276168\n",
      "Epoch 46/100: Loss: 0.2592551726673264\n",
      "Epoch 47/100: Loss: 0.13826278056949376\n",
      "Epoch 48/100: Loss: 0.28927007177844644\n",
      "Epoch 49/100: Loss: 0.31045369235798714\n",
      "Epoch 50/100: Loss: 0.18983260792447254\n",
      "Epoch 51/100: Loss: 0.18653138577356004\n",
      "Epoch 52/100: Loss: 0.2184566662472207\n",
      "Epoch 53/100: Loss: 0.26707656574435534\n",
      "Epoch 54/100: Loss: 0.30730387268122283\n",
      "Epoch 55/100: Loss: 0.29717230412061324\n",
      "Epoch 56/100: Loss: 0.24617482645262498\n",
      "Epoch 57/100: Loss: 0.15203388388035818\n",
      "Epoch 58/100: Loss: 0.22859750251518562\n",
      "Epoch 59/100: Loss: 0.2619426316465251\n",
      "Epoch 60/100: Loss: 0.26251493418531024\n",
      "Epoch 61/100: Loss: 0.18328937726982986\n",
      "Epoch 62/100: Loss: 0.18495271634601523\n",
      "Epoch 63/100: Loss: 0.11413664047795465\n",
      "Epoch 64/100: Loss: 0.17740626941231313\n",
      "Epoch 65/100: Loss: 0.15306538727600127\n",
      "Epoch 66/100: Loss: 0.15604121818323619\n",
      "Epoch 67/100: Loss: 0.1995846289821202\n",
      "Epoch 68/100: Loss: 0.13603809221240226\n",
      "Epoch 69/100: Loss: 0.1531710193492472\n",
      "Epoch 70/100: Loss: 0.14499672183883378\n",
      "Epoch 71/100: Loss: 0.13371367850195384\n",
      "Epoch 72/100: Loss: 0.1186818073503673\n",
      "Epoch 73/100: Loss: 0.11359705948270857\n",
      "Epoch 74/100: Loss: 0.09804343437426724\n",
      "Epoch 75/100: Loss: 0.08084659665182699\n",
      "Epoch 76/100: Loss: 0.0711725747998571\n",
      "Epoch 77/100: Loss: 0.07544313043763395\n",
      "Epoch 78/100: Loss: 0.09531039885914652\n",
      "Epoch 79/100: Loss: 0.10238105781318155\n",
      "Epoch 80/100: Loss: 0.04473312993068248\n",
      "Epoch 81/100: Loss: 0.08340471744304523\n",
      "Epoch 82/100: Loss: 0.0664260662626475\n",
      "Epoch 83/100: Loss: 0.08578807915255311\n",
      "Epoch 84/100: Loss: 0.03505501979252586\n",
      "Epoch 85/100: Loss: 0.06507211632560939\n",
      "Epoch 86/100: Loss: 0.04665755413716397\n",
      "Epoch 87/100: Loss: 0.04179770981063484\n",
      "Epoch 88/100: Loss: 0.044278182974085215\n",
      "Epoch 89/100: Loss: 0.045383529824903235\n",
      "Epoch 90/100: Loss: 0.03330064492765814\n",
      "Epoch 91/100: Loss: 0.02099249705497641\n",
      "Epoch 92/100: Loss: 0.04164835563569795\n",
      "Epoch 93/100: Loss: 0.03276532511808909\n",
      "Epoch 94/100: Loss: 0.034087060927413405\n",
      "Epoch 95/100: Loss: 0.017858992132096317\n",
      "Epoch 96/100: Loss: 0.02075873850408243\n",
      "Epoch 97/100: Loss: 0.02328635979865794\n",
      "Epoch 98/100: Loss: 0.01702675471897237\n",
      "Epoch 99/100: Loss: 0.01668514250777662\n",
      "Epoch 100/100: Loss: 0.01524842469370924\n",
      "time costs: 434.611670255661\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "accuarcies = []\n",
    "times = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'training epoch: {i + 1}')\n",
    "    qmodel = QModel(input_size, hidden_size, num_output, \n",
    "                num_layers = num_layers, ctx = ctx, mode='classical')\n",
    "    optimizer = torch.optim.Adam(qmodel.parameters(), lr = 0.001)\n",
    "    loss_func = nn.MSELoss()\n",
    "    start = time.time()\n",
    "    losses = train_model(qmodel, train_data, batch_size=20,          \n",
    "                   loss_func = loss_func, optimizer = optimizer, epoch = 100)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'time costs: {end - start}')\n",
    "    times.append(end - start)\n",
    "    \n",
    "    acc = calculate_accuarcy(qmodel, X_test, y_test)[0]\n",
    "    accuarcies.append(acc)\n",
    "    \n",
    "    with open(f'loss/layer3/loss_layer3_{i + 1}.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(losses, pkl_file)\n",
    "    torch.save(qmodel.state_dict(), f\"model/layer3/model_layer3_{i+1}.pt\")\n",
    "    \n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ac9c597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c7f5287040>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses, color=\"#FF6666\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3343ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantum paramers: 64\n"
     ]
    }
   ],
   "source": [
    "print(f'quantum paramers: {QLSTM(1, 1, ctx = ctx).qparameters_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a8bc2f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[66.97155716302437,\n",
       " 97.97834299609242,\n",
       " 80.21928645919574,\n",
       " 87.81126929411994,\n",
       " 92.99493915586694,\n",
       " 80.78482700751144,\n",
       " 74.8241111523289,\n",
       " 58.26437396663313,\n",
       " 44.517068365620695,\n",
       " 91.93319579156156]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuarcies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5af0126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.62989713519553"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 77.62989713519553\n",
    "# \n",
    "# [66.97155716302437,\n",
    "#  97.97834299609242,\n",
    "#  80.21928645919574,\n",
    "#  87.81126929411994,\n",
    "#  92.99493915586694,\n",
    "#  80.78482700751144,\n",
    "#  74.8241111523289,\n",
    "#  58.26437396663313,\n",
    "#  44.517068365620695,\n",
    "#  91.93319579156156]\n",
    "np.mean(accuarcies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e317ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443.9610315799713"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 443.9610315799713\n",
    "np.mean(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2370cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
