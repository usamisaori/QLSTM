{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6581af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94243045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff0ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyqpanda import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "455c7f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a618a3a",
   "metadata": {},
   "source": [
    "# 1. Prepare Dadaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c64faba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/sumanthvrao/daily-climate-time-series-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de627766",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './../data/DailyDelhiClimateTrain.csv'\n",
    "test_path = './../data/DailyDelhiClimateTest.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d0827fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [1,2,3,4]\n",
    "\n",
    "train = pd.read_csv(train_path, usecols=cols, engine=\"python\")\n",
    "test = pd.read_csv(test_path, usecols=cols, engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c3039c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train)=1462\n",
      "len(test)=114\n"
     ]
    }
   ],
   "source": [
    "print(f'len(train)={len(train)}')\n",
    "print(f'len(test)={len(test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b941db2e",
   "metadata": {},
   "source": [
    "## 1.1 Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e59fddc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove outliers num: 9\n"
     ]
    }
   ],
   "source": [
    "unnormal_num = 0\n",
    "for i in range(len(train)):\n",
    "    mp = train.iloc[i][3]\n",
    "    if mp > 1200 or mp < 950:\n",
    "        unnormal_num += 1\n",
    "        train.iloc[i][3] = train.iloc[i + 1][3]\n",
    "print(f'remove outliers num: {unnormal_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fefec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.iloc[0][3] = test.iloc[1][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6035297e",
   "metadata": {},
   "source": [
    "## 1.2 Transfer data to LSTM representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a1277fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "884bc13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(data, window_size, predict_size):\n",
    "    scaler = StandardScaler()\n",
    "    data = scaler.fit_transform(np.array(data).reshape(-1, 1))\n",
    "    \n",
    "    data_in = []\n",
    "    data_out = []\n",
    "    \n",
    "    for i in range(data.shape[0] - window_size - predict_size):\n",
    "        data_in.append(data[i:i + window_size].reshape(1, window_size)[0])\n",
    "        data_out.append(data[i + window_size:i + window_size + predict_size].reshape(1, predict_size)[0])\n",
    "        \n",
    "    data_in = np.array(data_in).reshape(-1, window_size)\n",
    "    data_out = np.array(data_out).reshape(-1, predict_size)\n",
    "    \n",
    "    data_process = {'datain': data_in, 'dataout': data_out}\n",
    "    \n",
    "    return data_process, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6517fe60",
   "metadata": {},
   "source": [
    "## 1.3 prepare train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d333c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_size = 4\n",
    "window_size = features_size * 3 # features num * time steps\n",
    "predict_size = features_size # features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ef548b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed, train_scaler = data_process(train, window_size, predict_size)\n",
    "X_train, y_train = train_processed['datain'], train_processed['dataout']\n",
    "\n",
    "test_processed, test_scaler = data_process(test, window_size, predict_size)\n",
    "X_test, y_test = test_processed['datain'], test_processed['dataout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f779325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dda516d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "\n",
    "train_data = Data.TensorDataset(X_train, y_train)\n",
    "test_data = Data.TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb132c",
   "metadata": {},
   "source": [
    "# 2. Quantum Enhanced LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d517f5f3",
   "metadata": {},
   "source": [
    "## 2.1 initiate quantum environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffc85d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitQMachine:\n",
    "    def __init__(self, qubitsCount, cbitsCount = 0, machineType = QMachineType.CPU):\n",
    "        self.machine = init_quantum_machine(machineType)\n",
    "        \n",
    "        self.qubits = self.machine.qAlloc_many(qubitsCount)\n",
    "        self.cbits = self.machine.cAlloc_many(cbitsCount)\n",
    "        \n",
    "        print(f'Init Quantum Machine with qubits:[{qubitsCount}] / cbits:[{cbitsCount}] Successfully')\n",
    "    \n",
    "    def __del__(self):\n",
    "        destroy_quantum_machine(self.machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "741a6a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Quantum Machine with qubits:[4] / cbits:[0] Successfully\n"
     ]
    }
   ],
   "source": [
    "# maximum qubits size\n",
    "ctx = InitQMachine(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a39cf3",
   "metadata": {},
   "source": [
    "## 2.2 Quantum Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b029d",
   "metadata": {},
   "source": [
    "### 2.2.1 Quantum layer base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5570e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f9cfd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayerBase(nn.Module):\n",
    "    def __init__(self, input_size, output_size, *, n_qubits, n_layers = 1, ctx = None):\n",
    "        super(QuantumLayerBase, self).__init__()\n",
    "        \n",
    "        self.data = None # need to input during forward\n",
    "    \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size # hidden size, not n_qubits\n",
    "        \n",
    "        # quantum infos\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.ctx = ctx\n",
    "        self.qubits = ctx.qubits\n",
    "        self.machine = ctx.machine\n",
    "        \n",
    "        # convert quantum input/output to match classical computation\n",
    "        self.qin = nn.Linear(self.input_size, self.n_qubits)\n",
    "        self.qout = nn.Linear(self.n_qubits, self.output_size)\n",
    "        \n",
    "    @property\n",
    "    def circuit(self):\n",
    "        raise NotImplementedError('Should init circuit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94c5bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(self):\n",
    "    HamiZ = [ PauliOperator({f'Z{i}': 1}) for i in range(len(self.qubits)) ]\n",
    "    res = [ eval(qop(self.circuit, Hami, self.machine, self.qubits))[0,0] for Hami in HamiZ ]\n",
    "    \n",
    "    return Parameter(Tensor(res[:self.n_qubits]))\n",
    "\n",
    "QuantumLayerBase.measure = measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4341341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, inputs):\n",
    "    y_t = self.qin(Parameter(inputs))\n",
    "    self.data = y_t[0]\n",
    "    \n",
    "    return self.qout(self.measure())\n",
    "\n",
    "QuantumLayerBase.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4f794b",
   "metadata": {},
   "source": [
    "### 2.2.2 Quantum layer design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b98a8984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayer(QuantumLayerBase):\n",
    "    def __init__(self, input_size, output_size, *, n_qubits, degree = 1, n_layers = 1, ctx = None):\n",
    "        super(QuantumLayer, self).__init__(input_size, output_size, \n",
    "                                         n_qubits = n_qubits, n_layers = n_layers, ctx = ctx)\n",
    "        \n",
    "        self.degree = degree\n",
    "        self.angles = Parameter(torch.rand(n_layers * 3, degree, self.n_qubits))\n",
    "        \n",
    "    @property\n",
    "    def qparameters_size(self):\n",
    "        return self.angles.flatten().size()[0]\n",
    "        \n",
    "    @property\n",
    "    def circuit(self):\n",
    "        if self.data == None:\n",
    "            raise ValueError('Need to feed a input data!')\n",
    "        \n",
    "        n = self.n_qubits\n",
    "        q = self.qubits\n",
    "        x = self.data\n",
    "        p = self.angles\n",
    "        degree = self.degree\n",
    "        \n",
    "        h = VariationalQuantumGate_H\n",
    "        ry = VariationalQuantumGate_RY\n",
    "        rx = VariationalQuantumGate_RX\n",
    "        rz = VariationalQuantumGate_RZ\n",
    "        crx = VariationalQuantumGate_CRX\n",
    "        \n",
    "        # init variational quantum circuit\n",
    "        vqc = VariationalQuantumCircuit()\n",
    "\n",
    "        # encoding layer\n",
    "        [ vqc.insert( h(q[i]) ) for i in range(n) ]\n",
    "        [ vqc.insert( ry(q[i], var(x[i] * torch.pi / 2)) ) for i in range(n) ]\n",
    "        \n",
    "        # variational layer\n",
    "        [ vqc.insert( rx(q[i], var(p[0][0][i]) )) for i in range(n) ]\n",
    "        [ vqc.insert( rz(q[i], var(p[1][0][i]) )) for i in range(n) ]\n",
    "        \n",
    "        vqc.insert(crx(q[0], q[3], var(p[1][0][0])))\n",
    "        vqc.insert(crx(q[3], q[2], var(p[1][0][1])))\n",
    "        vqc.insert(crx(q[2], q[1], var(p[1][0][2])))\n",
    "        vqc.insert(crx(q[1], q[0], var(p[1][0][3])))\n",
    "        \n",
    "        return vqc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766692de",
   "metadata": {},
   "source": [
    "### 2.2.3 Plot Quantum Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88ff286b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyqpanda.pyQPanda.QProg at 0x2da212abf70>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Tensor([[0.1, 0.2, 0.3, 0.4]])\n",
    "layer = QuantumLayer(4, 4, n_qubits=4, n_layers=1, degree=3, ctx=ctx)\n",
    "layer.data = data[0]\n",
    "vqc = layer.circuit\n",
    "prog = create_empty_qprog()\n",
    "prog.insert(vqc.feed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fd364d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'null'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_qprog(prog, 'pic', filename=f'pic/layer7')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50742459",
   "metadata": {},
   "source": [
    "## 2.3 Quantum-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e53ae0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLSTMBase(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, *, ctx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ctx = ctx\n",
    "        \n",
    "    @property\n",
    "    def qparameters_size(self):\n",
    "        num = 0\n",
    "        for attr in dir(self):\n",
    "            if attr.endswith('_circuit'):\n",
    "                num += getattr(self, attr).qparameters_size\n",
    "        return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "582b17af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, inputs, init_states = None):\n",
    "    sequence_size, batch_size, _ = inputs.size()\n",
    "    hidden_sequence = []\n",
    "    \n",
    "    if init_states == None:\n",
    "        h_t, c_t = (\n",
    "            torch.zeros(1, batch_size, self.hidden_size).to(inputs.device),\n",
    "            torch.zeros(1, batch_size, self.hidden_size).to(inputs.device),\n",
    "        )\n",
    "    else:\n",
    "        h_t, c_t = init_states\n",
    "    \n",
    "    return hidden_sequence, (h_t, c_t)\n",
    "\n",
    "QLSTMBase.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d6b0af",
   "metadata": {},
   "source": [
    "## - classical quatum enhanced LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adc8fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLSTM(QLSTMBase):\n",
    "    def __init__(self, input_size, hidden_size, *, ctx):\n",
    "        super().__init__(input_size, hidden_size, ctx = ctx)\n",
    "    \n",
    "        # input gates\n",
    "        self.input_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                        n_qubits = 4, ctx = ctx) # 15\n",
    "        # forget gates\n",
    "        self.forget_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                         n_qubits = 4, ctx = ctx) # 15\n",
    "        # candidate\n",
    "        self.candidate_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                       n_qubits = 4, ctx = ctx) # 15\n",
    "        # output gates\n",
    "        self.output_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                         n_qubits = 4, ctx = ctx) # 15\n",
    "        \n",
    "    def forward(self, inputs, init_states = None):\n",
    "        hidden_sequence, (h_t, c_t) = super(QLSTM, self).forward(inputs, init_states)\n",
    "\n",
    "        for t in range(inputs.size()[0]):\n",
    "            x_t = inputs[t, :, :]\n",
    "            v_t = torch.cat((h_t[0], x_t), dim = 1)\n",
    "\n",
    "            # input gates\n",
    "            i_t = torch.sigmoid(self.input_circuit(v_t))\n",
    "            # forget gates\n",
    "            f_t = torch.sigmoid(self.forget_circuit(v_t))\n",
    "            # candidate for cell state update\n",
    "            g_t = torch.tanh(self.candidate_circuit(v_t))\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "\n",
    "            # output gates\n",
    "            o_t = torch.sigmoid(self.output_circuit(v_t))\n",
    "            # update output ht\n",
    "            h_t = o_t * (torch.tanh(c_t))\n",
    "\n",
    "            hidden_sequence.append(h_t)\n",
    "\n",
    "        # reshape hidden_seq p/ retornar\n",
    "        #\n",
    "        # [tensor([[[0.0444, ...]]] => tensor([[[0.0444, ...]]]\n",
    "        # \n",
    "        hidden_sequence = torch.cat(hidden_sequence, dim = 0)\n",
    "\n",
    "        return hidden_sequence, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28836475",
   "metadata": {},
   "source": [
    "## 2.4 Stacked QLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ec06c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class StackedQLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, *, num_layers = 1, ctx = None, mode = 'classical'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.qlstms = nn.Sequential(OrderedDict([\n",
    "            (f'QLSTM {i + 1}', QLSTM(input_size if i == 0 else hidden_size , hidden_size, ctx = ctx)) \n",
    "                for i in range(num_layers)\n",
    "        ]))\n",
    "\n",
    "    def forward(self, inputs, parameters = None):\n",
    "        outputs = None\n",
    "        \n",
    "        for i, qlstm in enumerate(self.qlstms):\n",
    "            if i != 0:\n",
    "                inputs = outputs\n",
    "            \n",
    "            outputs, parameters = qlstm(inputs, parameters)\n",
    "        \n",
    "        return outputs, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0389535c",
   "metadata": {},
   "source": [
    "# 3. Quantum Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "413150bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_output, *, num_layers = 1, ctx = None, mode = 'classical'):\n",
    "        super(QModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.qlstm = StackedQLSTM(input_size, hidden_size, \n",
    "                                  num_layers = num_layers, ctx = ctx, mode = mode)\n",
    "        self.predict = nn.Linear(hidden_size, num_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        # sequence lenth , batch_size, features length\n",
    "        # \n",
    "        h0 = torch.zeros(1, x.size(1), self.hidden_size)\n",
    "        c0 = torch.zeros(1, x.size(1), self.hidden_size)\n",
    "        \n",
    "        out, _ = self.qlstm(x, (h0, c0))\n",
    "        out = self.predict(out[0])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111aebe4",
   "metadata": {},
   "source": [
    "## 3.1 train QModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc1d1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "def train_model(model, datas, batch_size, *, loss_func, optimizer, epoch = 50):\n",
    "    losses = []\n",
    "    sampler = RandomSampler(datas, num_samples = batch_size)\n",
    "    \n",
    "    for step in range(epoch):\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for index in sampler:\n",
    "            batch_x, batch_y = datas[index][0], datas[index][1]\n",
    "            b_x = batch_x.unsqueeze(0)\n",
    "            b_y = batch_y.unsqueeze(0)\n",
    "            \n",
    "            output = model(b_x)\n",
    "\n",
    "            loss = loss_func(output, b_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {step + 1}/{epoch}: Loss: {train_loss / batch_size}')\n",
    "        losses.append(train_loss / batch_size)\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef07934f",
   "metadata": {},
   "source": [
    "## 3.2 Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57e9b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "def MAE_naive(actuals, predicteds):\n",
    "    n = len(actuals)\n",
    "    err = 0.0\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        err += np.abs(actuals[i] - actuals[i - 1])\n",
    "    return err / (n - 1)\n",
    "\n",
    "def calculate_accuarcy(model, X_test, y_test, scaler=test_scaler):\n",
    "    n = len(X_test)\n",
    "    \n",
    "    actuals = []\n",
    "    predicteds = []\n",
    "    \n",
    "    for i in range(0, n, predict_size):\n",
    "        actual = scaler.inverse_transform(y_test[i:i+1].data)\n",
    "        actuals.append(np.array(actual[0]))\n",
    "        predicted = scaler.inverse_transform(model(X_test[i:i+1]).data)\n",
    "        predicteds.append(np.array(predicted[0]))\n",
    "    \n",
    "    actuals = np.array(actuals)\n",
    "    predicteds = np.array(predicteds)\n",
    "    \n",
    "    mae = mean_absolute_error(actuals, predicteds)\n",
    "    mase = mae / MAE_naive(actuals.flatten(), predicteds.flatten())\n",
    "    mape = mean_absolute_percentage_error(actuals, predicteds)\n",
    "    mse = mean_squared_error(actuals, predicteds)\n",
    "    rmse = mse ** 0.5\n",
    "    \n",
    "    return np.array([(1 - mase) * 100, rmse, mse, mae, mape])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6407f969",
   "metadata": {},
   "source": [
    "## 3.3 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "305c9471",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_size = 4\n",
    "window_size = features_size * 3 # \n",
    "predict_size = features_size # features\n",
    "\n",
    "input_size = window_size\n",
    "num_output = predict_size\n",
    "\n",
    "hidden_size = 32\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "50472565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: 1\n",
      "Epoch 1/200: Loss: 1.0505547881126405\n",
      "Epoch 2/200: Loss: 1.0044989496469499\n",
      "Epoch 3/200: Loss: 0.9852478384971619\n",
      "Epoch 4/200: Loss: 0.9758848428726197\n",
      "Epoch 5/200: Loss: 0.9647246569395065\n",
      "Epoch 6/200: Loss: 0.9374488890171051\n",
      "Epoch 7/200: Loss: 1.0041688054800033\n",
      "Epoch 8/200: Loss: 0.9778746634721756\n",
      "Epoch 9/200: Loss: 0.9322754979133606\n",
      "Epoch 10/200: Loss: 0.9369639962911606\n",
      "Epoch 11/200: Loss: 0.9430821776390076\n",
      "Epoch 12/200: Loss: 0.8904943764209747\n",
      "Epoch 13/200: Loss: 0.91146579682827\n",
      "Epoch 14/200: Loss: 0.858716431260109\n",
      "Epoch 15/200: Loss: 0.8733907818794251\n",
      "Epoch 16/200: Loss: 0.8389418929815292\n",
      "Epoch 17/200: Loss: 0.8332047909498215\n",
      "Epoch 18/200: Loss: 0.7863548755645752\n",
      "Epoch 19/200: Loss: 0.7620469480752945\n",
      "Epoch 20/200: Loss: 0.7294860154390335\n",
      "Epoch 21/200: Loss: 0.7171175658702851\n",
      "Epoch 22/200: Loss: 0.6890052676200866\n",
      "Epoch 23/200: Loss: 0.6105645716190338\n",
      "Epoch 24/200: Loss: 0.595866034924984\n",
      "Epoch 25/200: Loss: 0.5805321097373962\n",
      "Epoch 26/200: Loss: 0.549142874777317\n",
      "Epoch 27/200: Loss: 0.5381653413176537\n",
      "Epoch 28/200: Loss: 0.4945543557405472\n",
      "Epoch 29/200: Loss: 0.49195653200149536\n",
      "Epoch 30/200: Loss: 0.37718028351664545\n",
      "Epoch 31/200: Loss: 0.42392328530550005\n",
      "Epoch 32/200: Loss: 0.35596862956881525\n",
      "Epoch 33/200: Loss: 0.324177211150527\n",
      "Epoch 34/200: Loss: 0.35859450586140157\n",
      "Epoch 35/200: Loss: 0.30895093474537133\n",
      "Epoch 36/200: Loss: 0.3588354837149382\n",
      "Epoch 37/200: Loss: 0.27321100421249866\n",
      "Epoch 38/200: Loss: 0.33571069315075874\n",
      "Epoch 39/200: Loss: 0.24752785861492158\n",
      "Epoch 40/200: Loss: 0.24574896758422254\n",
      "Epoch 41/200: Loss: 0.21991539420560002\n",
      "Epoch 42/200: Loss: 0.22013751589693129\n",
      "Epoch 43/200: Loss: 0.14378462061285974\n",
      "Epoch 44/200: Loss: 0.2170313460053876\n",
      "Epoch 45/200: Loss: 0.21955842964816838\n",
      "Epoch 46/200: Loss: 0.1388519155443646\n",
      "Epoch 47/200: Loss: 0.19340869300067426\n",
      "Epoch 48/200: Loss: 0.18610398988821544\n",
      "Epoch 49/200: Loss: 0.15446613780222834\n",
      "Epoch 50/200: Loss: 0.1189574696298223\n",
      "Epoch 51/200: Loss: 0.09820838852756424\n",
      "Epoch 52/200: Loss: 0.09317849872313673\n",
      "Epoch 53/200: Loss: 0.08360003767847957\n",
      "Epoch 54/200: Loss: 0.056386843464861156\n",
      "Epoch 55/200: Loss: 0.05750017994541849\n",
      "Epoch 56/200: Loss: 0.09235156968497904\n",
      "Epoch 57/200: Loss: 0.061103437688507255\n",
      "Epoch 58/200: Loss: 0.06036769189377082\n",
      "Epoch 59/200: Loss: 0.04440983049280476\n",
      "Epoch 60/200: Loss: 0.04095767047401751\n",
      "Epoch 61/200: Loss: 0.030688507510058118\n",
      "Epoch 62/200: Loss: 0.026811443244150725\n",
      "Epoch 63/200: Loss: 0.028826696672877006\n",
      "Epoch 64/200: Loss: 0.017126807881868444\n",
      "Epoch 65/200: Loss: 0.021313908903903212\n",
      "Epoch 66/200: Loss: 0.01783864309763885\n",
      "Epoch 67/200: Loss: 0.011645520353340544\n",
      "Epoch 68/200: Loss: 0.011810053732551751\n",
      "Epoch 69/200: Loss: 0.012748618847399484\n",
      "Epoch 70/200: Loss: 0.0052751642659131905\n",
      "Epoch 71/200: Loss: 0.007562923844670877\n",
      "Epoch 72/200: Loss: 0.005486668968660524\n",
      "Epoch 73/200: Loss: 0.0023449437763702007\n",
      "Epoch 74/200: Loss: 0.0032160007722268347\n",
      "Epoch 75/200: Loss: 0.002573403653514106\n",
      "Epoch 76/200: Loss: 0.0020637986570363866\n",
      "Epoch 77/200: Loss: 0.0013342731126613216\n",
      "Epoch 78/200: Loss: 0.001369061416698969\n",
      "Epoch 79/200: Loss: 0.0008094952076135087\n",
      "Epoch 80/200: Loss: 0.0008747986939852126\n",
      "Epoch 81/200: Loss: 0.0006602583203857648\n",
      "Epoch 82/200: Loss: 0.0004886801740212832\n",
      "Epoch 83/200: Loss: 0.000639582215444534\n",
      "Epoch 84/200: Loss: 0.0004910601119263447\n",
      "Epoch 85/200: Loss: 0.0006431792970033711\n",
      "Epoch 86/200: Loss: 0.00037047501828055827\n",
      "Epoch 87/200: Loss: 0.000310831976321424\n",
      "Epoch 88/200: Loss: 0.00029108791504768307\n",
      "Epoch 89/200: Loss: 0.0002569854957982898\n",
      "Epoch 90/200: Loss: 0.00028171529752398785\n",
      "Epoch 91/200: Loss: 0.00021487315088961623\n",
      "Epoch 92/200: Loss: 0.00024402232629654462\n",
      "Epoch 93/200: Loss: 0.00017735595914700753\n",
      "Epoch 94/200: Loss: 0.0003182177674716513\n",
      "Epoch 95/200: Loss: 0.0002064837270154385\n",
      "Epoch 96/200: Loss: 0.00029332228605198906\n",
      "Epoch 97/200: Loss: 0.00014449209638769389\n",
      "Epoch 98/200: Loss: 0.00015192801129160216\n",
      "Epoch 99/200: Loss: 0.0001853380179454689\n",
      "Epoch 100/200: Loss: 0.00033539985997776967\n",
      "Epoch 101/200: Loss: 0.00025325555166091365\n",
      "Epoch 102/200: Loss: 0.00023897938990558032\n",
      "Epoch 103/200: Loss: 0.0002747472000010021\n",
      "Epoch 104/200: Loss: 0.0002255243369290838\n",
      "Epoch 105/200: Loss: 0.00019169697798133712\n",
      "Epoch 106/200: Loss: 0.00019345562668604543\n",
      "Epoch 107/200: Loss: 0.00028546843359436027\n",
      "Epoch 108/200: Loss: 0.0002638948237290606\n",
      "Epoch 109/200: Loss: 0.0002887379238018184\n",
      "Epoch 110/200: Loss: 0.0002797243092572899\n",
      "Epoch 111/200: Loss: 0.00028273945481487316\n",
      "Epoch 112/200: Loss: 0.0002539909250117489\n",
      "Epoch 113/200: Loss: 0.0002470275032464997\n",
      "Epoch 114/200: Loss: 0.0002070936982818239\n",
      "Epoch 115/200: Loss: 0.0002468263090122491\n",
      "Epoch 116/200: Loss: 0.0002399556809450587\n",
      "Epoch 117/200: Loss: 0.0002844350156010478\n",
      "Epoch 118/200: Loss: 0.0002809802799674799\n",
      "Epoch 119/200: Loss: 0.0002718169041145302\n",
      "Epoch 120/200: Loss: 0.00029472028036252594\n",
      "Epoch 121/200: Loss: 0.0003013529523741454\n",
      "Epoch 122/200: Loss: 0.0002484341875060636\n",
      "Epoch 123/200: Loss: 0.000327564108374645\n",
      "Epoch 124/200: Loss: 0.00046038961190788543\n",
      "Epoch 125/200: Loss: 0.0002594090718048392\n",
      "Epoch 126/200: Loss: 0.00017715744479573914\n",
      "Epoch 127/200: Loss: 0.000225592551396403\n",
      "Epoch 128/200: Loss: 0.0002842084356416308\n",
      "Epoch 129/200: Loss: 0.0002617717381326656\n",
      "Epoch 130/200: Loss: 0.0002490309329004958\n",
      "Epoch 131/200: Loss: 0.0003112366938694322\n",
      "Epoch 132/200: Loss: 0.00018819374108716146\n",
      "Epoch 133/200: Loss: 0.00025365793153468984\n",
      "Epoch 134/200: Loss: 0.000217156859048373\n",
      "Epoch 135/200: Loss: 0.00023361904195553506\n",
      "Epoch 136/200: Loss: 0.0002858782273506222\n",
      "Epoch 137/200: Loss: 0.00037067326775286347\n",
      "Epoch 138/200: Loss: 0.00020210473303450271\n",
      "Epoch 139/200: Loss: 0.00034929485900647705\n",
      "Epoch 140/200: Loss: 0.00019149018362440984\n",
      "Epoch 141/200: Loss: 0.00020891311405648593\n",
      "Epoch 142/200: Loss: 0.00024703181497898186\n",
      "Epoch 143/200: Loss: 0.00024294067807204556\n",
      "Epoch 144/200: Loss: 0.0002867629889806267\n",
      "Epoch 145/200: Loss: 0.0002648208010214148\n",
      "Epoch 146/200: Loss: 0.00029189927809056824\n",
      "Epoch 147/200: Loss: 0.00028672942044067896\n",
      "Epoch 148/200: Loss: 0.00035064210951532007\n",
      "Epoch 149/200: Loss: 0.00024702676046217675\n",
      "Epoch 150/200: Loss: 0.0003346629860629946\n",
      "Epoch 151/200: Loss: 0.00018131410415662686\n",
      "Epoch 152/200: Loss: 0.00029244279576232655\n",
      "Epoch 153/200: Loss: 0.0003402956055765571\n",
      "Epoch 154/200: Loss: 0.0002310280260644504\n",
      "Epoch 155/200: Loss: 0.0002877541971429309\n",
      "Epoch 156/200: Loss: 0.00028374767098284794\n",
      "Epoch 157/200: Loss: 0.0003569493173927185\n",
      "Epoch 158/200: Loss: 0.00026509529673148793\n",
      "Epoch 159/200: Loss: 0.0001923739087942522\n",
      "Epoch 160/200: Loss: 0.0003042834097868763\n",
      "Epoch 161/200: Loss: 0.00021521018552448368\n",
      "Epoch 162/200: Loss: 0.0002899892154346162\n",
      "Epoch 163/200: Loss: 0.0002281533865698293\n",
      "Epoch 164/200: Loss: 0.0003374369900484453\n",
      "Epoch 165/200: Loss: 0.0002360255531129951\n",
      "Epoch 166/200: Loss: 0.00025578914064681155\n",
      "Epoch 167/200: Loss: 0.00021558835487667238\n",
      "Epoch 168/200: Loss: 0.00016012455416785086\n",
      "Epoch 169/200: Loss: 0.0002781619359666365\n",
      "Epoch 170/200: Loss: 0.0002617778842250118\n",
      "Epoch 171/200: Loss: 0.0001973758709937101\n",
      "Epoch 172/200: Loss: 0.00020661108947024333\n",
      "Epoch 173/200: Loss: 0.0002198190740728023\n",
      "Epoch 174/200: Loss: 0.0003186180201737443\n",
      "Epoch 175/200: Loss: 0.0002441600236124941\n",
      "Epoch 176/200: Loss: 0.0002325762995496916\n",
      "Epoch 177/200: Loss: 0.0002479086871517211\n",
      "Epoch 178/200: Loss: 0.00023128100165195064\n",
      "Epoch 179/200: Loss: 0.0003163687061714882\n",
      "Epoch 180/200: Loss: 0.00020578930361807578\n",
      "Epoch 181/200: Loss: 0.00026096594815498974\n",
      "Epoch 182/200: Loss: 0.00026806889472936747\n",
      "Epoch 183/200: Loss: 0.000223077047849074\n",
      "Epoch 184/200: Loss: 0.00017690399577077187\n",
      "Epoch 185/200: Loss: 0.0002864515490728081\n",
      "Epoch 186/200: Loss: 0.00026575227439025185\n",
      "Epoch 187/200: Loss: 0.00020563989610309364\n",
      "Epoch 188/200: Loss: 0.00020170398893242235\n",
      "Epoch 189/200: Loss: 0.00023667362620471977\n",
      "Epoch 190/200: Loss: 0.0002691361715733365\n",
      "Epoch 191/200: Loss: 0.00031036739455885253\n",
      "Epoch 192/200: Loss: 0.0003080411043811182\n",
      "Epoch 193/200: Loss: 0.0003696602745549171\n",
      "Epoch 194/200: Loss: 0.0002410318801594258\n",
      "Epoch 195/200: Loss: 0.00029946516933705426\n",
      "Epoch 196/200: Loss: 0.0002962811671750387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197/200: Loss: 0.00026584428933347227\n",
      "Epoch 198/200: Loss: 0.00016995357664200127\n",
      "Epoch 199/200: Loss: 0.00030500849297823154\n",
      "Epoch 200/200: Loss: 0.0002819357449880044\n",
      "time costs: 1071.1398441791534\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "accuarcies = []\n",
    "times = []\n",
    "\n",
    "for i in range(1):\n",
    "    print(f'training epoch: {i + 1}')\n",
    "    qmodel = QModel(input_size, hidden_size, num_output, \n",
    "                num_layers = num_layers, ctx = ctx, mode='classical')\n",
    "    optimizer = torch.optim.Adam(qmodel.parameters(), lr = 0.001)\n",
    "    loss_func = nn.MSELoss()\n",
    "    start = time.time()\n",
    "    losses = train_model(qmodel, train_data, batch_size=20,          \n",
    "                   loss_func = loss_func, optimizer = optimizer, epoch = 200)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'time costs: {end - start}')\n",
    "    times.append(end - start)\n",
    "    \n",
    "    acc = calculate_accuarcy(qmodel, X_test, y_test)[0]\n",
    "    accuarcies.append(acc)\n",
    "    \n",
    "    with open(f'loss/layer7/loss_layer7_{i + 1}.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(losses, pkl_file)\n",
    "    torch.save(qmodel.state_dict(), f\"model/layer7/model_layer7_{i+1}.pt\")\n",
    "    \n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ac9c597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2da38b5fa00>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses, color=\"#FF6666\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d3343ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantum paramers: 48\n"
     ]
    }
   ],
   "source": [
    "print(f'quantum paramers: {QLSTM(1, 1, ctx = ctx).qparameters_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b73d6ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[98.77351115235274]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuarcies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5af0126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.30188012672066"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 98.30188012672066\n",
    "# \n",
    "# [97.20577802456741,\n",
    "#  98.83420214841753,\n",
    "#  98.92529640887034,\n",
    "#  99.00890772506791,\n",
    "#  98.77113562661228,\n",
    "#  98.39557917334662,\n",
    "#  98.62360773266879,\n",
    "#  98.01388268477184,\n",
    "#  98.73196978118206,\n",
    "#  96.5084419617019]\n",
    "np.mean(accuarcies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e317ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "546.60958776474"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 546.60958776474\n",
    "np.mean(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34545133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
