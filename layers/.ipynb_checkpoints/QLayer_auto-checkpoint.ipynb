{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6581af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94243045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff0ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyqpanda import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "455c7f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a618a3a",
   "metadata": {},
   "source": [
    "# 1. Prepare Dadaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c64faba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/sumanthvrao/daily-climate-time-series-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de627766",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './../data/DailyDelhiClimateTrain.csv'\n",
    "test_path = './../data/DailyDelhiClimateTest.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d0827fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [1,2,3,4]\n",
    "\n",
    "train = pd.read_csv(train_path, usecols=cols, engine=\"python\")\n",
    "test = pd.read_csv(test_path, usecols=cols, engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c3039c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train)=1462\n",
      "len(test)=114\n"
     ]
    }
   ],
   "source": [
    "print(f'len(train)={len(train)}')\n",
    "print(f'len(test)={len(test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b941db2e",
   "metadata": {},
   "source": [
    "## 1.1 Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e59fddc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove outliers num: 9\n"
     ]
    }
   ],
   "source": [
    "unnormal_num = 0\n",
    "for i in range(len(train)):\n",
    "    mp = train.iloc[i][3]\n",
    "    if mp > 1200 or mp < 950:\n",
    "        unnormal_num += 1\n",
    "        train.iloc[i][3] = train.iloc[i + 1][3]\n",
    "print(f'remove outliers num: {unnormal_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fefec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.iloc[0][3] = test.iloc[1][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6035297e",
   "metadata": {},
   "source": [
    "## 1.2 Transfer data to LSTM representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a1277fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "884bc13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(data, window_size, predict_size):\n",
    "    scaler = StandardScaler()\n",
    "    data = scaler.fit_transform(np.array(data).reshape(-1, 1))\n",
    "    \n",
    "    data_in = []\n",
    "    data_out = []\n",
    "    \n",
    "    for i in range(data.shape[0] - window_size - predict_size):\n",
    "        data_in.append(data[i:i + window_size].reshape(1, window_size)[0])\n",
    "        data_out.append(data[i + window_size:i + window_size + predict_size].reshape(1, predict_size)[0])\n",
    "        \n",
    "    data_in = np.array(data_in).reshape(-1, window_size)\n",
    "    data_out = np.array(data_out).reshape(-1, predict_size)\n",
    "    \n",
    "    data_process = {'datain': data_in, 'dataout': data_out}\n",
    "    \n",
    "    return data_process, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6517fe60",
   "metadata": {},
   "source": [
    "## 1.3 prepare train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d333c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_size = 4\n",
    "window_size = features_size * 3 # features num * time steps\n",
    "predict_size = features_size # features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ef548b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed, train_scaler = data_process(train, window_size, predict_size)\n",
    "X_train, y_train = train_processed['datain'], train_processed['dataout']\n",
    "\n",
    "test_processed, test_scaler = data_process(test, window_size, predict_size)\n",
    "X_test, y_test = test_processed['datain'], test_processed['dataout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f779325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dda516d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "\n",
    "train_data = Data.TensorDataset(X_train, y_train)\n",
    "test_data = Data.TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb132c",
   "metadata": {},
   "source": [
    "# 2. Quantum Enhanced LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d517f5f3",
   "metadata": {},
   "source": [
    "## 2.1 initiate quantum environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffc85d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitQMachine:\n",
    "    def __init__(self, qubitsCount, cbitsCount = 0, machineType = QMachineType.CPU):\n",
    "        self.machine = init_quantum_machine(machineType)\n",
    "        \n",
    "        self.qubits = self.machine.qAlloc_many(qubitsCount)\n",
    "        self.cbits = self.machine.cAlloc_many(cbitsCount)\n",
    "        \n",
    "        print(f'Init Quantum Machine with qubits:[{qubitsCount}] / cbits:[{cbitsCount}] Successfully')\n",
    "    \n",
    "    def __del__(self):\n",
    "        destroy_quantum_machine(self.machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "741a6a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Quantum Machine with qubits:[4] / cbits:[0] Successfully\n"
     ]
    }
   ],
   "source": [
    "# maximum qubits size\n",
    "ctx = InitQMachine(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a39cf3",
   "metadata": {},
   "source": [
    "## 2.2 Quantum Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b029d",
   "metadata": {},
   "source": [
    "### 2.2.1 Quantum layer base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5570e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f9cfd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayerBase(nn.Module):\n",
    "    def __init__(self, input_size, output_size, *, n_qubits, n_layers = 1, ctx = None):\n",
    "        super(QuantumLayerBase, self).__init__()\n",
    "        \n",
    "        self.data = None # need to input during forward\n",
    "    \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size # hidden size, not n_qubits\n",
    "        \n",
    "        # quantum infos\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.ctx = ctx\n",
    "        self.qubits = ctx.qubits\n",
    "        self.machine = ctx.machine\n",
    "        \n",
    "        # convert quantum input/output to match classical computation\n",
    "        self.qin = nn.Linear(self.input_size, self.n_qubits)\n",
    "        self.qout = nn.Linear(self.n_qubits, self.output_size)\n",
    "        \n",
    "    @property\n",
    "    def circuit(self):\n",
    "        raise NotImplementedError('Should init circuit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94c5bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(self):\n",
    "    HamiZ = [ PauliOperator({f'Z{i}': 1}) for i in range(len(self.qubits)) ]\n",
    "    res = [ eval(qop(self.circuit, Hami, self.machine, self.qubits))[0,0] for Hami in HamiZ ]\n",
    "    \n",
    "    return Parameter(Tensor(res[:self.n_qubits]))\n",
    "\n",
    "QuantumLayerBase.measure = measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4341341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, inputs):\n",
    "    y_t = self.qin(Parameter(inputs))\n",
    "    self.data = y_t[0]\n",
    "    \n",
    "    return self.qout(self.measure())\n",
    "\n",
    "QuantumLayerBase.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4f794b",
   "metadata": {},
   "source": [
    "### 2.2.2 Quantum layer design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b98a8984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoQuantumLayer(QuantumLayerBase):\n",
    "    def __init__(self, input_size, output_size, *, n_qubits, n_layers = 1, ctx = None):\n",
    "        super(QuantumLayer, self).__init__(input_size, output_size, \n",
    "                                         n_qubits = n_qubits, n_layers = n_layers, ctx = ctx)\n",
    "        \n",
    "        self.angles = Parameter(torch.rand(n_layers + 1, degree, self.n_qubits))\n",
    "        \n",
    "    @property\n",
    "    def qparameters_size(self):\n",
    "        return self.angles.flatten().size()[0]\n",
    "        \n",
    "    @property\n",
    "    def circuit(self):\n",
    "        if self.data == None:\n",
    "            raise ValueError('Need to feed a input data!')\n",
    "        \n",
    "        n = self.n_qubits\n",
    "        q = self.qubits\n",
    "        x = self.data\n",
    "        p = self.angles\n",
    "        degree = self.degree\n",
    "        \n",
    "        h = VariationalQuantumGate_H\n",
    "        ry = VariationalQuantumGate_RY\n",
    "        cz = VariationalQuantumGate_CZ\n",
    "        u = [\n",
    "            None,\n",
    "            VariationalQuantumGate_U1,\n",
    "            VariationalQuantumGate_U2,\n",
    "            VariationalQuantumGate_U3\n",
    "        ]\n",
    "        \n",
    "        # init variational quantum circuit\n",
    "        vqc = VariationalQuantumCircuit()\n",
    "\n",
    "        # encoding layer\n",
    "        [ vqc.insert( h(q[i]) ) for i in range(n) ]\n",
    "        [ vqc.insert( ry(q[i], var(x[i] * torch.pi / 2)) ) for i in range(n) ]\n",
    "        \n",
    "        # variational layer\n",
    "        [ vqc.insert( u[degree](q[i], *[ var(p[0][d][i]) for d in range(degree) ]) ) \n",
    "                 for i in range(n) ]\n",
    "        \n",
    "        for layer in range(self.n_layers):\n",
    "            for i in range(n - 1):\n",
    "                vqc.insert(cz(q[i], q[i + 1]))\n",
    "            vqc.insert(cz(q[n - 1], q[0]))\n",
    "            \n",
    "            [ vqc.insert( u[degree](q[i], *[ var(p[layer + 1][d][i]) for d in range(degree) ]) ) \n",
    "                 for i in range(n) ]\n",
    "        \n",
    "        return vqc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3156df",
   "metadata": {},
   "source": [
    "### 2.2.3 Plot Quantum Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd16d64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyqpanda.pyQPanda.QProg at 0x22f8e7e1470>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Tensor([[0.1, 0.2, 0.3, 0.4]])\n",
    "layer = QuantumLayer(4, 4, n_qubits=4, n_layers=1, degree=3, ctx=ctx)\n",
    "layer.data = data[0]\n",
    "vqc = layer.circuit\n",
    "prog = create_empty_qprog()\n",
    "prog.insert(vqc.feed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f82266f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'null'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_qprog(prog, 'pic', filename=f'pic/layer2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50742459",
   "metadata": {},
   "source": [
    "## 2.3 Quantum-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e53ae0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLSTMBase(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, *, ctx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ctx = ctx\n",
    "        \n",
    "    @property\n",
    "    def qparameters_size(self):\n",
    "        num = 0\n",
    "        for attr in dir(self):\n",
    "            if attr.endswith('_circuit'):\n",
    "                num += getattr(self, attr).qparameters_size\n",
    "        return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "582b17af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, inputs, init_states = None):\n",
    "    sequence_size, batch_size, _ = inputs.size()\n",
    "    hidden_sequence = []\n",
    "    \n",
    "    if init_states == None:\n",
    "        h_t, c_t = (\n",
    "            torch.zeros(1, batch_size, self.hidden_size).to(inputs.device),\n",
    "            torch.zeros(1, batch_size, self.hidden_size).to(inputs.device),\n",
    "        )\n",
    "    else:\n",
    "        h_t, c_t = init_states\n",
    "    \n",
    "    return hidden_sequence, (h_t, c_t)\n",
    "\n",
    "QLSTMBase.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d6b0af",
   "metadata": {},
   "source": [
    "## - classical quatum enhanced LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adc8fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLSTM(QLSTMBase):\n",
    "    def __init__(self, input_size, hidden_size, *, ctx):\n",
    "        super().__init__(input_size, hidden_size, ctx = ctx)\n",
    "    \n",
    "        # input gates\n",
    "        self.input_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                        n_qubits = 4, degree = 3, ctx = ctx) # 15\n",
    "        # forget gates\n",
    "        self.forget_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                         n_qubits = 4, degree = 3, ctx = ctx) # 15\n",
    "        # candidate\n",
    "        self.candidate_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                       n_qubits = 4, degree = 3, ctx = ctx) # 15\n",
    "        # output gates\n",
    "        self.output_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                         n_qubits = 4, degree = 3, ctx = ctx) # 15\n",
    "        \n",
    "    def forward(self, inputs, init_states = None):\n",
    "        hidden_sequence, (h_t, c_t) = super(QLSTM, self).forward(inputs, init_states)\n",
    "\n",
    "        for t in range(inputs.size()[0]):\n",
    "            x_t = inputs[t, :, :]\n",
    "            v_t = torch.cat((h_t[0], x_t), dim = 1)\n",
    "\n",
    "            # input gates\n",
    "            i_t = torch.sigmoid(self.input_circuit(v_t))\n",
    "            # forget gates\n",
    "            f_t = torch.sigmoid(self.forget_circuit(v_t))\n",
    "            # candidate for cell state update\n",
    "            g_t = torch.tanh(self.candidate_circuit(v_t))\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "\n",
    "            # output gates\n",
    "            o_t = torch.sigmoid(self.output_circuit(v_t))\n",
    "            # update output ht\n",
    "            h_t = o_t * (torch.tanh(c_t))\n",
    "\n",
    "            hidden_sequence.append(h_t)\n",
    "\n",
    "        # reshape hidden_seq p/ retornar\n",
    "        #\n",
    "        # [tensor([[[0.0444, ...]]] => tensor([[[0.0444, ...]]]\n",
    "        # \n",
    "        hidden_sequence = torch.cat(hidden_sequence, dim = 0)\n",
    "\n",
    "        return hidden_sequence, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28836475",
   "metadata": {},
   "source": [
    "## 2.4 Stacked QLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ec06c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class StackedQLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, *, num_layers = 1, ctx = None, mode = 'classical'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.qlstms = nn.Sequential(OrderedDict([\n",
    "            (f'QLSTM {i + 1}', QLSTM(input_size if i == 0 else hidden_size , hidden_size, ctx = ctx)) \n",
    "                for i in range(num_layers)\n",
    "        ]))\n",
    "\n",
    "    def forward(self, inputs, parameters = None):\n",
    "        outputs = None\n",
    "        \n",
    "        for i, qlstm in enumerate(self.qlstms):\n",
    "            if i != 0:\n",
    "                inputs = outputs\n",
    "            \n",
    "            outputs, parameters = qlstm(inputs, parameters)\n",
    "        \n",
    "        return outputs, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0389535c",
   "metadata": {},
   "source": [
    "# 3. Quantum Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "413150bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_output, *, num_layers = 1, ctx = None, mode = 'classical'):\n",
    "        super(QModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.qlstm = StackedQLSTM(input_size, hidden_size, \n",
    "                                  num_layers = num_layers, ctx = ctx, mode = mode)\n",
    "        self.predict = nn.Linear(hidden_size, num_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        # sequence lenth , batch_size, features length\n",
    "        # \n",
    "        h0 = torch.zeros(1, x.size(1), self.hidden_size)\n",
    "        c0 = torch.zeros(1, x.size(1), self.hidden_size)\n",
    "        \n",
    "        out, _ = self.qlstm(x, (h0, c0))\n",
    "        out = self.predict(out[0])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111aebe4",
   "metadata": {},
   "source": [
    "## 3.1 train QModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc1d1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "def train_model(model, datas, batch_size, *, loss_func, optimizer, epoch = 50):\n",
    "    losses = []\n",
    "    sampler = RandomSampler(datas, num_samples = batch_size)\n",
    "    \n",
    "    for step in range(epoch):\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for index in sampler:\n",
    "            batch_x, batch_y = datas[index][0], datas[index][1]\n",
    "            b_x = batch_x.unsqueeze(0)\n",
    "            b_y = batch_y.unsqueeze(0)\n",
    "            \n",
    "            output = model(b_x)\n",
    "\n",
    "            loss = loss_func(output, b_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {step + 1}/{epoch}: Loss: {train_loss / batch_size}')\n",
    "        losses.append(train_loss / batch_size)\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef07934f",
   "metadata": {},
   "source": [
    "## 3.2 Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57e9b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_E(actual, predict):\n",
    "    E = actual[0] - predict[0]\n",
    "    E = torch.norm(Tensor(E))\n",
    "    E /= torch.norm(Tensor(predict))\n",
    "    \n",
    "    return E\n",
    "\n",
    "def calculate_accuarcy(model, X_test, y_test, scaler=test_scaler):\n",
    "    n = len(X_test)\n",
    "    \n",
    "    err = 0.0\n",
    "    for i in range(0, n):\n",
    "        err += calculate_E(\n",
    "            scaler.inverse_transform(y_test[i:i + 1].data), # actual\n",
    "            scaler.inverse_transform(model(X_test[i:i + 1]).data) # predict\n",
    "        ) ** 2\n",
    "    err /= n\n",
    "    \n",
    "    return 1 - err ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6407f969",
   "metadata": {},
   "source": [
    "## 3.3 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "305c9471",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_size = 4\n",
    "window_size = features_size * 3 # \n",
    "predict_size = features_size # features\n",
    "\n",
    "input_size = window_size\n",
    "num_output = predict_size\n",
    "\n",
    "hidden_size = 32\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79660b22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: 1\n",
      "Epoch 1/100: Loss: 0.9723299384117127\n",
      "Epoch 2/100: Loss: 1.0063888758420945\n",
      "Epoch 3/100: Loss: 1.0292232930660248\n",
      "Epoch 4/100: Loss: 0.9582550615072251\n",
      "Epoch 5/100: Loss: 0.9980573385953904\n",
      "Epoch 6/100: Loss: 0.9545555770397186\n",
      "Epoch 7/100: Loss: 0.9440097123384475\n",
      "Epoch 8/100: Loss: 0.9670826137065888\n",
      "Epoch 9/100: Loss: 0.9390581727027894\n",
      "Epoch 10/100: Loss: 0.9651568114757538\n",
      "Epoch 11/100: Loss: 0.8976210087537766\n",
      "Epoch 12/100: Loss: 0.8730141162872315\n",
      "Epoch 13/100: Loss: 0.870593199133873\n",
      "Epoch 14/100: Loss: 0.8453139632940292\n",
      "Epoch 15/100: Loss: 0.7821796715259552\n",
      "Epoch 16/100: Loss: 0.7679004698991776\n",
      "Epoch 17/100: Loss: 0.7878747597336769\n",
      "Epoch 18/100: Loss: 0.781437149643898\n",
      "Epoch 19/100: Loss: 0.647131597995758\n",
      "Epoch 20/100: Loss: 0.6324775755405426\n",
      "Epoch 21/100: Loss: 0.7301140666007996\n",
      "Epoch 22/100: Loss: 0.5886983439326287\n",
      "Epoch 23/100: Loss: 0.5065693102777005\n",
      "Epoch 24/100: Loss: 0.44846390672028064\n",
      "Epoch 25/100: Loss: 0.484896843880415\n",
      "Epoch 26/100: Loss: 0.508930822648108\n",
      "Epoch 27/100: Loss: 0.523241581581533\n",
      "Epoch 28/100: Loss: 0.43114183340221646\n",
      "Epoch 29/100: Loss: 0.4729891261085868\n",
      "Epoch 30/100: Loss: 0.39122174233198165\n",
      "Epoch 31/100: Loss: 0.33596694003790617\n",
      "Epoch 32/100: Loss: 0.34795989813283085\n",
      "Epoch 33/100: Loss: 0.4476889674551785\n",
      "Epoch 34/100: Loss: 0.3393830577842891\n",
      "Epoch 35/100: Loss: 0.40877912659198046\n",
      "Epoch 36/100: Loss: 0.47976060598157344\n",
      "Epoch 37/100: Loss: 0.32164615937508645\n",
      "Epoch 38/100: Loss: 0.28302360279485583\n",
      "Epoch 39/100: Loss: 0.4254143968224525\n",
      "Epoch 40/100: Loss: 0.30211297478526833\n",
      "Epoch 41/100: Loss: 0.3750273540150374\n",
      "Epoch 42/100: Loss: 0.27513643498532475\n",
      "Epoch 43/100: Loss: 0.37824944232124835\n",
      "Epoch 44/100: Loss: 0.19326140967896208\n",
      "Epoch 45/100: Loss: 0.22226288579404355\n",
      "Epoch 46/100: Loss: 0.3314186635077931\n",
      "Epoch 47/100: Loss: 0.33531670942902564\n",
      "Epoch 48/100: Loss: 0.27150852780323476\n",
      "Epoch 49/100: Loss: 0.170836483273888\n",
      "Epoch 50/100: Loss: 0.22151034971175249\n",
      "Epoch 51/100: Loss: 0.28170953000662846\n",
      "Epoch 52/100: Loss: 0.28154750281246377\n",
      "Epoch 53/100: Loss: 0.17868170918518445\n",
      "Epoch 54/100: Loss: 0.2633638187704491\n",
      "Epoch 55/100: Loss: 0.3925877159304946\n",
      "Epoch 56/100: Loss: 0.28084905314899516\n",
      "Epoch 57/100: Loss: 0.3382797606929671\n",
      "Epoch 58/100: Loss: 0.28813617895357313\n",
      "Epoch 59/100: Loss: 0.2458047998836264\n",
      "Epoch 60/100: Loss: 0.2525097495985392\n",
      "Epoch 61/100: Loss: 0.37840443375389443\n",
      "Epoch 62/100: Loss: 0.125940597421868\n",
      "Epoch 63/100: Loss: 0.3089909204591095\n",
      "Epoch 64/100: Loss: 0.1716917964280583\n",
      "Epoch 65/100: Loss: 0.19966858226907788\n",
      "Epoch 66/100: Loss: 0.13963580987874594\n",
      "Epoch 67/100: Loss: 0.38347778088937046\n",
      "Epoch 68/100: Loss: 0.1488768502691528\n",
      "Epoch 69/100: Loss: 0.2820239205837424\n",
      "Epoch 70/100: Loss: 0.21988062831223942\n",
      "Epoch 71/100: Loss: 0.24822182401403553\n",
      "Epoch 72/100: Loss: 0.2807455445086816\n",
      "Epoch 73/100: Loss: 0.17098426376469433\n",
      "Epoch 74/100: Loss: 0.18033293560147284\n",
      "Epoch 75/100: Loss: 0.20867258688085713\n",
      "Epoch 76/100: Loss: 0.1185739743814338\n",
      "Epoch 77/100: Loss: 0.23633334749611096\n",
      "Epoch 78/100: Loss: 0.12870328349454213\n",
      "Epoch 79/100: Loss: 0.22240033062553266\n",
      "Epoch 80/100: Loss: 0.253467696509324\n",
      "Epoch 81/100: Loss: 0.1593743320176145\n",
      "Epoch 82/100: Loss: 0.22991647231974638\n",
      "Epoch 83/100: Loss: 0.23659928597626276\n",
      "Epoch 84/100: Loss: 0.21376071735285224\n",
      "Epoch 85/100: Loss: 0.25778761827386915\n",
      "Epoch 86/100: Loss: 0.19319362116511912\n",
      "Epoch 87/100: Loss: 0.13899696994340047\n",
      "Epoch 88/100: Loss: 0.2571728540933691\n",
      "Epoch 89/100: Loss: 0.10294438637210987\n",
      "Epoch 90/100: Loss: 0.17324155542301015\n",
      "Epoch 91/100: Loss: 0.13469714162056334\n",
      "Epoch 92/100: Loss: 0.19469382870011032\n",
      "Epoch 93/100: Loss: 0.08503708927892148\n",
      "Epoch 94/100: Loss: 0.08245386728813173\n",
      "Epoch 95/100: Loss: 0.1274637200811412\n",
      "Epoch 96/100: Loss: 0.15076657507597702\n",
      "Epoch 97/100: Loss: 0.11682918507503928\n",
      "Epoch 98/100: Loss: 0.06693917226657504\n",
      "Epoch 99/100: Loss: 0.07104062197322492\n",
      "Epoch 100/100: Loss: 0.13604199048641022\n",
      "time costs: 585.0346539020538\n",
      "--------------------\n",
      "training epoch: 2\n",
      "Epoch 1/100: Loss: 1.0049973547458648\n",
      "Epoch 2/100: Loss: 0.9824720799922944\n",
      "Epoch 3/100: Loss: 0.9926236361265183\n",
      "Epoch 4/100: Loss: 0.9723674803972244\n",
      "Epoch 5/100: Loss: 0.9553380995988846\n",
      "Epoch 6/100: Loss: 0.9920392632484436\n",
      "Epoch 7/100: Loss: 0.9872960060834884\n",
      "Epoch 8/100: Loss: 0.9629729419946671\n",
      "Epoch 9/100: Loss: 0.9402753412723541\n",
      "Epoch 10/100: Loss: 0.9156161576509476\n",
      "Epoch 11/100: Loss: 0.9375833362340927\n",
      "Epoch 12/100: Loss: 0.8983396172523499\n",
      "Epoch 13/100: Loss: 0.8708515405654907\n",
      "Epoch 14/100: Loss: 0.80526904463768\n",
      "Epoch 15/100: Loss: 0.8379112094640732\n",
      "Epoch 16/100: Loss: 0.8457037657499313\n",
      "Epoch 17/100: Loss: 0.8526864439249039\n",
      "Epoch 18/100: Loss: 0.7221801906824112\n",
      "Epoch 19/100: Loss: 0.7505391582846641\n",
      "Epoch 20/100: Loss: 0.6600429892539978\n",
      "Epoch 21/100: Loss: 0.7529053092002869\n",
      "Epoch 22/100: Loss: 0.5596390813589096\n",
      "Epoch 23/100: Loss: 0.593859426677227\n",
      "Epoch 24/100: Loss: 0.6408736228942871\n",
      "Epoch 25/100: Loss: 0.5379647448658943\n",
      "Epoch 26/100: Loss: 0.5967839881777763\n",
      "Epoch 27/100: Loss: 0.45778035297989844\n",
      "Epoch 28/100: Loss: 0.537881351262331\n",
      "Epoch 29/100: Loss: 0.6291123576462269\n",
      "Epoch 30/100: Loss: 0.4367371253669262\n",
      "Epoch 31/100: Loss: 0.3707102552056313\n",
      "Epoch 32/100: Loss: 0.4445099614560604\n",
      "Epoch 33/100: Loss: 0.34352847561240196\n",
      "Epoch 34/100: Loss: 0.31575760915875434\n",
      "Epoch 35/100: Loss: 0.3903870705515146\n",
      "Epoch 36/100: Loss: 0.4323279408738017\n",
      "Epoch 37/100: Loss: 0.382316929101944\n",
      "Epoch 38/100: Loss: 0.35336385369300843\n",
      "Epoch 39/100: Loss: 0.3458529725670815\n",
      "Epoch 40/100: Loss: 0.3102053502574563\n",
      "Epoch 41/100: Loss: 0.26709349527955056\n",
      "Epoch 42/100: Loss: 0.41203314289450643\n",
      "Epoch 43/100: Loss: 0.35450712200254203\n",
      "Epoch 44/100: Loss: 0.27854917049407957\n",
      "Epoch 45/100: Loss: 0.23903350764885545\n",
      "Epoch 46/100: Loss: 0.34164629951119424\n",
      "Epoch 47/100: Loss: 0.15582287502475084\n",
      "Epoch 48/100: Loss: 0.18118723041843623\n",
      "Epoch 49/100: Loss: 0.3334619577974081\n",
      "Epoch 50/100: Loss: 0.29722880413755776\n",
      "Epoch 51/100: Loss: 0.26068310244008897\n",
      "Epoch 52/100: Loss: 0.22600585790351033\n",
      "Epoch 53/100: Loss: 0.15225714347325264\n",
      "Epoch 54/100: Loss: 0.22789582307450473\n",
      "Epoch 55/100: Loss: 0.1494441802147776\n",
      "Epoch 56/100: Loss: 0.17697577541694046\n",
      "Epoch 57/100: Loss: 0.18970757815986872\n",
      "Epoch 58/100: Loss: 0.1540204373188317\n",
      "Epoch 59/100: Loss: 0.18140297019854187\n",
      "Epoch 60/100: Loss: 0.16380726522766054\n",
      "Epoch 61/100: Loss: 0.15222126487642526\n",
      "Epoch 62/100: Loss: 0.1456715354695916\n",
      "Epoch 63/100: Loss: 0.12351553766056896\n",
      "Epoch 64/100: Loss: 0.1186471805209294\n",
      "Epoch 65/100: Loss: 0.15440593417733908\n",
      "Epoch 66/100: Loss: 0.0686185672879219\n",
      "Epoch 67/100: Loss: 0.0589418679010123\n",
      "Epoch 68/100: Loss: 0.12661394393071532\n",
      "Epoch 69/100: Loss: 0.08230932616861537\n",
      "Epoch 70/100: Loss: 0.08797488636337221\n",
      "Epoch 71/100: Loss: 0.053837152174673976\n",
      "Epoch 72/100: Loss: 0.058951035418431275\n",
      "Epoch 73/100: Loss: 0.050926925119711086\n",
      "Epoch 74/100: Loss: 0.06362770467894734\n",
      "Epoch 75/100: Loss: 0.031208183558192104\n",
      "Epoch 76/100: Loss: 0.029339609754970297\n",
      "Epoch 77/100: Loss: 0.03616555767293903\n",
      "Epoch 78/100: Loss: 0.04813321107358206\n",
      "Epoch 79/100: Loss: 0.026927820072160102\n",
      "Epoch 80/100: Loss: 0.015524589200504124\n",
      "Epoch 81/100: Loss: 0.013702643860597163\n",
      "Epoch 82/100: Loss: 0.023880779378305305\n",
      "Epoch 83/100: Loss: 0.02437598787655588\n",
      "Epoch 84/100: Loss: 0.013932439440395683\n",
      "Epoch 85/100: Loss: 0.021562908051419073\n",
      "Epoch 86/100: Loss: 0.006457056191720767\n",
      "Epoch 87/100: Loss: 0.012748813432699535\n",
      "Epoch 88/100: Loss: 0.009685033090499928\n",
      "Epoch 89/100: Loss: 0.006436145375846536\n",
      "Epoch 90/100: Loss: 0.010267944737279322\n",
      "Epoch 91/100: Loss: 0.007012691584532149\n",
      "Epoch 92/100: Loss: 0.0039414339975337494\n",
      "Epoch 93/100: Loss: 0.002127138601645129\n",
      "Epoch 94/100: Loss: 0.0036260232507629555\n",
      "Epoch 95/100: Loss: 0.00212070983434387\n",
      "Epoch 96/100: Loss: 0.002000014970144548\n",
      "Epoch 97/100: Loss: 0.0013080067870760103\n",
      "Epoch 98/100: Loss: 0.0013394284833339043\n",
      "Epoch 99/100: Loss: 0.0015895897457085084\n",
      "Epoch 100/100: Loss: 0.0011235993650188902\n",
      "time costs: 597.5207495689392\n",
      "--------------------\n",
      "training epoch: 3\n",
      "Epoch 1/100: Loss: 1.0085896670818328\n",
      "Epoch 2/100: Loss: 1.006341704726219\n",
      "Epoch 3/100: Loss: 0.9994460135698319\n",
      "Epoch 4/100: Loss: 0.9818723142147064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: Loss: 0.9831278145313262\n",
      "Epoch 6/100: Loss: 0.9677993535995484\n",
      "Epoch 7/100: Loss: 0.9584271013736725\n",
      "Epoch 8/100: Loss: 0.9325637489557266\n",
      "Epoch 9/100: Loss: 0.9427330106496811\n",
      "Epoch 10/100: Loss: 0.9330524116754532\n",
      "Epoch 11/100: Loss: 0.8760593622922898\n",
      "Epoch 12/100: Loss: 0.9399225443601609\n",
      "Epoch 13/100: Loss: 0.8966864466667175\n",
      "Epoch 14/100: Loss: 0.8192639410495758\n",
      "Epoch 15/100: Loss: 0.7663145929574966\n",
      "Epoch 16/100: Loss: 0.8500656396150589\n",
      "Epoch 17/100: Loss: 0.7926865816116333\n",
      "Epoch 18/100: Loss: 0.7880895212292671\n",
      "Epoch 19/100: Loss: 0.7331205755472183\n",
      "Epoch 20/100: Loss: 0.7722426921129226\n",
      "Epoch 21/100: Loss: 0.7138788804411889\n",
      "Epoch 22/100: Loss: 0.6147872179746627\n",
      "Epoch 23/100: Loss: 0.6849691867828369\n",
      "Epoch 24/100: Loss: 0.573716177046299\n",
      "Epoch 25/100: Loss: 0.7133512813597918\n",
      "Epoch 26/100: Loss: 0.5748337283730507\n",
      "Epoch 27/100: Loss: 0.37910229004919527\n",
      "Epoch 28/100: Loss: 0.609294762276113\n",
      "Epoch 29/100: Loss: 0.573967195302248\n",
      "Epoch 30/100: Loss: 0.4193089068867266\n",
      "Epoch 31/100: Loss: 0.5141113051213324\n",
      "Epoch 32/100: Loss: 0.5196673715952784\n",
      "Epoch 33/100: Loss: 0.5649358373135328\n",
      "Epoch 34/100: Loss: 0.3508907345123589\n",
      "Epoch 35/100: Loss: 0.5841759747359901\n",
      "Epoch 36/100: Loss: 0.45800594296306374\n",
      "Epoch 37/100: Loss: 0.5626105776056647\n",
      "Epoch 38/100: Loss: 0.4252406762912869\n",
      "Epoch 39/100: Loss: 0.5806960656307638\n",
      "Epoch 40/100: Loss: 0.427292390819639\n",
      "Epoch 41/100: Loss: 0.4151318630203605\n",
      "Epoch 42/100: Loss: 0.43205280415713787\n",
      "Epoch 43/100: Loss: 0.3916248084977269\n",
      "Epoch 44/100: Loss: 0.38916303776204586\n",
      "Epoch 45/100: Loss: 0.39939163755625484\n",
      "Epoch 46/100: Loss: 0.34597169551998375\n",
      "Epoch 47/100: Loss: 0.28415815383195875\n",
      "Epoch 48/100: Loss: 0.25872492883354425\n",
      "Epoch 49/100: Loss: 0.21769073009490966\n",
      "Epoch 50/100: Loss: 0.33030223692767324\n",
      "Epoch 51/100: Loss: 0.2411905551329255\n",
      "Epoch 52/100: Loss: 0.2814543990418315\n",
      "Epoch 53/100: Loss: 0.23123808344826102\n",
      "Epoch 54/100: Loss: 0.2301798633299768\n",
      "Epoch 55/100: Loss: 0.19469857020303608\n",
      "Epoch 56/100: Loss: 0.19971684459596872\n",
      "Epoch 57/100: Loss: 0.1379880288615823\n",
      "Epoch 58/100: Loss: 0.16127821253612637\n",
      "Epoch 59/100: Loss: 0.14977574734948576\n",
      "Epoch 60/100: Loss: 0.20235953778028487\n",
      "Epoch 61/100: Loss: 0.13576194001361727\n",
      "Epoch 62/100: Loss: 0.10466903373599053\n",
      "Epoch 63/100: Loss: 0.10696586773265153\n",
      "Epoch 64/100: Loss: 0.08480348652228713\n",
      "Epoch 65/100: Loss: 0.08892199117690325\n",
      "Epoch 66/100: Loss: 0.06155315942596644\n",
      "Epoch 67/100: Loss: 0.0778264770982787\n",
      "Epoch 68/100: Loss: 0.06945656530442648\n",
      "Epoch 69/100: Loss: 0.03771463376178872\n",
      "Epoch 70/100: Loss: 0.042649281886406244\n",
      "Epoch 71/100: Loss: 0.033666646041092464\n",
      "Epoch 72/100: Loss: 0.03488473369143321\n",
      "Epoch 73/100: Loss: 0.04087812642683275\n",
      "Epoch 74/100: Loss: 0.05275961015140638\n",
      "Epoch 75/100: Loss: 0.0354117738083005\n",
      "Epoch 76/100: Loss: 0.020702733687357977\n",
      "Epoch 77/100: Loss: 0.010641553993627895\n",
      "Epoch 78/100: Loss: 0.013024907745420932\n",
      "Epoch 79/100: Loss: 0.02355665186769329\n",
      "Epoch 80/100: Loss: 0.012745424096647185\n",
      "Epoch 81/100: Loss: 0.010111525055253878\n",
      "Epoch 82/100: Loss: 0.008410660997105879\n",
      "Epoch 83/100: Loss: 0.009460869467329758\n",
      "Epoch 84/100: Loss: 0.006780339524630108\n",
      "Epoch 85/100: Loss: 0.009941512738441816\n",
      "Epoch 86/100: Loss: 0.0053581052059598734\n",
      "Epoch 87/100: Loss: 0.0047175653162412345\n",
      "Epoch 88/100: Loss: 0.003917929017916322\n",
      "Epoch 89/100: Loss: 0.0025229083745216484\n",
      "Epoch 90/100: Loss: 0.0025533013831591234\n",
      "Epoch 91/100: Loss: 0.0016939826469752005\n",
      "Epoch 92/100: Loss: 0.001954035387097974\n",
      "Epoch 93/100: Loss: 0.0013820812815538374\n",
      "Epoch 94/100: Loss: 0.0013928500935435294\n",
      "Epoch 95/100: Loss: 0.0011376287799976126\n",
      "Epoch 96/100: Loss: 0.0007455284639945603\n",
      "Epoch 97/100: Loss: 0.0008377233796636574\n",
      "Epoch 98/100: Loss: 0.0007198814320872771\n",
      "Epoch 99/100: Loss: 0.0006171594119223301\n",
      "Epoch 100/100: Loss: 0.0005621578065984068\n",
      "time costs: 597.4756889343262\n",
      "--------------------\n",
      "training epoch: 4\n",
      "Epoch 1/100: Loss: 0.9945880830287933\n",
      "Epoch 2/100: Loss: 1.005308699607849\n",
      "Epoch 3/100: Loss: 0.9778369247913361\n",
      "Epoch 4/100: Loss: 0.9417629271745682\n",
      "Epoch 5/100: Loss: 0.9354716569185257\n",
      "Epoch 6/100: Loss: 0.923987740278244\n",
      "Epoch 7/100: Loss: 0.9157378077507019\n",
      "Epoch 8/100: Loss: 0.9195602685213089\n",
      "Epoch 9/100: Loss: 0.8758403718471527\n",
      "Epoch 10/100: Loss: 0.8752947449684143\n",
      "Epoch 11/100: Loss: 0.8498206704854965\n",
      "Epoch 12/100: Loss: 0.7811392337083817\n",
      "Epoch 13/100: Loss: 0.7699569135904312\n",
      "Epoch 14/100: Loss: 0.7755002617835999\n",
      "Epoch 15/100: Loss: 0.7047691732645035\n",
      "Epoch 16/100: Loss: 0.6333125203847885\n",
      "Epoch 17/100: Loss: 0.6818089783191681\n",
      "Epoch 18/100: Loss: 0.5960096672177315\n",
      "Epoch 19/100: Loss: 0.5918208941817283\n",
      "Epoch 20/100: Loss: 0.6408959843218327\n",
      "Epoch 21/100: Loss: 0.5287840195000172\n",
      "Epoch 22/100: Loss: 0.5958459302783012\n",
      "Epoch 23/100: Loss: 0.678876806795597\n",
      "Epoch 24/100: Loss: 0.6071809016168117\n",
      "Epoch 25/100: Loss: 0.5441165283322335\n",
      "Epoch 26/100: Loss: 0.5582187324762344\n",
      "Epoch 27/100: Loss: 0.511921214312315\n",
      "Epoch 28/100: Loss: 0.5374715752899647\n",
      "Epoch 29/100: Loss: 0.4140912849456072\n",
      "Epoch 30/100: Loss: 0.5397968143224716\n",
      "Epoch 31/100: Loss: 0.5263713609427214\n",
      "Epoch 32/100: Loss: 0.5856710854917765\n",
      "Epoch 33/100: Loss: 0.41717881225049497\n",
      "Epoch 34/100: Loss: 0.5077451348304749\n",
      "Epoch 35/100: Loss: 0.5160737298429012\n",
      "Epoch 36/100: Loss: 0.46457477770745753\n",
      "Epoch 37/100: Loss: 0.39524007253348825\n",
      "Epoch 38/100: Loss: 0.3525984171777964\n",
      "Epoch 39/100: Loss: 0.40379685945808885\n",
      "Epoch 40/100: Loss: 0.39386259485036135\n",
      "Epoch 41/100: Loss: 0.2503714354708791\n",
      "Epoch 42/100: Loss: 0.3207276495173573\n",
      "Epoch 43/100: Loss: 0.2697472432628274\n",
      "Epoch 44/100: Loss: 0.26345203155651686\n",
      "Epoch 45/100: Loss: 0.2684130437672138\n",
      "Epoch 46/100: Loss: 0.3733310787938535\n",
      "Epoch 47/100: Loss: 0.3159056268632412\n",
      "Epoch 48/100: Loss: 0.205776627920568\n",
      "Epoch 49/100: Loss: 0.25521093253046273\n",
      "Epoch 50/100: Loss: 0.27901524044573306\n",
      "Epoch 51/100: Loss: 0.22888618037104608\n",
      "Epoch 52/100: Loss: 0.2317211820743978\n",
      "Epoch 53/100: Loss: 0.17314648851752282\n",
      "Epoch 54/100: Loss: 0.1270281232893467\n",
      "Epoch 55/100: Loss: 0.18517787149176002\n",
      "Epoch 56/100: Loss: 0.14426146030891687\n",
      "Epoch 57/100: Loss: 0.10566914479713888\n",
      "Epoch 58/100: Loss: 0.18912833156064152\n",
      "Epoch 59/100: Loss: 0.11119345978368074\n",
      "Epoch 60/100: Loss: 0.06866159782512113\n",
      "Epoch 61/100: Loss: 0.17126782052218914\n",
      "Epoch 62/100: Loss: 0.08567053782753646\n",
      "Epoch 63/100: Loss: 0.13177843503654002\n",
      "Epoch 64/100: Loss: 0.06871983121382072\n",
      "Epoch 65/100: Loss: 0.0749692133278586\n",
      "Epoch 66/100: Loss: 0.05753381109097973\n",
      "Epoch 67/100: Loss: 0.07599143662228017\n",
      "Epoch 68/100: Loss: 0.03970114706025925\n",
      "Epoch 69/100: Loss: 0.05401907281484455\n",
      "Epoch 70/100: Loss: 0.04152336122933775\n",
      "Epoch 71/100: Loss: 0.03440827233134769\n",
      "Epoch 72/100: Loss: 0.0389523341669701\n",
      "Epoch 73/100: Loss: 0.02680684743972961\n",
      "Epoch 74/100: Loss: 0.035995016811648385\n",
      "Epoch 75/100: Loss: 0.027243564202217387\n",
      "Epoch 76/100: Loss: 0.012858754667831818\n",
      "Epoch 77/100: Loss: 0.009608184568060096\n",
      "Epoch 78/100: Loss: 0.024655012297444046\n",
      "Epoch 79/100: Loss: 0.017417327981820562\n",
      "Epoch 80/100: Loss: 0.015923934998863843\n",
      "Epoch 81/100: Loss: 0.009042327335555456\n",
      "Epoch 82/100: Loss: 0.007848170353099704\n",
      "Epoch 83/100: Loss: 0.006683947235251253\n",
      "Epoch 84/100: Loss: 0.004385784611804411\n",
      "Epoch 85/100: Loss: 0.0026851820748561295\n",
      "Epoch 86/100: Loss: 0.003088118685991503\n",
      "Epoch 87/100: Loss: 0.003052436468351516\n",
      "Epoch 88/100: Loss: 0.0021774782489956124\n",
      "Epoch 89/100: Loss: 0.0017854469899248216\n",
      "Epoch 90/100: Loss: 0.0016025401471779332\n",
      "Epoch 91/100: Loss: 0.002493622395377315\n",
      "Epoch 92/100: Loss: 0.0016379604407120497\n",
      "Epoch 93/100: Loss: 0.0010323649628844577\n",
      "Epoch 94/100: Loss: 0.0012862235294960555\n",
      "Epoch 95/100: Loss: 0.0010253173481032719\n",
      "Epoch 96/100: Loss: 0.0008933920481467794\n",
      "Epoch 97/100: Loss: 0.0010838990376214497\n",
      "Epoch 98/100: Loss: 0.0006424062961741583\n",
      "Epoch 99/100: Loss: 0.000519380092737265\n",
      "Epoch 100/100: Loss: 0.0004371974986497662\n",
      "time costs: 606.6776251792908\n",
      "--------------------\n",
      "training epoch: 5\n",
      "Epoch 1/100: Loss: 1.007788148522377\n",
      "Epoch 2/100: Loss: 0.9364115715026855\n",
      "Epoch 3/100: Loss: 0.9963063269853591\n",
      "Epoch 4/100: Loss: 1.020223793387413\n",
      "Epoch 5/100: Loss: 0.9697695851325989\n",
      "Epoch 6/100: Loss: 0.9929611414670945\n",
      "Epoch 7/100: Loss: 0.9327902555465698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: Loss: 0.891581866145134\n",
      "Epoch 9/100: Loss: 0.9066569834947587\n",
      "Epoch 10/100: Loss: 0.9293412625789642\n",
      "Epoch 11/100: Loss: 0.934285643696785\n",
      "Epoch 12/100: Loss: 0.8893634974956512\n",
      "Epoch 13/100: Loss: 0.8684847861528396\n",
      "Epoch 14/100: Loss: 0.8443518042564392\n",
      "Epoch 15/100: Loss: 0.8150879591703415\n",
      "Epoch 16/100: Loss: 0.772573071718216\n",
      "Epoch 17/100: Loss: 0.7531217336654663\n",
      "Epoch 18/100: Loss: 0.698298242688179\n",
      "Epoch 19/100: Loss: 0.6620720803737641\n",
      "Epoch 20/100: Loss: 0.63815658390522\n",
      "Epoch 21/100: Loss: 0.6442943394184113\n",
      "Epoch 22/100: Loss: 0.5882562011480331\n",
      "Epoch 23/100: Loss: 0.530777333676815\n",
      "Epoch 24/100: Loss: 0.503069531917572\n",
      "Epoch 25/100: Loss: 0.45598665773868563\n",
      "Epoch 26/100: Loss: 0.42353432923555373\n",
      "Epoch 27/100: Loss: 0.3788626030087471\n",
      "Epoch 28/100: Loss: 0.34171026647090913\n",
      "Epoch 29/100: Loss: 0.36263424456119536\n",
      "Epoch 30/100: Loss: 0.28884700387716294\n",
      "Epoch 31/100: Loss: 0.24205537512898445\n",
      "Epoch 32/100: Loss: 0.19841802679002285\n",
      "Epoch 33/100: Loss: 0.2192355167120695\n",
      "Epoch 34/100: Loss: 0.19678915590047835\n",
      "Epoch 35/100: Loss: 0.16712302360683678\n",
      "Epoch 36/100: Loss: 0.15236852318048477\n",
      "Epoch 37/100: Loss: 0.12205161023885011\n",
      "Epoch 38/100: Loss: 0.12750594615936278\n",
      "Epoch 39/100: Loss: 0.08591339131817222\n",
      "Epoch 40/100: Loss: 0.07457939526066185\n",
      "Epoch 41/100: Loss: 0.06253050328232349\n",
      "Epoch 42/100: Loss: 0.04856041895691306\n",
      "Epoch 43/100: Loss: 0.039811917138285934\n",
      "Epoch 44/100: Loss: 0.036949812341481446\n",
      "Epoch 45/100: Loss: 0.03163577001541853\n",
      "Epoch 46/100: Loss: 0.02333735186839476\n",
      "Epoch 47/100: Loss: 0.02253963619004935\n",
      "Epoch 48/100: Loss: 0.015163763705641032\n",
      "Epoch 49/100: Loss: 0.008709315292071551\n",
      "Epoch 50/100: Loss: 0.007413659663870931\n",
      "Epoch 51/100: Loss: 0.007617991403094493\n",
      "Epoch 52/100: Loss: 0.0042047150724101815\n",
      "Epoch 53/100: Loss: 0.002505340432981029\n",
      "Epoch 54/100: Loss: 0.0024444448892609215\n",
      "Epoch 55/100: Loss: 0.0016866385092725977\n",
      "Epoch 56/100: Loss: 0.0014633072802098467\n",
      "Epoch 57/100: Loss: 0.0008530913262802641\n",
      "Epoch 58/100: Loss: 0.000973975988017628\n",
      "Epoch 59/100: Loss: 0.0006346492395095993\n",
      "Epoch 60/100: Loss: 0.0007502238448068965\n",
      "Epoch 61/100: Loss: 0.000635812296786753\n",
      "Epoch 62/100: Loss: 0.0004691562018706463\n",
      "Epoch 63/100: Loss: 0.0005754870959208347\n",
      "Epoch 64/100: Loss: 0.00036860934324067784\n",
      "Epoch 65/100: Loss: 0.0003466549290351395\n",
      "Epoch 66/100: Loss: 0.00040088598761940376\n",
      "Epoch 67/100: Loss: 0.00044753361780749403\n",
      "Epoch 68/100: Loss: 0.000380680811485945\n",
      "Epoch 69/100: Loss: 0.00037853323674426066\n",
      "Epoch 70/100: Loss: 0.0003143116478895536\n",
      "Epoch 71/100: Loss: 0.00030768515962336095\n",
      "Epoch 72/100: Loss: 0.00036480539129115643\n",
      "Epoch 73/100: Loss: 0.0003856600729704951\n",
      "Epoch 74/100: Loss: 0.0004075533994182479\n",
      "Epoch 75/100: Loss: 0.00039396613374265144\n",
      "Epoch 76/100: Loss: 0.00036425361531655653\n",
      "Epoch 77/100: Loss: 0.0003747454267795547\n",
      "Epoch 78/100: Loss: 0.0003510405572342279\n",
      "Epoch 79/100: Loss: 0.00031122869113460184\n",
      "Epoch 80/100: Loss: 0.00028534497832879426\n",
      "Epoch 81/100: Loss: 0.0004016828132989758\n",
      "Epoch 82/100: Loss: 0.00044035020205228647\n",
      "Epoch 83/100: Loss: 0.0003555451439751778\n",
      "Epoch 84/100: Loss: 0.00027784852627519283\n",
      "Epoch 85/100: Loss: 0.0002479207629221492\n",
      "Epoch 86/100: Loss: 0.00040940571379906033\n",
      "Epoch 87/100: Loss: 0.00048471520349266937\n",
      "Epoch 88/100: Loss: 0.000425548669136333\n",
      "Epoch 89/100: Loss: 0.00027605203572420576\n",
      "Epoch 90/100: Loss: 0.000382518097353568\n",
      "Epoch 91/100: Loss: 0.00031785542632860596\n",
      "Epoch 92/100: Loss: 0.0002517555900340085\n",
      "Epoch 93/100: Loss: 0.0002771901285086642\n",
      "Epoch 94/100: Loss: 0.0004501090146732167\n",
      "Epoch 95/100: Loss: 0.0002566537987149786\n",
      "Epoch 96/100: Loss: 0.0002890500012654229\n",
      "Epoch 97/100: Loss: 0.00036033728611073457\n",
      "Epoch 98/100: Loss: 0.00038937933541092205\n",
      "Epoch 99/100: Loss: 0.0005045126854383852\n",
      "Epoch 100/100: Loss: 0.0004165701734564209\n",
      "time costs: 608.7108809947968\n",
      "--------------------\n",
      "training epoch: 6\n",
      "Epoch 1/100: Loss: 1.0147847354412078\n",
      "Epoch 2/100: Loss: 0.9935085654258728\n",
      "Epoch 3/100: Loss: 0.9939271092414856\n",
      "Epoch 4/100: Loss: 0.9933481574058532\n",
      "Epoch 5/100: Loss: 0.972265574336052\n",
      "Epoch 6/100: Loss: 0.9633144825696945\n",
      "Epoch 7/100: Loss: 0.9322502851486206\n",
      "Epoch 8/100: Loss: 0.9318199753761292\n",
      "Epoch 9/100: Loss: 0.9443992346525192\n",
      "Epoch 10/100: Loss: 0.9073982924222946\n",
      "Epoch 11/100: Loss: 0.8842756778001786\n",
      "Epoch 12/100: Loss: 0.8725562185049057\n",
      "Epoch 13/100: Loss: 0.8758396416902542\n",
      "Epoch 14/100: Loss: 0.8299991250038147\n",
      "Epoch 15/100: Loss: 0.7961175113916397\n",
      "Epoch 16/100: Loss: 0.764707264304161\n",
      "Epoch 17/100: Loss: 0.7656314268708229\n",
      "Epoch 18/100: Loss: 0.7903843343257904\n",
      "Epoch 19/100: Loss: 0.6294189557433129\n",
      "Epoch 20/100: Loss: 0.7100814551115036\n",
      "Epoch 21/100: Loss: 0.6183321148157119\n",
      "Epoch 22/100: Loss: 0.6446381524205208\n",
      "Epoch 23/100: Loss: 0.5669241458177566\n",
      "Epoch 24/100: Loss: 0.6025069445371628\n",
      "Epoch 25/100: Loss: 0.4260925531387329\n",
      "Epoch 26/100: Loss: 0.486444079130888\n",
      "Epoch 27/100: Loss: 0.4441944882273674\n",
      "Epoch 28/100: Loss: 0.5123962834477425\n",
      "Epoch 29/100: Loss: 0.332448935508728\n",
      "Epoch 30/100: Loss: 0.3516387056559324\n",
      "Epoch 31/100: Loss: 0.3580633528530598\n",
      "Epoch 32/100: Loss: 0.27774481792002914\n",
      "Epoch 33/100: Loss: 0.17999833039939403\n",
      "Epoch 34/100: Loss: 0.2816713474690914\n",
      "Epoch 35/100: Loss: 0.1815525982528925\n",
      "Epoch 36/100: Loss: 0.23952345354482532\n",
      "Epoch 37/100: Loss: 0.3032955045811832\n",
      "Epoch 38/100: Loss: 0.3484106037765741\n",
      "Epoch 39/100: Loss: 0.21353228744119407\n",
      "Epoch 40/100: Loss: 0.27006075854878875\n",
      "Epoch 41/100: Loss: 0.28249894343316556\n",
      "Epoch 42/100: Loss: 0.22433271205518396\n",
      "Epoch 43/100: Loss: 0.23800775152631104\n",
      "Epoch 44/100: Loss: 0.10841561378911138\n",
      "Epoch 45/100: Loss: 0.18547119577415289\n",
      "Epoch 46/100: Loss: 0.2300823177676648\n",
      "Epoch 47/100: Loss: 0.20195687904488296\n",
      "Epoch 48/100: Loss: 0.1963106963550672\n",
      "Epoch 49/100: Loss: 0.20904796549584717\n",
      "Epoch 50/100: Loss: 0.1844560796627775\n",
      "Epoch 51/100: Loss: 0.19008347055932973\n",
      "Epoch 52/100: Loss: 0.1793524573702598\n",
      "Epoch 53/100: Loss: 0.1906420544779394\n",
      "Epoch 54/100: Loss: 0.18540687552886084\n",
      "Epoch 55/100: Loss: 0.1887459595222026\n",
      "Epoch 56/100: Loss: 0.12888210404198616\n",
      "Epoch 57/100: Loss: 0.1960148686543107\n",
      "Epoch 58/100: Loss: 0.19902951358817517\n",
      "Epoch 59/100: Loss: 0.222996219759807\n",
      "Epoch 60/100: Loss: 0.1398056471487507\n",
      "Epoch 61/100: Loss: 0.14090956709260355\n",
      "Epoch 62/100: Loss: 0.0735785371682141\n",
      "Epoch 63/100: Loss: 0.11421689696289831\n",
      "Epoch 64/100: Loss: 0.14778691593382973\n",
      "Epoch 65/100: Loss: 0.11184837601613254\n",
      "Epoch 66/100: Loss: 0.14984076715190894\n",
      "Epoch 67/100: Loss: 0.10141707077273168\n",
      "Epoch 68/100: Loss: 0.0693540423468221\n",
      "Epoch 69/100: Loss: 0.05343732952605933\n",
      "Epoch 70/100: Loss: 0.055897479341365394\n",
      "Epoch 71/100: Loss: 0.04724344768328592\n",
      "Epoch 72/100: Loss: 0.07232625223114156\n",
      "Epoch 73/100: Loss: 0.040846218634123944\n",
      "Epoch 74/100: Loss: 0.04081427523342427\n",
      "Epoch 75/100: Loss: 0.03405896215735993\n",
      "Epoch 76/100: Loss: 0.06457697118603392\n",
      "Epoch 77/100: Loss: 0.04016753982868977\n",
      "Epoch 78/100: Loss: 0.03044920320244273\n",
      "Epoch 79/100: Loss: 0.041456529572496945\n",
      "Epoch 80/100: Loss: 0.03092274951050058\n",
      "Epoch 81/100: Loss: 0.02968217168818228\n",
      "Epoch 82/100: Loss: 0.02146024544490501\n",
      "Epoch 83/100: Loss: 0.02200187696435023\n",
      "Epoch 84/100: Loss: 0.019412428699070006\n",
      "Epoch 85/100: Loss: 0.02870328104472719\n",
      "Epoch 86/100: Loss: 0.017137674069817876\n",
      "Epoch 87/100: Loss: 0.009822124037600587\n",
      "Epoch 88/100: Loss: 0.013083011176786386\n",
      "Epoch 89/100: Loss: 0.012817579037800897\n",
      "Epoch 90/100: Loss: 0.008111429309064989\n",
      "Epoch 91/100: Loss: 0.008774065886973404\n",
      "Epoch 92/100: Loss: 0.006209077590756351\n",
      "Epoch 93/100: Loss: 0.005248127583763562\n",
      "Epoch 94/100: Loss: 0.004153369750565616\n",
      "Epoch 95/100: Loss: 0.004067124431458069\n",
      "Epoch 96/100: Loss: 0.005172474549908657\n",
      "Epoch 97/100: Loss: 0.002528495831938926\n",
      "Epoch 98/100: Loss: 0.003432132283342071\n",
      "Epoch 99/100: Loss: 0.0017567161630722694\n",
      "Epoch 100/100: Loss: 0.0021430038737889844\n",
      "time costs: 604.5972380638123\n",
      "--------------------\n",
      "training epoch: 7\n",
      "Epoch 1/100: Loss: 0.9492487341165543\n",
      "Epoch 2/100: Loss: 0.9767456412315368\n",
      "Epoch 3/100: Loss: 0.9896146148443222\n",
      "Epoch 4/100: Loss: 0.9905934989452362\n",
      "Epoch 5/100: Loss: 0.9990379005670548\n",
      "Epoch 6/100: Loss: 0.9741914838552475\n",
      "Epoch 7/100: Loss: 0.9512549132108689\n",
      "Epoch 8/100: Loss: 0.916726353764534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: Loss: 0.9146692305803299\n",
      "Epoch 10/100: Loss: 0.8915426045656204\n",
      "Epoch 11/100: Loss: 0.8876475691795349\n",
      "Epoch 12/100: Loss: 0.8809503048658371\n",
      "Epoch 13/100: Loss: 0.8332865625619889\n",
      "Epoch 14/100: Loss: 0.8460574507713318\n",
      "Epoch 15/100: Loss: 0.8591940224170684\n",
      "Epoch 16/100: Loss: 0.787364149093628\n",
      "Epoch 17/100: Loss: 0.7275647342205047\n",
      "Epoch 18/100: Loss: 0.701690885424614\n",
      "Epoch 19/100: Loss: 0.6729546278715134\n",
      "Epoch 20/100: Loss: 0.6532307177782058\n",
      "Epoch 21/100: Loss: 0.6301151156425476\n",
      "Epoch 22/100: Loss: 0.5933316364884377\n",
      "Epoch 23/100: Loss: 0.5145813301205635\n",
      "Epoch 24/100: Loss: 0.5549816086888313\n",
      "Epoch 25/100: Loss: 0.5265016689896583\n",
      "Epoch 26/100: Loss: 0.47532795667648314\n",
      "Epoch 27/100: Loss: 0.4184799000620842\n",
      "Epoch 28/100: Loss: 0.3797062933444977\n",
      "Epoch 29/100: Loss: 0.374156603962183\n",
      "Epoch 30/100: Loss: 0.36095817387104034\n",
      "Epoch 31/100: Loss: 0.3797584153711796\n",
      "Epoch 32/100: Loss: 0.32597731351852416\n",
      "Epoch 33/100: Loss: 0.30433495715260506\n",
      "Epoch 34/100: Loss: 0.2677552789449692\n",
      "Epoch 35/100: Loss: 0.2530198346823454\n",
      "Epoch 36/100: Loss: 0.21385503765195607\n",
      "Epoch 37/100: Loss: 0.1770364636555314\n",
      "Epoch 38/100: Loss: 0.2295916685834527\n",
      "Epoch 39/100: Loss: 0.19266226282343268\n",
      "Epoch 40/100: Loss: 0.16559365708380938\n",
      "Epoch 41/100: Loss: 0.15781394187361003\n",
      "Epoch 42/100: Loss: 0.14231754448264838\n",
      "Epoch 43/100: Loss: 0.14741388726979493\n",
      "Epoch 44/100: Loss: 0.11345340609550476\n",
      "Epoch 45/100: Loss: 0.10439657142851502\n",
      "Epoch 46/100: Loss: 0.11545211839256808\n",
      "Epoch 47/100: Loss: 0.09701772378757596\n",
      "Epoch 48/100: Loss: 0.09964006142690778\n",
      "Epoch 49/100: Loss: 0.0625487683340907\n",
      "Epoch 50/100: Loss: 0.054163772892206904\n",
      "Epoch 51/100: Loss: 0.050605686753988265\n",
      "Epoch 52/100: Loss: 0.04253293990623206\n",
      "Epoch 53/100: Loss: 0.0433067986741662\n",
      "Epoch 54/100: Loss: 0.028407789813354612\n",
      "Epoch 55/100: Loss: 0.03750815780367702\n",
      "Epoch 56/100: Loss: 0.022663376003038138\n",
      "Epoch 57/100: Loss: 0.019515256187878548\n",
      "Epoch 58/100: Loss: 0.016143988142721354\n",
      "Epoch 59/100: Loss: 0.01462635892094113\n",
      "Epoch 60/100: Loss: 0.013272534986026586\n",
      "Epoch 61/100: Loss: 0.012289857701398432\n",
      "Epoch 62/100: Loss: 0.009349897265201435\n",
      "Epoch 63/100: Loss: 0.00703617301187478\n",
      "Epoch 64/100: Loss: 0.004959186230553314\n",
      "Epoch 65/100: Loss: 0.005331877546268515\n",
      "Epoch 66/100: Loss: 0.004816923616454005\n",
      "Epoch 67/100: Loss: 0.0031266760575817897\n",
      "Epoch 68/100: Loss: 0.002378195943310857\n",
      "Epoch 69/100: Loss: 0.0021029836585512384\n",
      "Epoch 70/100: Loss: 0.0017220574605744331\n",
      "Epoch 71/100: Loss: 0.0023942887084558604\n",
      "Epoch 72/100: Loss: 0.0013732315579545683\n",
      "Epoch 73/100: Loss: 0.0014756909426068888\n",
      "Epoch 74/100: Loss: 0.0009853880808805115\n",
      "Epoch 75/100: Loss: 0.0010838858303031883\n",
      "Epoch 76/100: Loss: 0.0006772279084543698\n",
      "Epoch 77/100: Loss: 0.0007245154069096315\n",
      "Epoch 78/100: Loss: 0.0007862043781642569\n",
      "Epoch 79/100: Loss: 0.0005715571409382392\n",
      "Epoch 80/100: Loss: 0.0005891034334126743\n",
      "Epoch 81/100: Loss: 0.00045868967335991326\n",
      "Epoch 82/100: Loss: 0.0006798948990763165\n",
      "Epoch 83/100: Loss: 0.0005556058022193611\n",
      "Epoch 84/100: Loss: 0.0005018879393901444\n",
      "Epoch 85/100: Loss: 0.00030663050138173275\n",
      "Epoch 86/100: Loss: 0.0003995139220933197\n",
      "Epoch 87/100: Loss: 0.000422973790045944\n",
      "Epoch 88/100: Loss: 0.0005143699304426264\n",
      "Epoch 89/100: Loss: 0.00036677530006272717\n",
      "Epoch 90/100: Loss: 0.0003814298455836251\n",
      "Epoch 91/100: Loss: 0.00042575890374791926\n",
      "Epoch 92/100: Loss: 0.00041594824142521247\n",
      "Epoch 93/100: Loss: 0.00034056216209137346\n",
      "Epoch 94/100: Loss: 0.0003077286059124162\n",
      "Epoch 95/100: Loss: 0.0003899046222613833\n",
      "Epoch 96/100: Loss: 0.0003301649286186148\n",
      "Epoch 97/100: Loss: 0.00028437382534320934\n",
      "Epoch 98/100: Loss: 0.0003378354942469741\n",
      "Epoch 99/100: Loss: 0.0002821492085331556\n",
      "Epoch 100/100: Loss: 0.00024091003469948192\n",
      "time costs: 607.794517993927\n",
      "--------------------\n",
      "training epoch: 8\n",
      "Epoch 1/100: Loss: 1.035212081670761\n",
      "Epoch 2/100: Loss: 0.9969513058662415\n",
      "Epoch 3/100: Loss: 1.0116817027330398\n",
      "Epoch 4/100: Loss: 1.019590637087822\n",
      "Epoch 5/100: Loss: 0.9625319927930832\n",
      "Epoch 6/100: Loss: 1.012719178199768\n",
      "Epoch 7/100: Loss: 0.9908492773771286\n",
      "Epoch 8/100: Loss: 0.9597009658813477\n",
      "Epoch 9/100: Loss: 0.9494867384433746\n",
      "Epoch 10/100: Loss: 0.9306858658790589\n",
      "Epoch 11/100: Loss: 0.9255491852760315\n",
      "Epoch 12/100: Loss: 0.9417356342077255\n",
      "Epoch 13/100: Loss: 0.8806859672069549\n",
      "Epoch 14/100: Loss: 0.858242642879486\n",
      "Epoch 15/100: Loss: 0.8205454647541046\n",
      "Epoch 16/100: Loss: 0.836132264137268\n",
      "Epoch 17/100: Loss: 0.8400108724832535\n",
      "Epoch 18/100: Loss: 0.8403050690889359\n",
      "Epoch 19/100: Loss: 0.764522123336792\n",
      "Epoch 20/100: Loss: 0.7535821750760079\n",
      "Epoch 21/100: Loss: 0.656937113404274\n",
      "Epoch 22/100: Loss: 0.6706713780760765\n",
      "Epoch 23/100: Loss: 0.6721178516745567\n",
      "Epoch 24/100: Loss: 0.4891121476888657\n",
      "Epoch 25/100: Loss: 0.4430716305971146\n",
      "Epoch 26/100: Loss: 0.4040641255676746\n",
      "Epoch 27/100: Loss: 0.5340543888509274\n",
      "Epoch 28/100: Loss: 0.41917308419942856\n",
      "Epoch 29/100: Loss: 0.4319322843104601\n",
      "Epoch 30/100: Loss: 0.49608412347733977\n",
      "Epoch 31/100: Loss: 0.42386185824871064\n",
      "Epoch 32/100: Loss: 0.3472299832850695\n",
      "Epoch 33/100: Loss: 0.3754602398723364\n",
      "Epoch 34/100: Loss: 0.3632841590791941\n",
      "Epoch 35/100: Loss: 0.37295256480574607\n",
      "Epoch 36/100: Loss: 0.33424194492399695\n",
      "Epoch 37/100: Loss: 0.32158414721488954\n",
      "Epoch 38/100: Loss: 0.2409439530223608\n",
      "Epoch 39/100: Loss: 0.20640688575804234\n",
      "Epoch 40/100: Loss: 0.2395857771858573\n",
      "Epoch 41/100: Loss: 0.19962293989956378\n",
      "Epoch 42/100: Loss: 0.17865899428725243\n",
      "Epoch 43/100: Loss: 0.23766418108716608\n",
      "Epoch 44/100: Loss: 0.14185105804353954\n",
      "Epoch 45/100: Loss: 0.16196010783314704\n",
      "Epoch 46/100: Loss: 0.1417975657619536\n",
      "Epoch 47/100: Loss: 0.1322042412124574\n",
      "Epoch 48/100: Loss: 0.17213081903755664\n",
      "Epoch 49/100: Loss: 0.14157692939043046\n",
      "Epoch 50/100: Loss: 0.14267504485324026\n",
      "Epoch 51/100: Loss: 0.10081225354224443\n",
      "Epoch 52/100: Loss: 0.11136499191634357\n",
      "Epoch 53/100: Loss: 0.10226471535861492\n",
      "Epoch 54/100: Loss: 0.08488391805440187\n",
      "Epoch 55/100: Loss: 0.08162718825042248\n",
      "Epoch 56/100: Loss: 0.05935079562477767\n",
      "Epoch 57/100: Loss: 0.0721685278462246\n",
      "Epoch 58/100: Loss: 0.06060659974464215\n",
      "Epoch 59/100: Loss: 0.03222945901216008\n",
      "Epoch 60/100: Loss: 0.056918973199208266\n",
      "Epoch 61/100: Loss: 0.031426445790566505\n",
      "Epoch 62/100: Loss: 0.021749389899196104\n",
      "Epoch 63/100: Loss: 0.028020099297282285\n",
      "Epoch 64/100: Loss: 0.03570538772037253\n",
      "Epoch 65/100: Loss: 0.020071693742647768\n",
      "Epoch 66/100: Loss: 0.01871537514962256\n",
      "Epoch 67/100: Loss: 0.012050260161049663\n",
      "Epoch 68/100: Loss: 0.004806671076221391\n",
      "Epoch 69/100: Loss: 0.01124744835542515\n",
      "Epoch 70/100: Loss: 0.010116719466168433\n",
      "Epoch 71/100: Loss: 0.0043088058300781995\n",
      "Epoch 72/100: Loss: 0.007984612096333877\n",
      "Epoch 73/100: Loss: 0.004043389484286309\n",
      "Epoch 74/100: Loss: 0.005679614636756014\n",
      "Epoch 75/100: Loss: 0.00466028141163406\n",
      "Epoch 76/100: Loss: 0.0039047842481522823\n",
      "Epoch 77/100: Loss: 0.00189031308000267\n",
      "Epoch 78/100: Loss: 0.0021899072919040917\n",
      "Epoch 79/100: Loss: 0.0022711581561452475\n",
      "Epoch 80/100: Loss: 0.002145941184244293\n",
      "Epoch 81/100: Loss: 0.0012845493220083881\n",
      "Epoch 82/100: Loss: 0.0012801672099158167\n",
      "Epoch 83/100: Loss: 0.0012037224707455608\n",
      "Epoch 84/100: Loss: 0.0012838036222092342\n",
      "Epoch 85/100: Loss: 0.0020450168471143116\n",
      "Epoch 86/100: Loss: 0.0006963036489651131\n",
      "Epoch 87/100: Loss: 0.0017120545304351253\n",
      "Epoch 88/100: Loss: 0.0007877850697695976\n",
      "Epoch 89/100: Loss: 0.0007888112178989104\n",
      "Epoch 90/100: Loss: 0.0007338384883041726\n",
      "Epoch 91/100: Loss: 0.0004716451159765711\n",
      "Epoch 92/100: Loss: 0.0008759003951126943\n",
      "Epoch 93/100: Loss: 0.0008713371142221149\n",
      "Epoch 94/100: Loss: 0.0006224169582765172\n",
      "Epoch 95/100: Loss: 0.0007618061536049936\n",
      "Epoch 96/100: Loss: 0.0007797332560130598\n",
      "Epoch 97/100: Loss: 0.00048559071510680953\n",
      "Epoch 98/100: Loss: 0.000462471237256068\n",
      "Epoch 99/100: Loss: 0.0005379034127145132\n",
      "Epoch 100/100: Loss: 0.0006545130681843148\n",
      "time costs: 608.4911501407623\n",
      "--------------------\n",
      "training epoch: 9\n",
      "Epoch 1/100: Loss: 1.015981513261795\n",
      "Epoch 2/100: Loss: 0.9195567697286606\n",
      "Epoch 3/100: Loss: 0.9972889930009842\n",
      "Epoch 4/100: Loss: 0.9631114542484284\n",
      "Epoch 5/100: Loss: 0.9641536861658097\n",
      "Epoch 6/100: Loss: 1.017931655049324\n",
      "Epoch 7/100: Loss: 0.951544314622879\n",
      "Epoch 8/100: Loss: 0.9057805866003037\n",
      "Epoch 9/100: Loss: 0.8977128893136979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: Loss: 0.8810741186141968\n",
      "Epoch 11/100: Loss: 0.8548682481050491\n",
      "Epoch 12/100: Loss: 0.8124677032232285\n",
      "Epoch 13/100: Loss: 0.7820042908191681\n",
      "Epoch 14/100: Loss: 0.7678450793027878\n",
      "Epoch 15/100: Loss: 0.7029441624879837\n",
      "Epoch 16/100: Loss: 0.6397228881716728\n",
      "Epoch 17/100: Loss: 0.6341834262013435\n",
      "Epoch 18/100: Loss: 0.6030403867363929\n",
      "Epoch 19/100: Loss: 0.5092219904065132\n",
      "Epoch 20/100: Loss: 0.5238802149891854\n",
      "Epoch 21/100: Loss: 0.6242881834506988\n",
      "Epoch 22/100: Loss: 0.4862735874950886\n",
      "Epoch 23/100: Loss: 0.4199796080589294\n",
      "Epoch 24/100: Loss: 0.46212268620729446\n",
      "Epoch 25/100: Loss: 0.3526179723441601\n",
      "Epoch 26/100: Loss: 0.37011212557554246\n",
      "Epoch 27/100: Loss: 0.36924795135855676\n",
      "Epoch 28/100: Loss: 0.2626678581349552\n",
      "Epoch 29/100: Loss: 0.2895601586438715\n",
      "Epoch 30/100: Loss: 0.21197131369262934\n",
      "Epoch 31/100: Loss: 0.3242779298685491\n",
      "Epoch 32/100: Loss: 0.24339097579941155\n",
      "Epoch 33/100: Loss: 0.21559978211298586\n",
      "Epoch 34/100: Loss: 0.1495514671318233\n",
      "Epoch 35/100: Loss: 0.13405596050433816\n",
      "Epoch 36/100: Loss: 0.12239164446946234\n",
      "Epoch 37/100: Loss: 0.10558059005998074\n",
      "Epoch 38/100: Loss: 0.11058084348333068\n",
      "Epoch 39/100: Loss: 0.08957118422840722\n",
      "Epoch 40/100: Loss: 0.05409460016526282\n",
      "Epoch 41/100: Loss: 0.07526575036972645\n",
      "Epoch 42/100: Loss: 0.03284568201415823\n",
      "Epoch 43/100: Loss: 0.03854592960124137\n",
      "Epoch 44/100: Loss: 0.028048008010955527\n",
      "Epoch 45/100: Loss: 0.02281445504304429\n",
      "Epoch 46/100: Loss: 0.019739702602964825\n",
      "Epoch 47/100: Loss: 0.023995046757045203\n",
      "Epoch 48/100: Loss: 0.010275250069389586\n",
      "Epoch 49/100: Loss: 0.008758184059115593\n",
      "Epoch 50/100: Loss: 0.008475229321629741\n",
      "Epoch 51/100: Loss: 0.005349562015180709\n",
      "Epoch 52/100: Loss: 0.0041484603440039795\n",
      "Epoch 53/100: Loss: 0.004891055502048403\n",
      "Epoch 54/100: Loss: 0.002471351264830446\n",
      "Epoch 55/100: Loss: 0.001916564478597138\n",
      "Epoch 56/100: Loss: 0.0014848123664705781\n",
      "Epoch 57/100: Loss: 0.0013195768646255602\n",
      "Epoch 58/100: Loss: 0.0014040789661521557\n",
      "Epoch 59/100: Loss: 0.0009394397526193643\n",
      "Epoch 60/100: Loss: 0.0008542175461116131\n",
      "Epoch 61/100: Loss: 0.0007369716098764911\n",
      "Epoch 62/100: Loss: 0.0005143914051586763\n",
      "Epoch 63/100: Loss: 0.00042472808872844324\n",
      "Epoch 64/100: Loss: 0.00048196279349213\n",
      "Epoch 65/100: Loss: 0.0005249498592093005\n",
      "Epoch 66/100: Loss: 0.0003272159588050272\n",
      "Epoch 67/100: Loss: 0.0003794055090111215\n",
      "Epoch 68/100: Loss: 0.00025668865705483766\n",
      "Epoch 69/100: Loss: 0.0004977007889465313\n",
      "Epoch 70/100: Loss: 0.00046140539070620433\n",
      "Epoch 71/100: Loss: 0.0004815620663066511\n",
      "Epoch 72/100: Loss: 0.0004018044681288302\n",
      "Epoch 73/100: Loss: 0.0003056827682485164\n",
      "Epoch 74/100: Loss: 0.00028624216793105006\n",
      "Epoch 75/100: Loss: 0.0003271822757596965\n",
      "Epoch 76/100: Loss: 0.0003793842401137226\n",
      "Epoch 77/100: Loss: 0.00025207934777427\n",
      "Epoch 78/100: Loss: 0.00032204688909587276\n",
      "Epoch 79/100: Loss: 0.0002543271953982185\n",
      "Epoch 80/100: Loss: 0.0003369087497048895\n",
      "Epoch 81/100: Loss: 0.0002473139766152599\n",
      "Epoch 82/100: Loss: 0.00028951832946404464\n",
      "Epoch 83/100: Loss: 0.0002731903421590687\n",
      "Epoch 84/100: Loss: 0.00031219501288433095\n",
      "Epoch 85/100: Loss: 0.0002642958774231374\n",
      "Epoch 86/100: Loss: 0.0002427487337627099\n",
      "Epoch 87/100: Loss: 0.0003218111900423537\n",
      "Epoch 88/100: Loss: 0.00019852685154546633\n",
      "Epoch 89/100: Loss: 0.00038000449135324745\n",
      "Epoch 90/100: Loss: 0.000301140208466677\n",
      "Epoch 91/100: Loss: 0.0003267903808591655\n",
      "Epoch 92/100: Loss: 0.00041686352305987385\n",
      "Epoch 93/100: Loss: 0.00025599216155569593\n",
      "Epoch 94/100: Loss: 0.00019576108334149467\n",
      "Epoch 95/100: Loss: 0.00035150357780366903\n",
      "Epoch 96/100: Loss: 0.00034873759886977496\n",
      "Epoch 97/100: Loss: 0.0002499313918633561\n",
      "Epoch 98/100: Loss: 0.00041157062905767817\n",
      "Epoch 99/100: Loss: 0.00031909949484543176\n",
      "Epoch 100/100: Loss: 0.00021750249161414103\n",
      "time costs: 596.4184439182281\n",
      "--------------------\n",
      "training epoch: 10\n",
      "Epoch 1/100: Loss: 1.0440350085496903\n",
      "Epoch 2/100: Loss: 1.0063629537820815\n",
      "Epoch 3/100: Loss: 0.974049550294876\n",
      "Epoch 4/100: Loss: 1.0167945474386215\n",
      "Epoch 5/100: Loss: 0.9584888815879822\n",
      "Epoch 6/100: Loss: 0.9698307693004609\n",
      "Epoch 7/100: Loss: 0.9294842153787612\n",
      "Epoch 8/100: Loss: 0.9542797416448593\n",
      "Epoch 9/100: Loss: 0.8860976129770279\n",
      "Epoch 10/100: Loss: 0.9280414074659348\n",
      "Epoch 11/100: Loss: 0.8834065735340119\n",
      "Epoch 12/100: Loss: 0.8719085156917572\n",
      "Epoch 13/100: Loss: 0.8406409382820129\n",
      "Epoch 14/100: Loss: 0.7820230633020401\n",
      "Epoch 15/100: Loss: 0.7746830314397812\n",
      "Epoch 16/100: Loss: 0.7446281790733338\n",
      "Epoch 17/100: Loss: 0.704583503305912\n",
      "Epoch 18/100: Loss: 0.6079942509531975\n",
      "Epoch 19/100: Loss: 0.6425980165600776\n",
      "Epoch 20/100: Loss: 0.594982685148716\n",
      "Epoch 21/100: Loss: 0.5488177359104156\n",
      "Epoch 22/100: Loss: 0.5150000087916851\n",
      "Epoch 23/100: Loss: 0.39490833729505537\n",
      "Epoch 24/100: Loss: 0.3615065760910511\n",
      "Epoch 25/100: Loss: 0.4289822779595852\n",
      "Epoch 26/100: Loss: 0.3727914238348603\n",
      "Epoch 27/100: Loss: 0.33484961520880463\n",
      "Epoch 28/100: Loss: 0.3960229896008968\n",
      "Epoch 29/100: Loss: 0.3061779522337019\n",
      "Epoch 30/100: Loss: 0.2914346564561129\n",
      "Epoch 31/100: Loss: 0.4127743254415691\n",
      "Epoch 32/100: Loss: 0.2486357031390071\n",
      "Epoch 33/100: Loss: 0.28629532107152045\n",
      "Epoch 34/100: Loss: 0.27342046247795226\n",
      "Epoch 35/100: Loss: 0.25735742568504066\n",
      "Epoch 36/100: Loss: 0.20309422966092824\n",
      "Epoch 37/100: Loss: 0.17308246656320989\n",
      "Epoch 38/100: Loss: 0.24836696224519983\n",
      "Epoch 39/100: Loss: 0.27851113891229035\n",
      "Epoch 40/100: Loss: 0.255858651350718\n",
      "Epoch 41/100: Loss: 0.2339531066827476\n",
      "Epoch 42/100: Loss: 0.19476036997511983\n",
      "Epoch 43/100: Loss: 0.22243410923983903\n",
      "Epoch 44/100: Loss: 0.183632157149259\n",
      "Epoch 45/100: Loss: 0.18583261664025486\n",
      "Epoch 46/100: Loss: 0.18573817878495902\n",
      "Epoch 47/100: Loss: 0.1187416541739367\n",
      "Epoch 48/100: Loss: 0.20330856570508332\n",
      "Epoch 49/100: Loss: 0.06089479110669345\n",
      "Epoch 50/100: Loss: 0.14198512467555702\n",
      "Epoch 51/100: Loss: 0.1323388480115682\n",
      "Epoch 52/100: Loss: 0.10605460735387169\n",
      "Epoch 53/100: Loss: 0.06644300516563817\n",
      "Epoch 54/100: Loss: 0.1131954673830478\n",
      "Epoch 55/100: Loss: 0.07313045583141502\n",
      "Epoch 56/100: Loss: 0.07553845704096603\n",
      "Epoch 57/100: Loss: 0.08197535493818578\n",
      "Epoch 58/100: Loss: 0.03274822226467222\n",
      "Epoch 59/100: Loss: 0.04529707086039707\n",
      "Epoch 60/100: Loss: 0.03683577530755429\n",
      "Epoch 61/100: Loss: 0.01994976370006043\n",
      "Epoch 62/100: Loss: 0.022782334617659217\n",
      "Epoch 63/100: Loss: 0.04593746915343218\n",
      "Epoch 64/100: Loss: 0.01545168088923674\n",
      "Epoch 65/100: Loss: 0.022336504282429816\n",
      "Epoch 66/100: Loss: 0.014247769259782217\n",
      "Epoch 67/100: Loss: 0.011653877406388347\n",
      "Epoch 68/100: Loss: 0.010782544526773564\n",
      "Epoch 69/100: Loss: 0.006237337681523058\n",
      "Epoch 70/100: Loss: 0.011379279682296328\n",
      "Epoch 71/100: Loss: 0.009541696040105307\n",
      "Epoch 72/100: Loss: 0.006368554060463794\n",
      "Epoch 73/100: Loss: 0.005166423675200349\n",
      "Epoch 74/100: Loss: 0.004625189305806998\n",
      "Epoch 75/100: Loss: 0.0031302369590775923\n",
      "Epoch 76/100: Loss: 0.0022443329624366017\n",
      "Epoch 77/100: Loss: 0.002330193623856758\n",
      "Epoch 78/100: Loss: 0.0009440702597203199\n",
      "Epoch 79/100: Loss: 0.0010040366378234467\n",
      "Epoch 80/100: Loss: 0.0015504777415117133\n",
      "Epoch 81/100: Loss: 0.0010289437335814\n",
      "Epoch 82/100: Loss: 0.0009951379865015043\n",
      "Epoch 83/100: Loss: 0.0009487787308898987\n",
      "Epoch 84/100: Loss: 0.0006067148788133636\n",
      "Epoch 85/100: Loss: 0.0008147850434397696\n",
      "Epoch 86/100: Loss: 0.0005604272452217173\n",
      "Epoch 87/100: Loss: 0.0005117322227306432\n",
      "Epoch 88/100: Loss: 0.0007084485674567986\n",
      "Epoch 89/100: Loss: 0.0005257818062091246\n",
      "Epoch 90/100: Loss: 0.00042929323826683684\n",
      "Epoch 91/100: Loss: 0.0006395808690285776\n",
      "Epoch 92/100: Loss: 0.0004054541823279578\n",
      "Epoch 93/100: Loss: 0.0006297628415268264\n",
      "Epoch 94/100: Loss: 0.0003759160052140942\n",
      "Epoch 95/100: Loss: 0.00040558183873145025\n",
      "Epoch 96/100: Loss: 0.0007042985438602045\n",
      "Epoch 97/100: Loss: 0.0005168465464521433\n",
      "Epoch 98/100: Loss: 0.0004736671407954418\n",
      "Epoch 99/100: Loss: 0.00035694320049515225\n",
      "Epoch 100/100: Loss: 0.0005272101123409811\n",
      "time costs: 561.8530948162079\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "accuarcies = []\n",
    "times = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'training epoch: {i + 1}')\n",
    "    qmodel = QModel(input_size, hidden_size, num_output, \n",
    "                num_layers = num_layers, ctx = ctx, mode='classical')\n",
    "    optimizer = torch.optim.Adam(qmodel.parameters(), lr = 0.001)\n",
    "    loss_func = nn.MSELoss()\n",
    "    start = time.time()\n",
    "    losses = train_model(qmodel, train_data, batch_size=20,          \n",
    "                   loss_func = loss_func, optimizer = optimizer, epoch = 100)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'time costs: {end - start}')\n",
    "    times.append(end - start)\n",
    "    \n",
    "    acc = calculate_accuarcy(qmodel, X_test, y_test)\n",
    "    accuarcies.append(acc)\n",
    "    \n",
    "    with open(f'loss/layer2/loss{i + 1}.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(losses, pkl_file)\n",
    "    \n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ac9c597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22fbbc75a30>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses, color=\"#FF6666\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3343ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantum paramers: 96\n"
     ]
    }
   ],
   "source": [
    "print(f'quantum paramers: {QLSTM(1, 1, ctx = ctx).qparameters_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4037ab75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94371223"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.94371223\n",
    "# \n",
    "# [tensor(0.6388),\n",
    "#  tensor(0.9699),\n",
    "#  tensor(0.9795),\n",
    "#  tensor(0.9805),\n",
    "#  tensor(0.9833),\n",
    "#  tensor(0.9587),\n",
    "#  tensor(0.9836),\n",
    "#  tensor(0.9782),\n",
    "#  tensor(0.9843),\n",
    "#  tensor(0.9803)]\n",
    "np.mean(accuarcies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5dfec15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "597.4574043512345"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 597.4574043512345\n",
    "np.mean(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60eb24b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
