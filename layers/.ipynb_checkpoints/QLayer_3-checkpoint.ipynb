{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6581af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94243045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff0ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyqpanda import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "455c7f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a618a3a",
   "metadata": {},
   "source": [
    "# 1. Prepare Dadaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c64faba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/sumanthvrao/daily-climate-time-series-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de627766",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './../data/DailyDelhiClimateTrain.csv'\n",
    "test_path = './../data/DailyDelhiClimateTest.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d0827fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [1,2,3,4]\n",
    "\n",
    "train = pd.read_csv(train_path, usecols=cols, engine=\"python\")\n",
    "test = pd.read_csv(test_path, usecols=cols, engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c3039c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train)=1462\n",
      "len(test)=114\n"
     ]
    }
   ],
   "source": [
    "print(f'len(train)={len(train)}')\n",
    "print(f'len(test)={len(test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b941db2e",
   "metadata": {},
   "source": [
    "## 1.1 Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e59fddc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove outliers num: 9\n"
     ]
    }
   ],
   "source": [
    "unnormal_num = 0\n",
    "for i in range(len(train)):\n",
    "    mp = train.iloc[i][3]\n",
    "    if mp > 1200 or mp < 950:\n",
    "        unnormal_num += 1\n",
    "        train.iloc[i][3] = train.iloc[i + 1][3]\n",
    "print(f'remove outliers num: {unnormal_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fefec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.iloc[0][3] = test.iloc[1][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6035297e",
   "metadata": {},
   "source": [
    "## 1.2 Transfer data to LSTM representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a1277fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "884bc13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(data, window_size, predict_size):\n",
    "    scaler = StandardScaler()\n",
    "    data = scaler.fit_transform(np.array(data).reshape(-1, 1))\n",
    "    \n",
    "    data_in = []\n",
    "    data_out = []\n",
    "    \n",
    "    for i in range(data.shape[0] - window_size - predict_size):\n",
    "        data_in.append(data[i:i + window_size].reshape(1, window_size)[0])\n",
    "        data_out.append(data[i + window_size:i + window_size + predict_size].reshape(1, predict_size)[0])\n",
    "        \n",
    "    data_in = np.array(data_in).reshape(-1, window_size)\n",
    "    data_out = np.array(data_out).reshape(-1, predict_size)\n",
    "    \n",
    "    data_process = {'datain': data_in, 'dataout': data_out}\n",
    "    \n",
    "    return data_process, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6517fe60",
   "metadata": {},
   "source": [
    "## 1.3 prepare train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d333c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_size = 4\n",
    "window_size = features_size * 3 # features num * time steps\n",
    "predict_size = features_size # features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ef548b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed, train_scaler = data_process(train, window_size, predict_size)\n",
    "X_train, y_train = train_processed['datain'], train_processed['dataout']\n",
    "\n",
    "test_processed, test_scaler = data_process(test, window_size, predict_size)\n",
    "X_test, y_test = test_processed['datain'], test_processed['dataout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f779325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dda516d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "\n",
    "train_data = Data.TensorDataset(X_train, y_train)\n",
    "test_data = Data.TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb132c",
   "metadata": {},
   "source": [
    "# 2. Quantum Enhanced LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d517f5f3",
   "metadata": {},
   "source": [
    "## 2.1 initiate quantum environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffc85d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitQMachine:\n",
    "    def __init__(self, qubitsCount, cbitsCount = 0, machineType = QMachineType.CPU):\n",
    "        self.machine = init_quantum_machine(machineType)\n",
    "        \n",
    "        self.qubits = self.machine.qAlloc_many(qubitsCount)\n",
    "        self.cbits = self.machine.cAlloc_many(cbitsCount)\n",
    "        \n",
    "        print(f'Init Quantum Machine with qubits:[{qubitsCount}] / cbits:[{cbitsCount}] Successfully')\n",
    "    \n",
    "    def __del__(self):\n",
    "        destroy_quantum_machine(self.machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "741a6a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Quantum Machine with qubits:[4] / cbits:[0] Successfully\n"
     ]
    }
   ],
   "source": [
    "# maximum qubits size\n",
    "ctx = InitQMachine(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a39cf3",
   "metadata": {},
   "source": [
    "## 2.2 Quantum Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b029d",
   "metadata": {},
   "source": [
    "### 2.2.1 Quantum layer base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5570e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f9cfd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayerBase(nn.Module):\n",
    "    def __init__(self, input_size, output_size, *, n_qubits, n_layers = 1, ctx = None):\n",
    "        super(QuantumLayerBase, self).__init__()\n",
    "        \n",
    "        self.data = None # need to input during forward\n",
    "    \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size # hidden size, not n_qubits\n",
    "        \n",
    "        # quantum infos\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.ctx = ctx\n",
    "        self.qubits = ctx.qubits\n",
    "        self.machine = ctx.machine\n",
    "        \n",
    "        # convert quantum input/output to match classical computation\n",
    "        self.qin = nn.Linear(self.input_size, self.n_qubits)\n",
    "        self.qout = nn.Linear(self.n_qubits, self.output_size)\n",
    "        \n",
    "    @property\n",
    "    def circuit(self):\n",
    "        raise NotImplementedError('Should init circuit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94c5bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(self):\n",
    "    HamiZ = [ PauliOperator({f'Z{i}': 1}) for i in range(len(self.qubits)) ]\n",
    "    res = [ eval(qop(self.circuit, Hami, self.machine, self.qubits))[0,0] for Hami in HamiZ ]\n",
    "    \n",
    "    return Parameter(Tensor(res[:self.n_qubits]))\n",
    "\n",
    "QuantumLayerBase.measure = measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4341341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, inputs):\n",
    "    y_t = self.qin(Parameter(inputs))\n",
    "    self.data = y_t[0]\n",
    "    \n",
    "    return self.qout(self.measure())\n",
    "\n",
    "QuantumLayerBase.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4f794b",
   "metadata": {},
   "source": [
    "### 2.2.2 Quantum layer design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b98a8984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayer(QuantumLayerBase):\n",
    "    def __init__(self, input_size, output_size, *, n_qubits, degree = 1, n_layers = 1, ctx = None):\n",
    "        super(QuantumLayer, self).__init__(input_size, output_size, \n",
    "                                         n_qubits = n_qubits, n_layers = n_layers, ctx = ctx)\n",
    "        \n",
    "        self.degree = degree\n",
    "        self.angles = Parameter(torch.rand(n_layers * 4, degree, self.n_qubits))\n",
    "        \n",
    "    @property\n",
    "    def qparameters_size(self):\n",
    "        return self.angles.flatten().size()[0]\n",
    "        \n",
    "    @property\n",
    "    def circuit(self):\n",
    "        if self.data == None:\n",
    "            raise ValueError('Need to feed a input data!')\n",
    "        \n",
    "        n = self.n_qubits\n",
    "        q = self.qubits\n",
    "        x = self.data\n",
    "        p = self.angles\n",
    "        degree = self.degree\n",
    "        \n",
    "        h = VariationalQuantumGate_H\n",
    "        ry = VariationalQuantumGate_RY\n",
    "        cz = VariationalQuantumGate_CZ\n",
    "        crz = VariationalQuantumGate_CRZ\n",
    "        \n",
    "        # init variational quantum circuit\n",
    "        vqc = VariationalQuantumCircuit()\n",
    "\n",
    "        # encoding layer\n",
    "        [ vqc.insert( h(q[i]) ) for i in range(n) ]\n",
    "        [ vqc.insert( ry(q[i], var(x[i] * torch.pi / 2)) ) for i in range(n) ]\n",
    "        \n",
    "        # variational layer\n",
    "        [ vqc.insert( ry(q[i], var(p[0][0][i]) )) for i in range(n) ]\n",
    "        \n",
    "        vqc.insert(crz(q[0], q[3], var(p[1][0][0])))\n",
    "        vqc.insert(crz(q[3], q[2], var(p[1][0][1])))\n",
    "        vqc.insert(crz(q[2], q[1], var(p[1][0][2])))\n",
    "        vqc.insert(crz(q[1], q[0], var(p[1][0][3])))\n",
    "        \n",
    "        [ vqc.insert( ry(q[i], var(p[2][0][i]) )) for i in range(n) ]\n",
    "        \n",
    "        vqc.insert(crz(q[2], q[3], var(p[3][0][0])))\n",
    "        vqc.insert(crz(q[3], q[0], var(p[3][0][1])))\n",
    "        vqc.insert(crz(q[0], q[1], var(p[3][0][2])))\n",
    "        vqc.insert(crz(q[1], q[2], var(p[3][0][3])))\n",
    "        \n",
    "        return vqc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766692de",
   "metadata": {},
   "source": [
    "### 2.2.3 Plot Quantum Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88ff286b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyqpanda.pyQPanda.QProg at 0x1863f093eb0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Tensor([[0.1, 0.2, 0.3, 0.4]])\n",
    "layer = QuantumLayer(4, 4, n_qubits=4, n_layers=1, degree=3, ctx=ctx)\n",
    "layer.data = data[0]\n",
    "vqc = layer.circuit\n",
    "prog = create_empty_qprog()\n",
    "prog.insert(vqc.feed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fd364d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'null'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_qprog(prog, 'pic', filename=f'pic/layer3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50742459",
   "metadata": {},
   "source": [
    "## 2.3 Quantum-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e53ae0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLSTMBase(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, *, ctx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ctx = ctx\n",
    "        \n",
    "    @property\n",
    "    def qparameters_size(self):\n",
    "        num = 0\n",
    "        for attr in dir(self):\n",
    "            if attr.endswith('_circuit'):\n",
    "                num += getattr(self, attr).qparameters_size\n",
    "        return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "582b17af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, inputs, init_states = None):\n",
    "    sequence_size, batch_size, _ = inputs.size()\n",
    "    hidden_sequence = []\n",
    "    \n",
    "    if init_states == None:\n",
    "        h_t, c_t = (\n",
    "            torch.zeros(1, batch_size, self.hidden_size).to(inputs.device),\n",
    "            torch.zeros(1, batch_size, self.hidden_size).to(inputs.device),\n",
    "        )\n",
    "    else:\n",
    "        h_t, c_t = init_states\n",
    "    \n",
    "    return hidden_sequence, (h_t, c_t)\n",
    "\n",
    "QLSTMBase.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d6b0af",
   "metadata": {},
   "source": [
    "## - classical quatum enhanced LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adc8fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLSTM(QLSTMBase):\n",
    "    def __init__(self, input_size, hidden_size, *, ctx):\n",
    "        super().__init__(input_size, hidden_size, ctx = ctx)\n",
    "    \n",
    "        # input gates\n",
    "        self.input_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                        n_qubits = 4, ctx = ctx) # 15\n",
    "        # forget gates\n",
    "        self.forget_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                         n_qubits = 4, ctx = ctx) # 15\n",
    "        # candidate\n",
    "        self.candidate_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                       n_qubits = 4, ctx = ctx) # 15\n",
    "        # output gates\n",
    "        self.output_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                         n_qubits = 4, ctx = ctx) # 15\n",
    "        \n",
    "    def forward(self, inputs, init_states = None):\n",
    "        hidden_sequence, (h_t, c_t) = super(QLSTM, self).forward(inputs, init_states)\n",
    "\n",
    "        for t in range(inputs.size()[0]):\n",
    "            x_t = inputs[t, :, :]\n",
    "            v_t = torch.cat((h_t[0], x_t), dim = 1)\n",
    "\n",
    "            # input gates\n",
    "            i_t = torch.sigmoid(self.input_circuit(v_t))\n",
    "            # forget gates\n",
    "            f_t = torch.sigmoid(self.forget_circuit(v_t))\n",
    "            # candidate for cell state update\n",
    "            g_t = torch.tanh(self.candidate_circuit(v_t))\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "\n",
    "            # output gates\n",
    "            o_t = torch.sigmoid(self.output_circuit(v_t))\n",
    "            # update output ht\n",
    "            h_t = o_t * (torch.tanh(c_t))\n",
    "\n",
    "            hidden_sequence.append(h_t)\n",
    "\n",
    "        # reshape hidden_seq p/ retornar\n",
    "        #\n",
    "        # [tensor([[[0.0444, ...]]] => tensor([[[0.0444, ...]]]\n",
    "        # \n",
    "        hidden_sequence = torch.cat(hidden_sequence, dim = 0)\n",
    "\n",
    "        return hidden_sequence, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28836475",
   "metadata": {},
   "source": [
    "## 2.4 Stacked QLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ec06c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class StackedQLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, *, num_layers = 1, ctx = None, mode = 'classical'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.qlstms = nn.Sequential(OrderedDict([\n",
    "            (f'QLSTM {i + 1}', QLSTM(input_size if i == 0 else hidden_size , hidden_size, ctx = ctx)) \n",
    "                for i in range(num_layers)\n",
    "        ]))\n",
    "\n",
    "    def forward(self, inputs, parameters = None):\n",
    "        outputs = None\n",
    "        \n",
    "        for i, qlstm in enumerate(self.qlstms):\n",
    "            if i != 0:\n",
    "                inputs = outputs\n",
    "            \n",
    "            outputs, parameters = qlstm(inputs, parameters)\n",
    "        \n",
    "        return outputs, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0389535c",
   "metadata": {},
   "source": [
    "# 3. Quantum Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "413150bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_output, *, num_layers = 1, ctx = None, mode = 'classical'):\n",
    "        super(QModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.qlstm = StackedQLSTM(input_size, hidden_size, \n",
    "                                  num_layers = num_layers, ctx = ctx, mode = mode)\n",
    "        self.predict = nn.Linear(hidden_size, num_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        # sequence lenth , batch_size, features length\n",
    "        # \n",
    "        h0 = torch.zeros(1, x.size(1), self.hidden_size)\n",
    "        c0 = torch.zeros(1, x.size(1), self.hidden_size)\n",
    "        \n",
    "        out, _ = self.qlstm(x, (h0, c0))\n",
    "        out = self.predict(out[0])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111aebe4",
   "metadata": {},
   "source": [
    "## 3.1 train QModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc1d1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "def train_model(model, datas, batch_size, *, loss_func, optimizer, epoch = 50):\n",
    "    losses = []\n",
    "    sampler = RandomSampler(datas, num_samples = batch_size)\n",
    "    \n",
    "    for step in range(epoch):\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for index in sampler:\n",
    "            batch_x, batch_y = datas[index][0], datas[index][1]\n",
    "            b_x = batch_x.unsqueeze(0)\n",
    "            b_y = batch_y.unsqueeze(0)\n",
    "            \n",
    "            output = model(b_x)\n",
    "\n",
    "            loss = loss_func(output, b_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {step + 1}/{epoch}: Loss: {train_loss / batch_size}')\n",
    "        losses.append(train_loss / batch_size)\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef07934f",
   "metadata": {},
   "source": [
    "## 3.2 Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57e9b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_E(actual, predict):\n",
    "    E = actual[0] - predict[0]\n",
    "    E = torch.norm(Tensor(E))\n",
    "    E /= torch.norm(Tensor(predict))\n",
    "    \n",
    "    return E\n",
    "\n",
    "def calculate_accuarcy(model, X_test, y_test, scaler=test_scaler):\n",
    "    n = len(X_test)\n",
    "    \n",
    "    err = 0.0\n",
    "    for i in range(0, n):\n",
    "        err += calculate_E(\n",
    "            scaler.inverse_transform(y_test[i:i + 1].data), # actual\n",
    "            scaler.inverse_transform(model(X_test[i:i + 1]).data) # predict\n",
    "        ) ** 2\n",
    "    err /= n\n",
    "    \n",
    "    return 1 - err ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6407f969",
   "metadata": {},
   "source": [
    "## 3.3 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "305c9471",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_size = 4\n",
    "window_size = features_size * 3 # \n",
    "predict_size = features_size # features\n",
    "\n",
    "input_size = window_size\n",
    "num_output = predict_size\n",
    "\n",
    "hidden_size = 32\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50472565",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: 1\n",
      "Epoch 1/100: Loss: 1.040247455239296\n",
      "Epoch 2/100: Loss: 1.0227775007486344\n",
      "Epoch 3/100: Loss: 1.0080004185438156\n",
      "Epoch 4/100: Loss: 1.0196101188659668\n",
      "Epoch 5/100: Loss: 0.985627469420433\n",
      "Epoch 6/100: Loss: 0.9721136748790741\n",
      "Epoch 7/100: Loss: 0.9904998391866684\n",
      "Epoch 8/100: Loss: 0.9651327908039093\n",
      "Epoch 9/100: Loss: 0.9743323892354965\n",
      "Epoch 10/100: Loss: 0.9596571117639542\n",
      "Epoch 11/100: Loss: 0.9415213137865066\n",
      "Epoch 12/100: Loss: 0.9243689924478531\n",
      "Epoch 13/100: Loss: 0.9311662405729294\n",
      "Epoch 14/100: Loss: 0.8776075184345246\n",
      "Epoch 15/100: Loss: 0.8613368153572083\n",
      "Epoch 16/100: Loss: 0.8839202910661698\n",
      "Epoch 17/100: Loss: 0.8407944321632386\n",
      "Epoch 18/100: Loss: 0.8293715089559555\n",
      "Epoch 19/100: Loss: 0.7867189794778824\n",
      "Epoch 20/100: Loss: 0.734797964990139\n",
      "Epoch 21/100: Loss: 0.7433068096637726\n",
      "Epoch 22/100: Loss: 0.7366900593042374\n",
      "Epoch 23/100: Loss: 0.6749001309275627\n",
      "Epoch 24/100: Loss: 0.6123886689543724\n",
      "Epoch 25/100: Loss: 0.5338872142136097\n",
      "Epoch 26/100: Loss: 0.5860545419156551\n",
      "Epoch 27/100: Loss: 0.6304303340613842\n",
      "Epoch 28/100: Loss: 0.5834773853421211\n",
      "Epoch 29/100: Loss: 0.49517811127007005\n",
      "Epoch 30/100: Loss: 0.5427446071058512\n",
      "Epoch 31/100: Loss: 0.6163365941494703\n",
      "Epoch 32/100: Loss: 0.5028463706374169\n",
      "Epoch 33/100: Loss: 0.40412776358425617\n",
      "Epoch 34/100: Loss: 0.30014373306185005\n",
      "Epoch 35/100: Loss: 0.3127641903236508\n",
      "Epoch 36/100: Loss: 0.4789488025009632\n",
      "Epoch 37/100: Loss: 0.3272421669214964\n",
      "Epoch 38/100: Loss: 0.3256393711548299\n",
      "Epoch 39/100: Loss: 0.31322823725640775\n",
      "Epoch 40/100: Loss: 0.350202301191166\n",
      "Epoch 41/100: Loss: 0.3874423023778945\n",
      "Epoch 42/100: Loss: 0.24349112249910831\n",
      "Epoch 43/100: Loss: 0.3884117058943957\n",
      "Epoch 44/100: Loss: 0.32712924787774683\n",
      "Epoch 45/100: Loss: 0.21592303526122122\n",
      "Epoch 46/100: Loss: 0.2965074137784541\n",
      "Epoch 47/100: Loss: 0.37800392056815324\n",
      "Epoch 48/100: Loss: 0.21468431688845158\n",
      "Epoch 49/100: Loss: 0.2732284540310502\n",
      "Epoch 50/100: Loss: 0.28409583419561385\n",
      "Epoch 51/100: Loss: 0.416837950469926\n",
      "Epoch 52/100: Loss: 0.37848073078785094\n",
      "Epoch 53/100: Loss: 0.3640978960087523\n",
      "Epoch 54/100: Loss: 0.3069988161325455\n",
      "Epoch 55/100: Loss: 0.344852160429582\n",
      "Epoch 56/100: Loss: 0.3673144655767828\n",
      "Epoch 57/100: Loss: 0.26215206512715666\n",
      "Epoch 58/100: Loss: 0.34277200791984797\n",
      "Epoch 59/100: Loss: 0.20179275423288345\n",
      "Epoch 60/100: Loss: 0.29154659737832844\n",
      "Epoch 61/100: Loss: 0.23976344764232635\n",
      "Epoch 62/100: Loss: 0.2876412302721292\n",
      "Epoch 63/100: Loss: 0.382775635283906\n",
      "Epoch 64/100: Loss: 0.23065515597118064\n",
      "Epoch 65/100: Loss: 0.2658388766925782\n",
      "Epoch 66/100: Loss: 0.3981059356359765\n",
      "Epoch 67/100: Loss: 0.3309071205847431\n",
      "Epoch 68/100: Loss: 0.20041439521883148\n",
      "Epoch 69/100: Loss: 0.37243084682995686\n",
      "Epoch 70/100: Loss: 0.3850227134069428\n",
      "Epoch 71/100: Loss: 0.2979174046369735\n",
      "Epoch 72/100: Loss: 0.2867862045764923\n",
      "Epoch 73/100: Loss: 0.2876704815775156\n",
      "Epoch 74/100: Loss: 0.21822771308943628\n",
      "Epoch 75/100: Loss: 0.3455513297580183\n",
      "Epoch 76/100: Loss: 0.17328401568811386\n",
      "Epoch 77/100: Loss: 0.3015016689198092\n",
      "Epoch 78/100: Loss: 0.4018977015512064\n",
      "Epoch 79/100: Loss: 0.20868119839869906\n",
      "Epoch 80/100: Loss: 0.3397232762770727\n",
      "Epoch 81/100: Loss: 0.3129024460533401\n",
      "Epoch 82/100: Loss: 0.2936914808989968\n",
      "Epoch 83/100: Loss: 0.3188340392400278\n",
      "Epoch 84/100: Loss: 0.3391301929601468\n",
      "Epoch 85/100: Loss: 0.322043254080927\n",
      "Epoch 86/100: Loss: 0.34203715199255386\n",
      "Epoch 87/100: Loss: 0.30248418301343916\n",
      "Epoch 88/100: Loss: 0.3189807249204023\n",
      "Epoch 89/100: Loss: 0.2734638371388428\n",
      "Epoch 90/100: Loss: 0.26459747112821785\n",
      "Epoch 91/100: Loss: 0.3775884489994496\n",
      "Epoch 92/100: Loss: 0.23763187699951233\n",
      "Epoch 93/100: Loss: 0.4299301516264677\n",
      "Epoch 94/100: Loss: 0.22318640567245893\n",
      "Epoch 95/100: Loss: 0.24819515724666416\n",
      "Epoch 96/100: Loss: 0.2647629489656538\n",
      "Epoch 97/100: Loss: 0.2361763243097812\n",
      "Epoch 98/100: Loss: 0.25245770467445255\n",
      "Epoch 99/100: Loss: 0.36178971637273205\n",
      "Epoch 100/100: Loss: 0.167799466336146\n",
      "time costs: 449.25483798980713\n",
      "--------------------\n",
      "training epoch: 2\n",
      "Epoch 1/100: Loss: 1.0136185824871062\n",
      "Epoch 2/100: Loss: 0.991175627708435\n",
      "Epoch 3/100: Loss: 1.0149158269166947\n",
      "Epoch 4/100: Loss: 1.0121962636709214\n",
      "Epoch 5/100: Loss: 0.9759657770395279\n",
      "Epoch 6/100: Loss: 0.9469164490699769\n",
      "Epoch 7/100: Loss: 0.9922323137521744\n",
      "Epoch 8/100: Loss: 0.9575506150722504\n",
      "Epoch 9/100: Loss: 1.0490846395492555\n",
      "Epoch 10/100: Loss: 0.9829655379056931\n",
      "Epoch 11/100: Loss: 0.9893950909376145\n",
      "Epoch 12/100: Loss: 0.9845575571060181\n",
      "Epoch 13/100: Loss: 0.9749583244323731\n",
      "Epoch 14/100: Loss: 0.9755014687776565\n",
      "Epoch 15/100: Loss: 0.9680165767669677\n",
      "Epoch 16/100: Loss: 0.968392813205719\n",
      "Epoch 17/100: Loss: 0.9602373212575912\n",
      "Epoch 18/100: Loss: 0.9607278168201446\n",
      "Epoch 19/100: Loss: 0.9426677107810975\n",
      "Epoch 20/100: Loss: 0.9185833156108856\n",
      "Epoch 21/100: Loss: 0.931184533238411\n",
      "Epoch 22/100: Loss: 0.9526451677083969\n",
      "Epoch 23/100: Loss: 0.9190602511167526\n",
      "Epoch 24/100: Loss: 0.9209868341684342\n",
      "Epoch 25/100: Loss: 0.9183364272117615\n",
      "Epoch 26/100: Loss: 0.9299162238836288\n",
      "Epoch 27/100: Loss: 0.8654301166534424\n",
      "Epoch 28/100: Loss: 0.8987914770841599\n",
      "Epoch 29/100: Loss: 0.8887502133846283\n",
      "Epoch 30/100: Loss: 0.8788670390844345\n",
      "Epoch 31/100: Loss: 0.8925635904073715\n",
      "Epoch 32/100: Loss: 0.8248411566019058\n",
      "Epoch 33/100: Loss: 0.8323371320962906\n",
      "Epoch 34/100: Loss: 0.721599605679512\n",
      "Epoch 35/100: Loss: 0.8083811134099961\n",
      "Epoch 36/100: Loss: 0.8128546267747879\n",
      "Epoch 37/100: Loss: 0.7627191632986069\n",
      "Epoch 38/100: Loss: 0.7368890106678009\n",
      "Epoch 39/100: Loss: 0.7129529818892479\n",
      "Epoch 40/100: Loss: 0.7538621440529824\n",
      "Epoch 41/100: Loss: 0.7258934333920479\n",
      "Epoch 42/100: Loss: 0.7191896706819534\n",
      "Epoch 43/100: Loss: 0.6089090645313263\n",
      "Epoch 44/100: Loss: 0.6618649661540985\n",
      "Epoch 45/100: Loss: 0.6474446505308151\n",
      "Epoch 46/100: Loss: 0.704446479678154\n",
      "Epoch 47/100: Loss: 0.6625926345586777\n",
      "Epoch 48/100: Loss: 0.6260907560586929\n",
      "Epoch 49/100: Loss: 0.4813491553068161\n",
      "Epoch 50/100: Loss: 0.5026725254952907\n",
      "Epoch 51/100: Loss: 0.5565495081245899\n",
      "Epoch 52/100: Loss: 0.4259876549243927\n",
      "Epoch 53/100: Loss: 0.3352993506938219\n",
      "Epoch 54/100: Loss: 0.40460028275847437\n",
      "Epoch 55/100: Loss: 0.5553022511303425\n",
      "Epoch 56/100: Loss: 0.5179385207593441\n",
      "Epoch 57/100: Loss: 0.40600684136152265\n",
      "Epoch 58/100: Loss: 0.5241904743015766\n",
      "Epoch 59/100: Loss: 0.3863134179264307\n",
      "Epoch 60/100: Loss: 0.5465142078697681\n",
      "Epoch 61/100: Loss: 0.4791810899972916\n",
      "Epoch 62/100: Loss: 0.3809094078838825\n",
      "Epoch 63/100: Loss: 0.42413213588297366\n",
      "Epoch 64/100: Loss: 0.41279249750077723\n",
      "Epoch 65/100: Loss: 0.28056290931999683\n",
      "Epoch 66/100: Loss: 0.3407926132902503\n",
      "Epoch 67/100: Loss: 0.3297343023121357\n",
      "Epoch 68/100: Loss: 0.3351709647104144\n",
      "Epoch 69/100: Loss: 0.39345985315740106\n",
      "Epoch 70/100: Loss: 0.4542101239785552\n",
      "Epoch 71/100: Loss: 0.45424635969102384\n",
      "Epoch 72/100: Loss: 0.32552949134260417\n",
      "Epoch 73/100: Loss: 0.3374414561316371\n",
      "Epoch 74/100: Loss: 0.3347858566790819\n",
      "Epoch 75/100: Loss: 0.3415015091188252\n",
      "Epoch 76/100: Loss: 0.3839682184625417\n",
      "Epoch 77/100: Loss: 0.31323079210706056\n",
      "Epoch 78/100: Loss: 0.22863059248775244\n",
      "Epoch 79/100: Loss: 0.4355193652911112\n",
      "Epoch 80/100: Loss: 0.35939496927894654\n",
      "Epoch 81/100: Loss: 0.19776654932647944\n",
      "Epoch 82/100: Loss: 0.44532311614602804\n",
      "Epoch 83/100: Loss: 0.3603050802368671\n",
      "Epoch 84/100: Loss: 0.3440795126836747\n",
      "Epoch 85/100: Loss: 0.25050561921671033\n",
      "Epoch 86/100: Loss: 0.35039900355041026\n",
      "Epoch 87/100: Loss: 0.44669984295032916\n",
      "Epoch 88/100: Loss: 0.31466756234876814\n",
      "Epoch 89/100: Loss: 0.26160938176326454\n",
      "Epoch 90/100: Loss: 0.3940275745233521\n",
      "Epoch 91/100: Loss: 0.2473555008880794\n",
      "Epoch 92/100: Loss: 0.4678226549876854\n",
      "Epoch 93/100: Loss: 0.35635732393711805\n",
      "Epoch 94/100: Loss: 0.21315390300296713\n",
      "Epoch 95/100: Loss: 0.3764137797697913\n",
      "Epoch 96/100: Loss: 0.3766378897300456\n",
      "Epoch 97/100: Loss: 0.37887170873582365\n",
      "Epoch 98/100: Loss: 0.2963640557485633\n",
      "Epoch 99/100: Loss: 0.31656622907612475\n",
      "Epoch 100/100: Loss: 0.35132323009893296\n",
      "time costs: 605.957751750946\n",
      "--------------------\n",
      "training epoch: 3\n",
      "Epoch 1/100: Loss: 0.9717349231243133\n",
      "Epoch 2/100: Loss: 1.0343953162431716\n",
      "Epoch 3/100: Loss: 0.9870643079280853\n",
      "Epoch 4/100: Loss: 1.0023074269294738\n",
      "Epoch 5/100: Loss: 0.9819098651409149\n",
      "Epoch 6/100: Loss: 0.9791155457496643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: Loss: 0.9282280743122101\n",
      "Epoch 8/100: Loss: 0.9810461401939392\n",
      "Epoch 9/100: Loss: 0.9683622628450393\n",
      "Epoch 10/100: Loss: 0.9816998928785324\n",
      "Epoch 11/100: Loss: 0.9213032275438309\n",
      "Epoch 12/100: Loss: 0.9247815042734147\n",
      "Epoch 13/100: Loss: 0.9460227191448212\n",
      "Epoch 14/100: Loss: 0.8970009624958039\n",
      "Epoch 15/100: Loss: 0.876427736878395\n",
      "Epoch 16/100: Loss: 0.8852925002574921\n",
      "Epoch 17/100: Loss: 0.8737897157669068\n",
      "Epoch 18/100: Loss: 0.8029019266366959\n",
      "Epoch 19/100: Loss: 0.8181481748819351\n",
      "Epoch 20/100: Loss: 0.789116981625557\n",
      "Epoch 21/100: Loss: 0.7514916777610778\n",
      "Epoch 22/100: Loss: 0.7532470181584359\n",
      "Epoch 23/100: Loss: 0.6497808776795864\n",
      "Epoch 24/100: Loss: 0.686496126651764\n",
      "Epoch 25/100: Loss: 0.6988841306418181\n",
      "Epoch 26/100: Loss: 0.6922452740371228\n",
      "Epoch 27/100: Loss: 0.7967245977371931\n",
      "Epoch 28/100: Loss: 0.5889718124642969\n",
      "Epoch 29/100: Loss: 0.5711219362914562\n",
      "Epoch 30/100: Loss: 0.6555240562185645\n",
      "Epoch 31/100: Loss: 0.7187207688577473\n",
      "Epoch 32/100: Loss: 0.5830570682883263\n",
      "Epoch 33/100: Loss: 0.5544054699596017\n",
      "Epoch 34/100: Loss: 0.7327283308841288\n",
      "Epoch 35/100: Loss: 0.5419330454897135\n",
      "Epoch 36/100: Loss: 0.611358370212838\n",
      "Epoch 37/100: Loss: 0.6119414364919067\n",
      "Epoch 38/100: Loss: 0.4471201412845403\n",
      "Epoch 39/100: Loss: 0.5806471239542589\n",
      "Epoch 40/100: Loss: 0.5704023031052202\n",
      "Epoch 41/100: Loss: 0.6281387240160257\n",
      "Epoch 42/100: Loss: 0.5848413086961954\n",
      "Epoch 43/100: Loss: 0.49377403003163634\n",
      "Epoch 44/100: Loss: 0.6048844885546714\n",
      "Epoch 45/100: Loss: 0.6368657989427448\n",
      "Epoch 46/100: Loss: 0.567232329165563\n",
      "Epoch 47/100: Loss: 0.6489003938157112\n",
      "Epoch 48/100: Loss: 0.5650295799132437\n",
      "Epoch 49/100: Loss: 0.5971941076684744\n",
      "Epoch 50/100: Loss: 0.6032981820404529\n",
      "Epoch 51/100: Loss: 0.45840570935979486\n",
      "Epoch 52/100: Loss: 0.408359820349142\n",
      "Epoch 53/100: Loss: 0.3525104867760092\n",
      "Epoch 54/100: Loss: 0.42659319888334724\n",
      "Epoch 55/100: Loss: 0.4835892191214953\n",
      "Epoch 56/100: Loss: 0.4613635275454726\n",
      "Epoch 57/100: Loss: 0.4311972085153684\n",
      "Epoch 58/100: Loss: 0.39166641007177533\n",
      "Epoch 59/100: Loss: 0.4952733395854011\n",
      "Epoch 60/100: Loss: 0.31269134473986926\n",
      "Epoch 61/100: Loss: 0.4858390994835645\n",
      "Epoch 62/100: Loss: 0.42148625930713024\n",
      "Epoch 63/100: Loss: 0.4817608205601573\n",
      "Epoch 64/100: Loss: 0.4037936276290566\n",
      "Epoch 65/100: Loss: 0.3939658108749427\n",
      "Epoch 66/100: Loss: 0.4172377875074744\n",
      "Epoch 67/100: Loss: 0.3594532269053161\n",
      "Epoch 68/100: Loss: 0.31575594731839374\n",
      "Epoch 69/100: Loss: 0.31588925653195477\n",
      "Epoch 70/100: Loss: 0.29321979501983153\n",
      "Epoch 71/100: Loss: 0.2904660998727195\n",
      "Epoch 72/100: Loss: 0.2187798369966913\n",
      "Epoch 73/100: Loss: 0.2906348187752883\n",
      "Epoch 74/100: Loss: 0.34138197982683777\n",
      "Epoch 75/100: Loss: 0.3078529626131058\n",
      "Epoch 76/100: Loss: 0.3210461531300098\n",
      "Epoch 77/100: Loss: 0.24748749532736838\n",
      "Epoch 78/100: Loss: 0.23028152696788312\n",
      "Epoch 79/100: Loss: 0.1811530404142104\n",
      "Epoch 80/100: Loss: 0.33391128879738974\n",
      "Epoch 81/100: Loss: 0.25201776477042587\n",
      "Epoch 82/100: Loss: 0.2162512443988817\n",
      "Epoch 83/100: Loss: 0.2600365535930905\n",
      "Epoch 84/100: Loss: 0.23840350275859237\n",
      "Epoch 85/100: Loss: 0.29388572843745353\n",
      "Epoch 86/100: Loss: 0.19918558358331212\n",
      "Epoch 87/100: Loss: 0.1555446380749345\n",
      "Epoch 88/100: Loss: 0.17367701979819686\n",
      "Epoch 89/100: Loss: 0.16489140265621244\n",
      "Epoch 90/100: Loss: 0.05812595595343737\n",
      "Epoch 91/100: Loss: 0.17827790607552743\n",
      "Epoch 92/100: Loss: 0.16845031714765354\n",
      "Epoch 93/100: Loss: 0.1741269221995026\n",
      "Epoch 94/100: Loss: 0.12716531410696916\n",
      "Epoch 95/100: Loss: 0.1409221144160256\n",
      "Epoch 96/100: Loss: 0.1326266788179055\n",
      "Epoch 97/100: Loss: 0.07583790250355378\n",
      "Epoch 98/100: Loss: 0.0868638672225643\n",
      "Epoch 99/100: Loss: 0.10510752942063846\n",
      "Epoch 100/100: Loss: 0.08042381111554278\n",
      "time costs: 575.490651845932\n",
      "--------------------\n",
      "training epoch: 4\n",
      "Epoch 1/100: Loss: 1.0249266386032105\n",
      "Epoch 2/100: Loss: 0.9941337138414383\n",
      "Epoch 3/100: Loss: 0.9834418296813965\n",
      "Epoch 4/100: Loss: 1.016148567199707\n",
      "Epoch 5/100: Loss: 0.970552122592926\n",
      "Epoch 6/100: Loss: 0.9611645966768265\n",
      "Epoch 7/100: Loss: 0.9595615774393081\n",
      "Epoch 8/100: Loss: 0.9900047361850739\n",
      "Epoch 9/100: Loss: 0.9547836124897003\n",
      "Epoch 10/100: Loss: 0.9122485369443893\n",
      "Epoch 11/100: Loss: 0.9180223494768143\n",
      "Epoch 12/100: Loss: 0.9015474885702133\n",
      "Epoch 13/100: Loss: 0.861788684129715\n",
      "Epoch 14/100: Loss: 0.8096788644790649\n",
      "Epoch 15/100: Loss: 0.8485603660345078\n",
      "Epoch 16/100: Loss: 0.7882110834121704\n",
      "Epoch 17/100: Loss: 0.7997467011213303\n",
      "Epoch 18/100: Loss: 0.7812686711549759\n",
      "Epoch 19/100: Loss: 0.7114892333745957\n",
      "Epoch 20/100: Loss: 0.682373908162117\n",
      "Epoch 21/100: Loss: 0.6567835360765457\n",
      "Epoch 22/100: Loss: 0.6835174798965454\n",
      "Epoch 23/100: Loss: 0.688412356376648\n",
      "Epoch 24/100: Loss: 0.6776210576295852\n",
      "Epoch 25/100: Loss: 0.6383562326431275\n",
      "Epoch 26/100: Loss: 0.6110358387231827\n",
      "Epoch 27/100: Loss: 0.657530988752842\n",
      "Epoch 28/100: Loss: 0.636038126051426\n",
      "Epoch 29/100: Loss: 0.579434135556221\n",
      "Epoch 30/100: Loss: 0.5449465408921241\n",
      "Epoch 31/100: Loss: 0.5580300018191338\n",
      "Epoch 32/100: Loss: 0.5373069554567337\n",
      "Epoch 33/100: Loss: 0.533975088596344\n",
      "Epoch 34/100: Loss: 0.5114122033119202\n",
      "Epoch 35/100: Loss: 0.608813364803791\n",
      "Epoch 36/100: Loss: 0.5067807078361511\n",
      "Epoch 37/100: Loss: 0.5760326318442821\n",
      "Epoch 38/100: Loss: 0.5320467635989189\n",
      "Epoch 39/100: Loss: 0.47827626690268515\n",
      "Epoch 40/100: Loss: 0.42954355999827387\n",
      "Epoch 41/100: Loss: 0.44562977105379104\n",
      "Epoch 42/100: Loss: 0.5504251033067703\n",
      "Epoch 43/100: Loss: 0.4938392736017704\n",
      "Epoch 44/100: Loss: 0.4426498956978321\n",
      "Epoch 45/100: Loss: 0.43520464599132536\n",
      "Epoch 46/100: Loss: 0.4452242501080036\n",
      "Epoch 47/100: Loss: 0.38904649168252947\n",
      "Epoch 48/100: Loss: 0.3221065364778042\n",
      "Epoch 49/100: Loss: 0.39299288392066956\n",
      "Epoch 50/100: Loss: 0.31579030156135557\n",
      "Epoch 51/100: Loss: 0.2525787830352783\n",
      "Epoch 52/100: Loss: 0.38113722391426563\n",
      "Epoch 53/100: Loss: 0.46215579509735105\n",
      "Epoch 54/100: Loss: 0.385166784748435\n",
      "Epoch 55/100: Loss: 0.4211725279688835\n",
      "Epoch 56/100: Loss: 0.2564202774316072\n",
      "Epoch 57/100: Loss: 0.38706283625215293\n",
      "Epoch 58/100: Loss: 0.29316146271303295\n",
      "Epoch 59/100: Loss: 0.26230637738481166\n",
      "Epoch 60/100: Loss: 0.2520843024365604\n",
      "Epoch 61/100: Loss: 0.2880713145248592\n",
      "Epoch 62/100: Loss: 0.19333012821152806\n",
      "Epoch 63/100: Loss: 0.18791522933752275\n",
      "Epoch 64/100: Loss: 0.23100710652652195\n",
      "Epoch 65/100: Loss: 0.21382701754919253\n",
      "Epoch 66/100: Loss: 0.15837669691536577\n",
      "Epoch 67/100: Loss: 0.34934826453682033\n",
      "Epoch 68/100: Loss: 0.3005424846895039\n",
      "Epoch 69/100: Loss: 0.22463929620571435\n",
      "Epoch 70/100: Loss: 0.17390139689669012\n",
      "Epoch 71/100: Loss: 0.16965826389787253\n",
      "Epoch 72/100: Loss: 0.23438175736228004\n",
      "Epoch 73/100: Loss: 0.12214592699310742\n",
      "Epoch 74/100: Loss: 0.17113589909276924\n",
      "Epoch 75/100: Loss: 0.14763354629394598\n",
      "Epoch 76/100: Loss: 0.04261555750272237\n",
      "Epoch 77/100: Loss: 0.1208833171054721\n",
      "Epoch 78/100: Loss: 0.08694924432784319\n",
      "Epoch 79/100: Loss: 0.23776044930855278\n",
      "Epoch 80/100: Loss: 0.1109591981396079\n",
      "Epoch 81/100: Loss: 0.11265779984641995\n",
      "Epoch 82/100: Loss: 0.12767254654318094\n",
      "Epoch 83/100: Loss: 0.14436939979786984\n",
      "Epoch 84/100: Loss: 0.10048836141140782\n",
      "Epoch 85/100: Loss: 0.15501833891430578\n",
      "Epoch 86/100: Loss: 0.1414160882261058\n",
      "Epoch 87/100: Loss: 0.12473235055804252\n",
      "Epoch 88/100: Loss: 0.07169948518276215\n",
      "Epoch 89/100: Loss: 0.09042454332520719\n",
      "Epoch 90/100: Loss: 0.10809582940419205\n",
      "Epoch 91/100: Loss: 0.06319020064620418\n",
      "Epoch 92/100: Loss: 0.07001706566952634\n",
      "Epoch 93/100: Loss: 0.07471701810136437\n",
      "Epoch 94/100: Loss: 0.03278178533364553\n",
      "Epoch 95/100: Loss: 0.06714589967596112\n",
      "Epoch 96/100: Loss: 0.03700724672526121\n",
      "Epoch 97/100: Loss: 0.052457290914753686\n",
      "Epoch 98/100: Loss: 0.038783866172889245\n",
      "Epoch 99/100: Loss: 0.03401670091843698\n",
      "Epoch 100/100: Loss: 0.022119848395232112\n",
      "time costs: 615.0207493305206\n",
      "--------------------\n",
      "training epoch: 5\n",
      "Epoch 1/100: Loss: 1.0065380096435548\n",
      "Epoch 2/100: Loss: 0.9777992844581604\n",
      "Epoch 3/100: Loss: 0.9874770492315292\n",
      "Epoch 4/100: Loss: 0.9725507467985153\n",
      "Epoch 5/100: Loss: 0.9591837435960769\n",
      "Epoch 6/100: Loss: 0.937448924779892\n",
      "Epoch 7/100: Loss: 0.9423788398504257\n",
      "Epoch 8/100: Loss: 0.9429338693618774\n",
      "Epoch 9/100: Loss: 0.9392915338277816\n",
      "Epoch 10/100: Loss: 0.9509771972894668\n",
      "Epoch 11/100: Loss: 0.911079752445221\n",
      "Epoch 12/100: Loss: 0.8901499181985855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: Loss: 0.8735001027584076\n",
      "Epoch 14/100: Loss: 0.842764139175415\n",
      "Epoch 15/100: Loss: 0.864831131696701\n",
      "Epoch 16/100: Loss: 0.8075498014688491\n",
      "Epoch 17/100: Loss: 0.7583928495645523\n",
      "Epoch 18/100: Loss: 0.7840737015008926\n",
      "Epoch 19/100: Loss: 0.7922179937362671\n",
      "Epoch 20/100: Loss: 0.6851882547140121\n",
      "Epoch 21/100: Loss: 0.6711106672883034\n",
      "Epoch 22/100: Loss: 0.6650238692760467\n",
      "Epoch 23/100: Loss: 0.6873019114136696\n",
      "Epoch 24/100: Loss: 0.622537462413311\n",
      "Epoch 25/100: Loss: 0.5951233580708504\n",
      "Epoch 26/100: Loss: 0.5257158175110817\n",
      "Epoch 27/100: Loss: 0.5270237907767296\n",
      "Epoch 28/100: Loss: 0.5401587978005409\n",
      "Epoch 29/100: Loss: 0.5111534595489502\n",
      "Epoch 30/100: Loss: 0.4521332874894142\n",
      "Epoch 31/100: Loss: 0.5281688939779997\n",
      "Epoch 32/100: Loss: 0.45310015939176085\n",
      "Epoch 33/100: Loss: 0.4062839850783348\n",
      "Epoch 34/100: Loss: 0.3983511723577976\n",
      "Epoch 35/100: Loss: 0.34281888268887994\n",
      "Epoch 36/100: Loss: 0.3373769650235772\n",
      "Epoch 37/100: Loss: 0.3195670840330422\n",
      "Epoch 38/100: Loss: 0.30324388248845935\n",
      "Epoch 39/100: Loss: 0.21097751511260868\n",
      "Epoch 40/100: Loss: 0.2392728437669575\n",
      "Epoch 41/100: Loss: 0.3410647166892886\n",
      "Epoch 42/100: Loss: 0.18901705676689745\n",
      "Epoch 43/100: Loss: 0.16624891271349043\n",
      "Epoch 44/100: Loss: 0.23607024538796395\n",
      "Epoch 45/100: Loss: 0.18138023821520619\n",
      "Epoch 46/100: Loss: 0.25544861459638923\n",
      "Epoch 47/100: Loss: 0.15894521127920597\n",
      "Epoch 48/100: Loss: 0.21058522565290333\n",
      "Epoch 49/100: Loss: 0.1383471223176457\n",
      "Epoch 50/100: Loss: 0.22664900519012007\n",
      "Epoch 51/100: Loss: 0.23144914638251066\n",
      "Epoch 52/100: Loss: 0.17030741514172404\n",
      "Epoch 53/100: Loss: 0.13994038050295785\n",
      "Epoch 54/100: Loss: 0.1978311236714944\n",
      "Epoch 55/100: Loss: 0.1423956535058096\n",
      "Epoch 56/100: Loss: 0.11724355237092823\n",
      "Epoch 57/100: Loss: 0.16408614195534027\n",
      "Epoch 58/100: Loss: 0.16686705738538876\n",
      "Epoch 59/100: Loss: 0.14799594939686359\n",
      "Epoch 60/100: Loss: 0.14202681370079517\n",
      "Epoch 61/100: Loss: 0.10476539829978719\n",
      "Epoch 62/100: Loss: 0.08496044797357172\n",
      "Epoch 63/100: Loss: 0.1095815131557174\n",
      "Epoch 64/100: Loss: 0.08321854447713121\n",
      "Epoch 65/100: Loss: 0.05725856473436579\n",
      "Epoch 66/100: Loss: 0.09032780933775938\n",
      "Epoch 67/100: Loss: 0.07340467800968327\n",
      "Epoch 68/100: Loss: 0.07758916512830183\n",
      "Epoch 69/100: Loss: 0.06891460870392621\n",
      "Epoch 70/100: Loss: 0.10832476118812337\n",
      "Epoch 71/100: Loss: 0.08377467840909958\n",
      "Epoch 72/100: Loss: 0.061266012582927944\n",
      "Epoch 73/100: Loss: 0.05808741110377014\n",
      "Epoch 74/100: Loss: 0.034037189953960476\n",
      "Epoch 75/100: Loss: 0.05715534136397764\n",
      "Epoch 76/100: Loss: 0.061416602291865276\n",
      "Epoch 77/100: Loss: 0.0375306116649881\n",
      "Epoch 78/100: Loss: 0.05855951139237732\n",
      "Epoch 79/100: Loss: 0.038414524041581896\n",
      "Epoch 80/100: Loss: 0.025518481677863747\n",
      "Epoch 81/100: Loss: 0.03638198520056903\n",
      "Epoch 82/100: Loss: 0.04460824927082285\n",
      "Epoch 83/100: Loss: 0.02733777042012662\n",
      "Epoch 84/100: Loss: 0.022820946073625236\n",
      "Epoch 85/100: Loss: 0.032414765167050066\n",
      "Epoch 86/100: Loss: 0.019822414743248373\n",
      "Epoch 87/100: Loss: 0.02800055835978128\n",
      "Epoch 88/100: Loss: 0.024345051590353252\n",
      "Epoch 89/100: Loss: 0.02224812670610845\n",
      "Epoch 90/100: Loss: 0.013348166004288942\n",
      "Epoch 91/100: Loss: 0.012730556222959422\n",
      "Epoch 92/100: Loss: 0.014196963728318223\n",
      "Epoch 93/100: Loss: 0.012696341537230182\n",
      "Epoch 94/100: Loss: 0.011901075503556059\n",
      "Epoch 95/100: Loss: 0.013443148991791531\n",
      "Epoch 96/100: Loss: 0.011137140152277425\n",
      "Epoch 97/100: Loss: 0.0072364635416306555\n",
      "Epoch 98/100: Loss: 0.008357746254478115\n",
      "Epoch 99/100: Loss: 0.00637478849675972\n",
      "Epoch 100/100: Loss: 0.0054214830728597\n",
      "time costs: 614.4453225135803\n",
      "--------------------\n",
      "training epoch: 6\n",
      "Epoch 1/100: Loss: 0.9972084552049637\n",
      "Epoch 2/100: Loss: 1.0031308323144912\n",
      "Epoch 3/100: Loss: 0.989971587061882\n",
      "Epoch 4/100: Loss: 0.9595791310071945\n",
      "Epoch 5/100: Loss: 1.0032082349061966\n",
      "Epoch 6/100: Loss: 1.0021266609430313\n",
      "Epoch 7/100: Loss: 0.989458242058754\n",
      "Epoch 8/100: Loss: 0.9905860871076584\n",
      "Epoch 9/100: Loss: 0.9629077792167664\n",
      "Epoch 10/100: Loss: 0.9569686025381088\n",
      "Epoch 11/100: Loss: 0.9661750584840775\n",
      "Epoch 12/100: Loss: 0.9445186346769333\n",
      "Epoch 13/100: Loss: 0.9356183767318725\n",
      "Epoch 14/100: Loss: 0.9423644125461579\n",
      "Epoch 15/100: Loss: 0.9111284613609314\n",
      "Epoch 16/100: Loss: 0.914643082022667\n",
      "Epoch 17/100: Loss: 0.9209706544876098\n",
      "Epoch 18/100: Loss: 0.8680568903684616\n",
      "Epoch 19/100: Loss: 0.8848745465278626\n",
      "Epoch 20/100: Loss: 0.864365142583847\n",
      "Epoch 21/100: Loss: 0.8429015100002288\n",
      "Epoch 22/100: Loss: 0.8116252928972244\n",
      "Epoch 23/100: Loss: 0.8045313835144043\n",
      "Epoch 24/100: Loss: 0.8052863031625748\n",
      "Epoch 25/100: Loss: 0.7468714833259582\n",
      "Epoch 26/100: Loss: 0.7567730873823166\n",
      "Epoch 27/100: Loss: 0.7111808896064759\n",
      "Epoch 28/100: Loss: 0.6597005650401115\n",
      "Epoch 29/100: Loss: 0.7060256391763687\n",
      "Epoch 30/100: Loss: 0.6279322534799576\n",
      "Epoch 31/100: Loss: 0.605133818089962\n",
      "Epoch 32/100: Loss: 0.5348416179418564\n",
      "Epoch 33/100: Loss: 0.5453845724463463\n",
      "Epoch 34/100: Loss: 0.6074391521513463\n",
      "Epoch 35/100: Loss: 0.5404310338199139\n",
      "Epoch 36/100: Loss: 0.44325975626707076\n",
      "Epoch 37/100: Loss: 0.45363929495215416\n",
      "Epoch 38/100: Loss: 0.4402559123933315\n",
      "Epoch 39/100: Loss: 0.48243421092629435\n",
      "Epoch 40/100: Loss: 0.4496659629046917\n",
      "Epoch 41/100: Loss: 0.3711285412311554\n",
      "Epoch 42/100: Loss: 0.3712942648679018\n",
      "Epoch 43/100: Loss: 0.41382607370615004\n",
      "Epoch 44/100: Loss: 0.36323150582611563\n",
      "Epoch 45/100: Loss: 0.36005097068846226\n",
      "Epoch 46/100: Loss: 0.339818998798728\n",
      "Epoch 47/100: Loss: 0.33323758132755754\n",
      "Epoch 48/100: Loss: 0.37155959717929365\n",
      "Epoch 49/100: Loss: 0.29284707792103293\n",
      "Epoch 50/100: Loss: 0.3075760953128338\n",
      "Epoch 51/100: Loss: 0.3445538505911827\n",
      "Epoch 52/100: Loss: 0.29713820815086367\n",
      "Epoch 53/100: Loss: 0.2645020060241222\n",
      "Epoch 54/100: Loss: 0.24016723074018956\n",
      "Epoch 55/100: Loss: 0.2527718681842089\n",
      "Epoch 56/100: Loss: 0.2806218519806862\n",
      "Epoch 57/100: Loss: 0.23313994240015745\n",
      "Epoch 58/100: Loss: 0.24431385695934296\n",
      "Epoch 59/100: Loss: 0.23959153424948454\n",
      "Epoch 60/100: Loss: 0.2293373763561249\n",
      "Epoch 61/100: Loss: 0.2217148534953594\n",
      "Epoch 62/100: Loss: 0.23052026331424713\n",
      "Epoch 63/100: Loss: 0.2246939618140459\n",
      "Epoch 64/100: Loss: 0.2075752917677164\n",
      "Epoch 65/100: Loss: 0.24000374861061574\n",
      "Epoch 66/100: Loss: 0.1681834625080228\n",
      "Epoch 67/100: Loss: 0.1683260379359126\n",
      "Epoch 68/100: Loss: 0.17627289984375238\n",
      "Epoch 69/100: Loss: 0.20299612507224082\n",
      "Epoch 70/100: Loss: 0.11060762703418732\n",
      "Epoch 71/100: Loss: 0.13832952454686165\n",
      "Epoch 72/100: Loss: 0.1437490565702319\n",
      "Epoch 73/100: Loss: 0.13005889263004064\n",
      "Epoch 74/100: Loss: 0.11526079662144184\n",
      "Epoch 75/100: Loss: 0.1499729797244072\n",
      "Epoch 76/100: Loss: 0.1173921400681138\n",
      "Epoch 77/100: Loss: 0.11662960257381201\n",
      "Epoch 78/100: Loss: 0.10827642902731896\n",
      "Epoch 79/100: Loss: 0.10945204347372055\n",
      "Epoch 80/100: Loss: 0.11123830424621702\n",
      "Epoch 81/100: Loss: 0.1142551951110363\n",
      "Epoch 82/100: Loss: 0.0799336364492774\n",
      "Epoch 83/100: Loss: 0.08996933542657644\n",
      "Epoch 84/100: Loss: 0.07867170595563948\n",
      "Epoch 85/100: Loss: 0.07921802513301372\n",
      "Epoch 86/100: Loss: 0.07397335348650813\n",
      "Epoch 87/100: Loss: 0.06410346878692508\n",
      "Epoch 88/100: Loss: 0.058962625847198066\n",
      "Epoch 89/100: Loss: 0.056010598130524156\n",
      "Epoch 90/100: Loss: 0.04686199091374874\n",
      "Epoch 91/100: Loss: 0.04561742888763547\n",
      "Epoch 92/100: Loss: 0.04076980734243989\n",
      "Epoch 93/100: Loss: 0.044740361906588075\n",
      "Epoch 94/100: Loss: 0.03650596998631954\n",
      "Epoch 95/100: Loss: 0.033086296264082195\n",
      "Epoch 96/100: Loss: 0.025991155160591008\n",
      "Epoch 97/100: Loss: 0.025067741004750132\n",
      "Epoch 98/100: Loss: 0.02632430465891957\n",
      "Epoch 99/100: Loss: 0.019709067814983428\n",
      "Epoch 100/100: Loss: 0.02167060235515237\n",
      "time costs: 619.1111214160919\n",
      "--------------------\n",
      "training epoch: 7\n",
      "Epoch 1/100: Loss: 1.0052335679531097\n",
      "Epoch 2/100: Loss: 0.9949824422597885\n",
      "Epoch 3/100: Loss: 0.9701866120100021\n",
      "Epoch 4/100: Loss: 0.9866058230400085\n",
      "Epoch 5/100: Loss: 0.9813680440187454\n",
      "Epoch 6/100: Loss: 0.9601589530706406\n",
      "Epoch 7/100: Loss: 0.9569913625717164\n",
      "Epoch 8/100: Loss: 0.9499954402446746\n",
      "Epoch 9/100: Loss: 0.9219889312982559\n",
      "Epoch 10/100: Loss: 0.9326843738555908\n",
      "Epoch 11/100: Loss: 0.8999202460050583\n",
      "Epoch 12/100: Loss: 0.872999319434166\n",
      "Epoch 13/100: Loss: 0.9104766428470612\n",
      "Epoch 14/100: Loss: 0.8640415698289872\n",
      "Epoch 15/100: Loss: 0.8355985283851624\n",
      "Epoch 16/100: Loss: 0.8283038198947906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: Loss: 0.7947362005710602\n",
      "Epoch 18/100: Loss: 0.7101563185453414\n",
      "Epoch 19/100: Loss: 0.6372885897755622\n",
      "Epoch 20/100: Loss: 0.7432883933186532\n",
      "Epoch 21/100: Loss: 0.708518011868\n",
      "Epoch 22/100: Loss: 0.6240133963525295\n",
      "Epoch 23/100: Loss: 0.6517679184675217\n",
      "Epoch 24/100: Loss: 0.6572148017585278\n",
      "Epoch 25/100: Loss: 0.5236246217042207\n",
      "Epoch 26/100: Loss: 0.6561927560716867\n",
      "Epoch 27/100: Loss: 0.6017126884311438\n",
      "Epoch 28/100: Loss: 0.5880280576646328\n",
      "Epoch 29/100: Loss: 0.5048808366060257\n",
      "Epoch 30/100: Loss: 0.4606605745851994\n",
      "Epoch 31/100: Loss: 0.35266581950709225\n",
      "Epoch 32/100: Loss: 0.5435842678882181\n",
      "Epoch 33/100: Loss: 0.46426132079213855\n",
      "Epoch 34/100: Loss: 0.42795384451746943\n",
      "Epoch 35/100: Loss: 0.46307976003736256\n",
      "Epoch 36/100: Loss: 0.41264392491430046\n",
      "Epoch 37/100: Loss: 0.3310009678825736\n",
      "Epoch 38/100: Loss: 0.45053189247846603\n",
      "Epoch 39/100: Loss: 0.4457584545016289\n",
      "Epoch 40/100: Loss: 0.3859289661049843\n",
      "Epoch 41/100: Loss: 0.3589462887495756\n",
      "Epoch 42/100: Loss: 0.34016878716647625\n",
      "Epoch 43/100: Loss: 0.3349817916750908\n",
      "Epoch 44/100: Loss: 0.306221729144454\n",
      "Epoch 45/100: Loss: 0.25020704977214336\n",
      "Epoch 46/100: Loss: 0.15270044999197124\n",
      "Epoch 47/100: Loss: 0.34905653540045023\n",
      "Epoch 48/100: Loss: 0.1780386627651751\n",
      "Epoch 49/100: Loss: 0.2316392043605447\n",
      "Epoch 50/100: Loss: 0.3119109258055687\n",
      "Epoch 51/100: Loss: 0.21621513180434704\n",
      "Epoch 52/100: Loss: 0.18939740341156722\n",
      "Epoch 53/100: Loss: 0.24777985382825135\n",
      "Epoch 54/100: Loss: 0.24277884373441339\n",
      "Epoch 55/100: Loss: 0.24984388621523976\n",
      "Epoch 56/100: Loss: 0.31402429696172474\n",
      "Epoch 57/100: Loss: 0.1961133187636733\n",
      "Epoch 58/100: Loss: 0.21310617150738836\n",
      "Epoch 59/100: Loss: 0.23604396935552358\n",
      "Epoch 60/100: Loss: 0.2076715525239706\n",
      "Epoch 61/100: Loss: 0.2850320406258106\n",
      "Epoch 62/100: Loss: 0.21931070275604725\n",
      "Epoch 63/100: Loss: 0.24050685316324233\n",
      "Epoch 64/100: Loss: 0.19513388350605965\n",
      "Epoch 65/100: Loss: 0.20748892407864333\n",
      "Epoch 66/100: Loss: 0.22432216312736272\n",
      "Epoch 67/100: Loss: 0.1553194910287857\n",
      "Epoch 68/100: Loss: 0.20666514430195093\n",
      "Epoch 69/100: Loss: 0.11377631518989802\n",
      "Epoch 70/100: Loss: 0.08793272287584841\n",
      "Epoch 71/100: Loss: 0.10592761787120253\n",
      "Epoch 72/100: Loss: 0.18743001190014183\n",
      "Epoch 73/100: Loss: 0.15874094385653734\n",
      "Epoch 74/100: Loss: 0.1467025162652135\n",
      "Epoch 75/100: Loss: 0.12173352655954658\n",
      "Epoch 76/100: Loss: 0.14626408303156496\n",
      "Epoch 77/100: Loss: 0.15818057572469116\n",
      "Epoch 78/100: Loss: 0.15776813235133885\n",
      "Epoch 79/100: Loss: 0.13489196933805941\n",
      "Epoch 80/100: Loss: 0.09598339265212416\n",
      "Epoch 81/100: Loss: 0.08169791242107749\n",
      "Epoch 82/100: Loss: 0.140144629124552\n",
      "Epoch 83/100: Loss: 0.06499589206650853\n",
      "Epoch 84/100: Loss: 0.10244765400420874\n",
      "Epoch 85/100: Loss: 0.13327336532529444\n",
      "Epoch 86/100: Loss: 0.09235162222757935\n",
      "Epoch 87/100: Loss: 0.09609502581879496\n",
      "Epoch 88/100: Loss: 0.11090917591936886\n",
      "Epoch 89/100: Loss: 0.07154549723491073\n",
      "Epoch 90/100: Loss: 0.0846303426194936\n",
      "Epoch 91/100: Loss: 0.10284037608653307\n",
      "Epoch 92/100: Loss: 0.08362956997007132\n",
      "Epoch 93/100: Loss: 0.07382979420945049\n",
      "Epoch 94/100: Loss: 0.03545547593384981\n",
      "Epoch 95/100: Loss: 0.08000625628046691\n",
      "Epoch 96/100: Loss: 0.0587926039705053\n",
      "Epoch 97/100: Loss: 0.06327737965621054\n",
      "Epoch 98/100: Loss: 0.03025690340436995\n",
      "Epoch 99/100: Loss: 0.05440185710031074\n",
      "Epoch 100/100: Loss: 0.0528781330678612\n",
      "time costs: 621.3260200023651\n",
      "--------------------\n",
      "training epoch: 8\n",
      "Epoch 1/100: Loss: 1.0363399863243103\n",
      "Epoch 2/100: Loss: 1.0245608299970628\n",
      "Epoch 3/100: Loss: 1.0251706212759018\n",
      "Epoch 4/100: Loss: 0.9842930138111115\n",
      "Epoch 5/100: Loss: 0.977897834777832\n",
      "Epoch 6/100: Loss: 0.9608821898698807\n",
      "Epoch 7/100: Loss: 0.9931500107049942\n",
      "Epoch 8/100: Loss: 0.9708285123109818\n",
      "Epoch 9/100: Loss: 0.9749644637107849\n",
      "Epoch 10/100: Loss: 0.9574263721704483\n",
      "Epoch 11/100: Loss: 0.905433389544487\n",
      "Epoch 12/100: Loss: 0.9066242963075638\n",
      "Epoch 13/100: Loss: 0.8918124556541442\n",
      "Epoch 14/100: Loss: 0.9245789259672165\n",
      "Epoch 15/100: Loss: 0.8600151032209397\n",
      "Epoch 16/100: Loss: 0.7912329316139222\n",
      "Epoch 17/100: Loss: 0.8915872663259506\n",
      "Epoch 18/100: Loss: 0.7699373677372933\n",
      "Epoch 19/100: Loss: 0.773363108932972\n",
      "Epoch 20/100: Loss: 0.6421864308416844\n",
      "Epoch 21/100: Loss: 0.707417045533657\n",
      "Epoch 22/100: Loss: 0.7016737692058086\n",
      "Epoch 23/100: Loss: 0.7867022812366485\n",
      "Epoch 24/100: Loss: 0.7193295493721962\n",
      "Epoch 25/100: Loss: 0.6314695795997978\n",
      "Epoch 26/100: Loss: 0.6090267468243837\n",
      "Epoch 27/100: Loss: 0.7126235645264387\n",
      "Epoch 28/100: Loss: 0.6042894626036286\n",
      "Epoch 29/100: Loss: 0.5503611466381699\n",
      "Epoch 30/100: Loss: 0.5990408356068656\n",
      "Epoch 31/100: Loss: 0.6715237363940105\n",
      "Epoch 32/100: Loss: 0.5552112518809735\n",
      "Epoch 33/100: Loss: 0.6011780520668253\n",
      "Epoch 34/100: Loss: 0.7179811859503389\n",
      "Epoch 35/100: Loss: 0.5996364722028374\n",
      "Epoch 36/100: Loss: 0.5768252007197588\n",
      "Epoch 37/100: Loss: 0.4555759871378541\n",
      "Epoch 38/100: Loss: 0.5181117363506929\n",
      "Epoch 39/100: Loss: 0.4319219801807776\n",
      "Epoch 40/100: Loss: 0.5356673689093441\n",
      "Epoch 41/100: Loss: 0.44146828781813385\n",
      "Epoch 42/100: Loss: 0.41177787114866077\n",
      "Epoch 43/100: Loss: 0.602577023440972\n",
      "Epoch 44/100: Loss: 0.452790473587811\n",
      "Epoch 45/100: Loss: 0.40846768834162506\n",
      "Epoch 46/100: Loss: 0.5014215747825801\n",
      "Epoch 47/100: Loss: 0.48747581969946624\n",
      "Epoch 48/100: Loss: 0.42749104206450284\n",
      "Epoch 49/100: Loss: 0.29088134081102907\n",
      "Epoch 50/100: Loss: 0.420164373726584\n",
      "Epoch 51/100: Loss: 0.3266253270674497\n",
      "Epoch 52/100: Loss: 0.3691607397980988\n",
      "Epoch 53/100: Loss: 0.27845884654670955\n",
      "Epoch 54/100: Loss: 0.35323225776082834\n",
      "Epoch 55/100: Loss: 0.2809651034185663\n",
      "Epoch 56/100: Loss: 0.4019615681841969\n",
      "Epoch 57/100: Loss: 0.3287667605094612\n",
      "Epoch 58/100: Loss: 0.27960068951360884\n",
      "Epoch 59/100: Loss: 0.298010551603511\n",
      "Epoch 60/100: Loss: 0.23136416003108023\n",
      "Epoch 61/100: Loss: 0.3212957525160164\n",
      "Epoch 62/100: Loss: 0.29729565398301927\n",
      "Epoch 63/100: Loss: 0.16467009874759242\n",
      "Epoch 64/100: Loss: 0.2234430104261264\n",
      "Epoch 65/100: Loss: 0.20451991902664304\n",
      "Epoch 66/100: Loss: 0.2242581869242713\n",
      "Epoch 67/100: Loss: 0.19917456451803445\n",
      "Epoch 68/100: Loss: 0.22741770264692604\n",
      "Epoch 69/100: Loss: 0.13886895161122084\n",
      "Epoch 70/100: Loss: 0.17113237988669425\n",
      "Epoch 71/100: Loss: 0.1428366088774055\n",
      "Epoch 72/100: Loss: 0.1518544309801655\n",
      "Epoch 73/100: Loss: 0.09490543966530822\n",
      "Epoch 74/100: Loss: 0.11354133994318545\n",
      "Epoch 75/100: Loss: 0.13395327867474408\n",
      "Epoch 76/100: Loss: 0.1614151356741786\n",
      "Epoch 77/100: Loss: 0.060147686605341734\n",
      "Epoch 78/100: Loss: 0.13764254201669246\n",
      "Epoch 79/100: Loss: 0.14985324681038037\n",
      "Epoch 80/100: Loss: 0.1085174212232232\n",
      "Epoch 81/100: Loss: 0.09042801142204553\n",
      "Epoch 82/100: Loss: 0.12333218257408589\n",
      "Epoch 83/100: Loss: 0.08584920917637646\n",
      "Epoch 84/100: Loss: 0.05918520085979253\n",
      "Epoch 85/100: Loss: 0.059805800765752794\n",
      "Epoch 86/100: Loss: 0.06126124302973039\n",
      "Epoch 87/100: Loss: 0.06284983332734555\n",
      "Epoch 88/100: Loss: 0.03908328890101984\n",
      "Epoch 89/100: Loss: 0.06275318559492007\n",
      "Epoch 90/100: Loss: 0.061686909408308566\n",
      "Epoch 91/100: Loss: 0.029851633525686337\n",
      "Epoch 92/100: Loss: 0.026298122334992513\n",
      "Epoch 93/100: Loss: 0.04341091875685379\n",
      "Epoch 94/100: Loss: 0.02289819440629799\n",
      "Epoch 95/100: Loss: 0.032205114383395995\n",
      "Epoch 96/100: Loss: 0.031678847459988904\n",
      "Epoch 97/100: Loss: 0.023249148465401957\n",
      "Epoch 98/100: Loss: 0.014869336708215997\n",
      "Epoch 99/100: Loss: 0.020441442404990084\n",
      "Epoch 100/100: Loss: 0.017930682055884974\n",
      "time costs: 622.7621483802795\n",
      "--------------------\n",
      "training epoch: 9\n",
      "Epoch 1/100: Loss: 1.0403956353664399\n",
      "Epoch 2/100: Loss: 0.9890468508005142\n",
      "Epoch 3/100: Loss: 1.0044741749763488\n",
      "Epoch 4/100: Loss: 1.0050720483064652\n",
      "Epoch 5/100: Loss: 0.9920009642839431\n",
      "Epoch 6/100: Loss: 0.9945591956377029\n",
      "Epoch 7/100: Loss: 0.991897639632225\n",
      "Epoch 8/100: Loss: 0.9641587764024735\n",
      "Epoch 9/100: Loss: 0.9846698015928268\n",
      "Epoch 10/100: Loss: 0.9916219025850296\n",
      "Epoch 11/100: Loss: 0.9947942584753037\n",
      "Epoch 12/100: Loss: 0.9621853709220887\n",
      "Epoch 13/100: Loss: 0.951722827553749\n",
      "Epoch 14/100: Loss: 0.9508769065141678\n",
      "Epoch 15/100: Loss: 0.971730837225914\n",
      "Epoch 16/100: Loss: 0.9254119724035264\n",
      "Epoch 17/100: Loss: 0.9597701013088227\n",
      "Epoch 18/100: Loss: 0.9405568718910218\n",
      "Epoch 19/100: Loss: 0.9259080290794373\n",
      "Epoch 20/100: Loss: 0.9273252725601197\n",
      "Epoch 21/100: Loss: 0.918703842163086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: Loss: 0.8895073264837265\n",
      "Epoch 23/100: Loss: 0.8372854262590408\n",
      "Epoch 24/100: Loss: 0.8225513309240341\n",
      "Epoch 25/100: Loss: 0.9082793712615966\n",
      "Epoch 26/100: Loss: 0.8613242357969284\n",
      "Epoch 27/100: Loss: 0.7691125333309173\n",
      "Epoch 28/100: Loss: 0.8259631335735321\n",
      "Epoch 29/100: Loss: 0.767895045876503\n",
      "Epoch 30/100: Loss: 0.8041195288300514\n",
      "Epoch 31/100: Loss: 0.8309613347053528\n",
      "Epoch 32/100: Loss: 0.7375715807080269\n",
      "Epoch 33/100: Loss: 0.7590333417057991\n",
      "Epoch 34/100: Loss: 0.7246410980820656\n",
      "Epoch 35/100: Loss: 0.7063037842512131\n",
      "Epoch 36/100: Loss: 0.588618440926075\n",
      "Epoch 37/100: Loss: 0.6294200703501701\n",
      "Epoch 38/100: Loss: 0.6599722370505333\n",
      "Epoch 39/100: Loss: 0.6005200281739235\n",
      "Epoch 40/100: Loss: 0.7054622173309326\n",
      "Epoch 41/100: Loss: 0.617984976619482\n",
      "Epoch 42/100: Loss: 0.6860066168010235\n",
      "Epoch 43/100: Loss: 0.730484490096569\n",
      "Epoch 44/100: Loss: 0.5366542637348175\n",
      "Epoch 45/100: Loss: 0.7094791203737258\n",
      "Epoch 46/100: Loss: 0.6546778410673142\n",
      "Epoch 47/100: Loss: 0.6206422053277493\n",
      "Epoch 48/100: Loss: 0.5130021400749684\n",
      "Epoch 49/100: Loss: 0.5892775014042855\n",
      "Epoch 50/100: Loss: 0.6721349492669105\n",
      "Epoch 51/100: Loss: 0.6061470612883568\n",
      "Epoch 52/100: Loss: 0.6463890030980111\n",
      "Epoch 53/100: Loss: 0.5476852536201477\n",
      "Epoch 54/100: Loss: 0.5156491741538047\n",
      "Epoch 55/100: Loss: 0.5023511976003647\n",
      "Epoch 56/100: Loss: 0.5275511335581541\n",
      "Epoch 57/100: Loss: 0.5834848374128342\n",
      "Epoch 58/100: Loss: 0.6494295038282871\n",
      "Epoch 59/100: Loss: 0.6467965260148049\n",
      "Epoch 60/100: Loss: 0.6743917472660541\n",
      "Epoch 61/100: Loss: 0.5599988862872124\n",
      "Epoch 62/100: Loss: 0.4855040907859802\n",
      "Epoch 63/100: Loss: 0.5449326440691948\n",
      "Epoch 64/100: Loss: 0.5515391625463962\n",
      "Epoch 65/100: Loss: 0.4756231740117073\n",
      "Epoch 66/100: Loss: 0.5092394519597292\n",
      "Epoch 67/100: Loss: 0.5388626385480165\n",
      "Epoch 68/100: Loss: 0.5437500074505806\n",
      "Epoch 69/100: Loss: 0.4790498372167349\n",
      "Epoch 70/100: Loss: 0.5663935203105211\n",
      "Epoch 71/100: Loss: 0.5709750212728977\n",
      "Epoch 72/100: Loss: 0.5020470708608628\n",
      "Epoch 73/100: Loss: 0.47884327098727225\n",
      "Epoch 74/100: Loss: 0.44589312449097634\n",
      "Epoch 75/100: Loss: 0.40408106446266173\n",
      "Epoch 76/100: Loss: 0.4697227204218507\n",
      "Epoch 77/100: Loss: 0.5904098369181157\n",
      "Epoch 78/100: Loss: 0.47063820213079455\n",
      "Epoch 79/100: Loss: 0.4675182715058327\n",
      "Epoch 80/100: Loss: 0.46391732171177863\n",
      "Epoch 81/100: Loss: 0.4377114959061146\n",
      "Epoch 82/100: Loss: 0.5110780499875546\n",
      "Epoch 83/100: Loss: 0.37449592277407645\n",
      "Epoch 84/100: Loss: 0.4171067584306002\n",
      "Epoch 85/100: Loss: 0.434679476544261\n",
      "Epoch 86/100: Loss: 0.46338713355362415\n",
      "Epoch 87/100: Loss: 0.4353730298578739\n",
      "Epoch 88/100: Loss: 0.42036906592547896\n",
      "Epoch 89/100: Loss: 0.36855641975998876\n",
      "Epoch 90/100: Loss: 0.4269512087106705\n",
      "Epoch 91/100: Loss: 0.378989876806736\n",
      "Epoch 92/100: Loss: 0.38859614804387094\n",
      "Epoch 93/100: Loss: 0.37872111797332764\n",
      "Epoch 94/100: Loss: 0.3186030887067318\n",
      "Epoch 95/100: Loss: 0.30541740991175176\n",
      "Epoch 96/100: Loss: 0.31109179481863974\n",
      "Epoch 97/100: Loss: 0.3011697966605425\n",
      "Epoch 98/100: Loss: 0.3192083083093166\n",
      "Epoch 99/100: Loss: 0.3089912999421358\n",
      "Epoch 100/100: Loss: 0.24454751294106244\n",
      "time costs: 627.8255014419556\n",
      "--------------------\n",
      "training epoch: 10\n",
      "Epoch 1/100: Loss: 0.9966003060340881\n",
      "Epoch 2/100: Loss: 0.9852993696928024\n",
      "Epoch 3/100: Loss: 0.9350566357374192\n",
      "Epoch 4/100: Loss: 1.0312654674053192\n",
      "Epoch 5/100: Loss: 0.982419365644455\n",
      "Epoch 6/100: Loss: 0.9816699028015137\n",
      "Epoch 7/100: Loss: 1.0156382501125336\n",
      "Epoch 8/100: Loss: 0.9768806457519531\n",
      "Epoch 9/100: Loss: 0.9642568945884704\n",
      "Epoch 10/100: Loss: 0.9417243450880051\n",
      "Epoch 11/100: Loss: 0.9365448355674744\n",
      "Epoch 12/100: Loss: 0.9362317115068436\n",
      "Epoch 13/100: Loss: 0.9130622178316117\n",
      "Epoch 14/100: Loss: 0.8897190779447556\n",
      "Epoch 15/100: Loss: 0.9074551284313201\n",
      "Epoch 16/100: Loss: 0.8864008635282516\n",
      "Epoch 17/100: Loss: 0.8600517719984054\n",
      "Epoch 18/100: Loss: 0.8582845628261566\n",
      "Epoch 19/100: Loss: 0.847729179263115\n",
      "Epoch 20/100: Loss: 0.8194895505905151\n",
      "Epoch 21/100: Loss: 0.7903323203325272\n",
      "Epoch 22/100: Loss: 0.7859789073467255\n",
      "Epoch 23/100: Loss: 0.7649311542510986\n",
      "Epoch 24/100: Loss: 0.7178679227828979\n",
      "Epoch 25/100: Loss: 0.7214883506298065\n",
      "Epoch 26/100: Loss: 0.6644308149814606\n",
      "Epoch 27/100: Loss: 0.6747402161359787\n",
      "Epoch 28/100: Loss: 0.6811002597212792\n",
      "Epoch 29/100: Loss: 0.6532829105854034\n",
      "Epoch 30/100: Loss: 0.6080691754817963\n",
      "Epoch 31/100: Loss: 0.5485897496342659\n",
      "Epoch 32/100: Loss: 0.6124928042292594\n",
      "Epoch 33/100: Loss: 0.543622350692749\n",
      "Epoch 34/100: Loss: 0.5233649775385857\n",
      "Epoch 35/100: Loss: 0.4774914085865021\n",
      "Epoch 36/100: Loss: 0.5241003356873989\n",
      "Epoch 37/100: Loss: 0.5321751773357392\n",
      "Epoch 38/100: Loss: 0.5362184278666973\n",
      "Epoch 39/100: Loss: 0.5185827478766442\n",
      "Epoch 40/100: Loss: 0.5121546342968941\n",
      "Epoch 41/100: Loss: 0.4359896153211594\n",
      "Epoch 42/100: Loss: 0.5248210959136486\n",
      "Epoch 43/100: Loss: 0.4492236703634262\n",
      "Epoch 44/100: Loss: 0.42807929292321206\n",
      "Epoch 45/100: Loss: 0.4465091109275818\n",
      "Epoch 46/100: Loss: 0.4296163022518158\n",
      "Epoch 47/100: Loss: 0.5051100485026836\n",
      "Epoch 48/100: Loss: 0.39844174310564995\n",
      "Epoch 49/100: Loss: 0.42056396752595904\n",
      "Epoch 50/100: Loss: 0.43238635212182996\n",
      "Epoch 51/100: Loss: 0.4034821718931198\n",
      "Epoch 52/100: Loss: 0.3621062207967043\n",
      "Epoch 53/100: Loss: 0.415294074267149\n",
      "Epoch 54/100: Loss: 0.3172381196171045\n",
      "Epoch 55/100: Loss: 0.3345831375569105\n",
      "Epoch 56/100: Loss: 0.4296642251312733\n",
      "Epoch 57/100: Loss: 0.377312283962965\n",
      "Epoch 58/100: Loss: 0.262204309925437\n",
      "Epoch 59/100: Loss: 0.3225569862872362\n",
      "Epoch 60/100: Loss: 0.37049592286348343\n",
      "Epoch 61/100: Loss: 0.2977270305156708\n",
      "Epoch 62/100: Loss: 0.312410569190979\n",
      "Epoch 63/100: Loss: 0.25451409369707106\n",
      "Epoch 64/100: Loss: 0.324217290058732\n",
      "Epoch 65/100: Loss: 0.330031081661582\n",
      "Epoch 66/100: Loss: 0.27953401654958726\n",
      "Epoch 67/100: Loss: 0.27946179881691935\n",
      "Epoch 68/100: Loss: 0.22671449854969977\n",
      "Epoch 69/100: Loss: 0.24762511579319835\n",
      "Epoch 70/100: Loss: 0.22709191497415304\n",
      "Epoch 71/100: Loss: 0.22740633599460125\n",
      "Epoch 72/100: Loss: 0.2297572972252965\n",
      "Epoch 73/100: Loss: 0.1706492222379893\n",
      "Epoch 74/100: Loss: 0.24337597489356994\n",
      "Epoch 75/100: Loss: 0.25501776169985535\n",
      "Epoch 76/100: Loss: 0.21320304479449986\n",
      "Epoch 77/100: Loss: 0.11799996783956886\n",
      "Epoch 78/100: Loss: 0.18946257941424846\n",
      "Epoch 79/100: Loss: 0.2363297173753381\n",
      "Epoch 80/100: Loss: 0.18161472035571932\n",
      "Epoch 81/100: Loss: 0.22297772038727998\n",
      "Epoch 82/100: Loss: 0.22834977898746728\n",
      "Epoch 83/100: Loss: 0.19587684106081724\n",
      "Epoch 84/100: Loss: 0.16677723601460456\n",
      "Epoch 85/100: Loss: 0.17805868992581964\n",
      "Epoch 86/100: Loss: 0.15308903641998767\n",
      "Epoch 87/100: Loss: 0.15114710051566363\n",
      "Epoch 88/100: Loss: 0.13190325135365127\n",
      "Epoch 89/100: Loss: 0.18682241667993366\n",
      "Epoch 90/100: Loss: 0.1321285021142103\n",
      "Epoch 91/100: Loss: 0.10936730045359581\n",
      "Epoch 92/100: Loss: 0.0737667077803053\n",
      "Epoch 93/100: Loss: 0.12003439508844167\n",
      "Epoch 94/100: Loss: 0.09348553827730939\n",
      "Epoch 95/100: Loss: 0.08818143983371556\n",
      "Epoch 96/100: Loss: 0.14419652735814453\n",
      "Epoch 97/100: Loss: 0.09642042699269951\n",
      "Epoch 98/100: Loss: 0.058025567629374566\n",
      "Epoch 99/100: Loss: 0.09584711752831936\n",
      "Epoch 100/100: Loss: 0.053506634780205786\n",
      "time costs: 631.3726766109467\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "accuarcies = []\n",
    "times = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'training epoch: {i + 1}')\n",
    "    qmodel = QModel(input_size, hidden_size, num_output, \n",
    "                num_layers = num_layers, ctx = ctx, mode='classical')\n",
    "    optimizer = torch.optim.Adam(qmodel.parameters(), lr = 0.001)\n",
    "    loss_func = nn.MSELoss()\n",
    "    start = time.time()\n",
    "    losses = train_model(qmodel, train_data, batch_size=20,          \n",
    "                   loss_func = loss_func, optimizer = optimizer, epoch = 100)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'time costs: {end - start}')\n",
    "    times.append(end - start)\n",
    "    \n",
    "    acc = calculate_accuarcy(qmodel, X_test, y_test)\n",
    "    accuarcies.append(acc)\n",
    "    \n",
    "    with open(f'loss/layer3/loss{i + 1}.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(losses, pkl_file)\n",
    "    \n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ac9c597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x18641e2f2e0>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses, color=\"#FF6666\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3343ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantum paramers: 64\n"
     ]
    }
   ],
   "source": [
    "print(f'quantum paramers: {QLSTM(1, 1, ctx = ctx).qparameters_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5af0126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6611309"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.6611309\n",
    "# \n",
    "# [tensor(0.3898),\n",
    "#  tensor(0.2961),\n",
    "#  tensor(0.6735),\n",
    "#  tensor(0.8223),\n",
    "#  tensor(0.9300),\n",
    "#  tensor(0.8827),\n",
    "#  tensor(0.7797),\n",
    "#  tensor(0.8625),\n",
    "#  tensor(0.3196),\n",
    "#  tensor(0.6551)]\n",
    "np.mean(accuarcies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e317ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "598.2566781282425"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 598.2566781282425\n",
    "np.mean(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2370cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
