{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6581af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94243045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff0ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyqpanda import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "455c7f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a618a3a",
   "metadata": {},
   "source": [
    "# 1. Prepare Dadaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c64faba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/sumanthvrao/daily-climate-time-series-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de627766",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './../data/DailyDelhiClimateTrain.csv'\n",
    "test_path = './../data/DailyDelhiClimateTest.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d0827fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [1,2,3,4]\n",
    "\n",
    "train = pd.read_csv(train_path, usecols=cols, engine=\"python\")\n",
    "test = pd.read_csv(test_path, usecols=cols, engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c3039c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train)=1462\n",
      "len(test)=114\n"
     ]
    }
   ],
   "source": [
    "print(f'len(train)={len(train)}')\n",
    "print(f'len(test)={len(test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b941db2e",
   "metadata": {},
   "source": [
    "## 1.1 Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e59fddc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove outliers num: 9\n"
     ]
    }
   ],
   "source": [
    "unnormal_num = 0\n",
    "for i in range(len(train)):\n",
    "    mp = train.iloc[i][3]\n",
    "    if mp > 1200 or mp < 950:\n",
    "        unnormal_num += 1\n",
    "        train.iloc[i][3] = train.iloc[i + 1][3]\n",
    "print(f'remove outliers num: {unnormal_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fefec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.iloc[0][3] = test.iloc[1][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6035297e",
   "metadata": {},
   "source": [
    "## 1.2 Transfer data to LSTM representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a1277fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "884bc13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(data, window_size, predict_size):\n",
    "    scaler = StandardScaler()\n",
    "    data = scaler.fit_transform(np.array(data).reshape(-1, 1))\n",
    "    \n",
    "    data_in = []\n",
    "    data_out = []\n",
    "    \n",
    "    for i in range(data.shape[0] - window_size - predict_size):\n",
    "        data_in.append(data[i:i + window_size].reshape(1, window_size)[0])\n",
    "        data_out.append(data[i + window_size:i + window_size + predict_size].reshape(1, predict_size)[0])\n",
    "        \n",
    "    data_in = np.array(data_in).reshape(-1, window_size)\n",
    "    data_out = np.array(data_out).reshape(-1, predict_size)\n",
    "    \n",
    "    data_process = {'datain': data_in, 'dataout': data_out}\n",
    "    \n",
    "    return data_process, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6517fe60",
   "metadata": {},
   "source": [
    "## 1.3 prepare train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d333c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_size = 4\n",
    "window_size = features_size * 3 # features num * time steps\n",
    "predict_size = features_size # features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ef548b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed, train_scaler = data_process(train, window_size, predict_size)\n",
    "X_train, y_train = train_processed['datain'], train_processed['dataout']\n",
    "\n",
    "test_processed, test_scaler = data_process(test, window_size, predict_size)\n",
    "X_test, y_test = test_processed['datain'], test_processed['dataout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f779325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dda516d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "\n",
    "train_data = Data.TensorDataset(X_train, y_train)\n",
    "test_data = Data.TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb132c",
   "metadata": {},
   "source": [
    "# 2. Quantum Enhanced LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d517f5f3",
   "metadata": {},
   "source": [
    "## 2.1 initiate quantum environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffc85d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitQMachine:\n",
    "    def __init__(self, qubitsCount, cbitsCount = 0, machineType = QMachineType.CPU):\n",
    "        self.machine = init_quantum_machine(machineType)\n",
    "        \n",
    "        self.qubits = self.machine.qAlloc_many(qubitsCount)\n",
    "        self.cbits = self.machine.cAlloc_many(cbitsCount)\n",
    "        \n",
    "        print(f'Init Quantum Machine with qubits:[{qubitsCount}] / cbits:[{cbitsCount}] Successfully')\n",
    "    \n",
    "    def __del__(self):\n",
    "        destroy_quantum_machine(self.machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "741a6a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Quantum Machine with qubits:[4] / cbits:[0] Successfully\n"
     ]
    }
   ],
   "source": [
    "# maximum qubits size\n",
    "ctx = InitQMachine(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a39cf3",
   "metadata": {},
   "source": [
    "## 2.2 Quantum Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b029d",
   "metadata": {},
   "source": [
    "### 2.2.1 Quantum layer base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5570e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f9cfd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayerBase(nn.Module):\n",
    "    def __init__(self, input_size, output_size, *, n_qubits, n_layers = 1, ctx = None):\n",
    "        super(QuantumLayerBase, self).__init__()\n",
    "        \n",
    "        self.data = None # need to input during forward\n",
    "    \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size # hidden size, not n_qubits\n",
    "        \n",
    "        # quantum infos\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.ctx = ctx\n",
    "        self.qubits = ctx.qubits\n",
    "        self.machine = ctx.machine\n",
    "        \n",
    "        # convert quantum input/output to match classical computation\n",
    "        self.qin = nn.Linear(self.input_size, self.n_qubits)\n",
    "        self.qout = nn.Linear(self.n_qubits, self.output_size)\n",
    "        \n",
    "    @property\n",
    "    def circuit(self):\n",
    "        raise NotImplementedError('Should init circuit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94c5bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(self):\n",
    "    HamiZ = [ PauliOperator({f'Z{i}': 1}) for i in range(len(self.qubits)) ]\n",
    "    res = [ eval(qop(self.circuit, Hami, self.machine, self.qubits))[0,0] for Hami in HamiZ ]\n",
    "    \n",
    "    return Parameter(Tensor(res[:self.n_qubits]))\n",
    "\n",
    "QuantumLayerBase.measure = measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4341341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, inputs):\n",
    "    y_t = self.qin(Parameter(inputs))\n",
    "    self.data = y_t[0]\n",
    "    \n",
    "    return self.qout(self.measure())\n",
    "\n",
    "QuantumLayerBase.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4f794b",
   "metadata": {},
   "source": [
    "### 2.2.2 Quantum layer design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b98a8984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayer(QuantumLayerBase):\n",
    "    def __init__(self, input_size, output_size, *, n_qubits, degree = 1, n_layers = 1, ctx = None):\n",
    "        super(QuantumLayer, self).__init__(input_size, output_size, \n",
    "                                         n_qubits = n_qubits, n_layers = n_layers, ctx = ctx)\n",
    "        \n",
    "        self.degree = degree\n",
    "        self.angles = Parameter(torch.rand(n_layers * 7, degree, self.n_qubits))\n",
    "        \n",
    "    @property\n",
    "    def qparameters_size(self):\n",
    "        return self.angles.flatten().size()[0]\n",
    "        \n",
    "    @property\n",
    "    def circuit(self):\n",
    "        if self.data == None:\n",
    "            raise ValueError('Need to feed a input data!')\n",
    "        \n",
    "        n = self.n_qubits\n",
    "        q = self.qubits\n",
    "        x = self.data\n",
    "        p = self.angles\n",
    "        degree = self.degree\n",
    "        \n",
    "        h = VariationalQuantumGate_H\n",
    "        rx = VariationalQuantumGate_RX\n",
    "        ry = VariationalQuantumGate_RY\n",
    "        rz = VariationalQuantumGate_RZ\n",
    "        crx = VariationalQuantumGate_CRX\n",
    "        \n",
    "        # init variational quantum circuit\n",
    "        vqc = VariationalQuantumCircuit()\n",
    "\n",
    "        # encoding layer\n",
    "        [ vqc.insert( h(q[i]) ) for i in range(n) ]\n",
    "        [ vqc.insert( ry(q[i], var(x[i] * torch.pi / 2)) ) for i in range(n) ]\n",
    "        \n",
    "        # variational layer\n",
    "        [ vqc.insert( rx(q[i], var(p[0][0][i]) )) for i in range(n) ]\n",
    "        [ vqc.insert( rz(q[i], var(p[1][0][i]) )) for i in range(n) ]\n",
    "        \n",
    "        vqc.insert(crx(q[2], q[3], var(p[2][0][0])))\n",
    "        vqc.insert(crx(q[1], q[3], var(p[2][0][1])))\n",
    "        vqc.insert(crx(q[0], q[3], var(p[2][0][2])))\n",
    "        vqc.insert(crx(q[3], q[2], var(p[2][0][3])))\n",
    "        \n",
    "        vqc.insert(crx(q[1], q[2], var(p[3][0][0])))\n",
    "        vqc.insert(crx(q[0], q[2], var(p[3][0][1])))\n",
    "        vqc.insert(crx(q[3], q[1], var(p[3][0][2])))\n",
    "        vqc.insert(crx(q[2], q[1], var(p[3][0][3])))\n",
    "        \n",
    "        vqc.insert(crx(q[0], q[1], var(p[4][0][0])))\n",
    "        vqc.insert(crx(q[3], q[0], var(p[4][0][1])))\n",
    "        vqc.insert(crx(q[2], q[0], var(p[4][0][2])))\n",
    "        vqc.insert(crx(q[1], q[0], var(p[4][0][3])))\n",
    "        \n",
    "        [ vqc.insert( rx(q[i], var(p[5][0][i]) )) for i in range(n) ]\n",
    "        [ vqc.insert( rz(q[i], var(p[6][0][i]) )) for i in range(n) ]\n",
    "        \n",
    "        return vqc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766692de",
   "metadata": {},
   "source": [
    "### 2.2.3 Plot Quantum Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88ff286b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyqpanda.pyQPanda.QProg at 0x1c391a0bf30>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Tensor([[0.1, 0.2, 0.3, 0.4]])\n",
    "layer = QuantumLayer(4, 4, n_qubits=4, n_layers=1, degree=3, ctx=ctx)\n",
    "layer.data = data[0]\n",
    "vqc = layer.circuit\n",
    "prog = create_empty_qprog()\n",
    "prog.insert(vqc.feed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fd364d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'null'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_qprog(prog, 'pic', filename=f'pic/layer4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50742459",
   "metadata": {},
   "source": [
    "## 2.3 Quantum-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e53ae0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLSTMBase(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, *, ctx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ctx = ctx\n",
    "        \n",
    "    @property\n",
    "    def qparameters_size(self):\n",
    "        num = 0\n",
    "        for attr in dir(self):\n",
    "            if attr.endswith('_circuit'):\n",
    "                num += getattr(self, attr).qparameters_size\n",
    "        return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "582b17af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, inputs, init_states = None):\n",
    "    sequence_size, batch_size, _ = inputs.size()\n",
    "    hidden_sequence = []\n",
    "    \n",
    "    if init_states == None:\n",
    "        h_t, c_t = (\n",
    "            torch.zeros(1, batch_size, self.hidden_size).to(inputs.device),\n",
    "            torch.zeros(1, batch_size, self.hidden_size).to(inputs.device),\n",
    "        )\n",
    "    else:\n",
    "        h_t, c_t = init_states\n",
    "    \n",
    "    return hidden_sequence, (h_t, c_t)\n",
    "\n",
    "QLSTMBase.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d6b0af",
   "metadata": {},
   "source": [
    "## - classical quatum enhanced LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adc8fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLSTM(QLSTMBase):\n",
    "    def __init__(self, input_size, hidden_size, *, ctx):\n",
    "        super().__init__(input_size, hidden_size, ctx = ctx)\n",
    "    \n",
    "        # input gates\n",
    "        self.input_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                        n_qubits = 4, ctx = ctx) # 15\n",
    "        # forget gates\n",
    "        self.forget_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                         n_qubits = 4, ctx = ctx) # 15\n",
    "        # candidate\n",
    "        self.candidate_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                       n_qubits = 4, ctx = ctx) # 15\n",
    "        # output gates\n",
    "        self.output_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                         n_qubits = 4, ctx = ctx) # 15\n",
    "        \n",
    "    def forward(self, inputs, init_states = None):\n",
    "        hidden_sequence, (h_t, c_t) = super(QLSTM, self).forward(inputs, init_states)\n",
    "\n",
    "        for t in range(inputs.size()[0]):\n",
    "            x_t = inputs[t, :, :]\n",
    "            v_t = torch.cat((h_t[0], x_t), dim = 1)\n",
    "\n",
    "            # input gates\n",
    "            i_t = torch.sigmoid(self.input_circuit(v_t))\n",
    "            # forget gates\n",
    "            f_t = torch.sigmoid(self.forget_circuit(v_t))\n",
    "            # candidate for cell state update\n",
    "            g_t = torch.tanh(self.candidate_circuit(v_t))\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "\n",
    "            # output gates\n",
    "            o_t = torch.sigmoid(self.output_circuit(v_t))\n",
    "            # update output ht\n",
    "            h_t = o_t * (torch.tanh(c_t))\n",
    "\n",
    "            hidden_sequence.append(h_t)\n",
    "\n",
    "        # reshape hidden_seq p/ retornar\n",
    "        #\n",
    "        # [tensor([[[0.0444, ...]]] => tensor([[[0.0444, ...]]]\n",
    "        # \n",
    "        hidden_sequence = torch.cat(hidden_sequence, dim = 0)\n",
    "\n",
    "        return hidden_sequence, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28836475",
   "metadata": {},
   "source": [
    "## 2.4 Stacked QLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ec06c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class StackedQLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, *, num_layers = 1, ctx = None, mode = 'classical'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.qlstms = nn.Sequential(OrderedDict([\n",
    "            (f'QLSTM {i + 1}', QLSTM(input_size if i == 0 else hidden_size , hidden_size, ctx = ctx)) \n",
    "                for i in range(num_layers)\n",
    "        ]))\n",
    "\n",
    "    def forward(self, inputs, parameters = None):\n",
    "        outputs = None\n",
    "        \n",
    "        for i, qlstm in enumerate(self.qlstms):\n",
    "            if i != 0:\n",
    "                inputs = outputs\n",
    "            \n",
    "            outputs, parameters = qlstm(inputs, parameters)\n",
    "        \n",
    "        return outputs, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0389535c",
   "metadata": {},
   "source": [
    "# 3. Quantum Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "413150bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_output, *, num_layers = 1, ctx = None, mode = 'classical'):\n",
    "        super(QModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.qlstm = StackedQLSTM(input_size, hidden_size, \n",
    "                                  num_layers = num_layers, ctx = ctx, mode = mode)\n",
    "        self.predict = nn.Linear(hidden_size, num_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        # sequence lenth , batch_size, features length\n",
    "        # \n",
    "        h0 = torch.zeros(1, x.size(1), self.hidden_size)\n",
    "        c0 = torch.zeros(1, x.size(1), self.hidden_size)\n",
    "        \n",
    "        out, _ = self.qlstm(x, (h0, c0))\n",
    "        out = self.predict(out[0])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111aebe4",
   "metadata": {},
   "source": [
    "## 3.1 train QModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc1d1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "def train_model(model, datas, batch_size, *, loss_func, optimizer, epoch = 50):\n",
    "    losses = []\n",
    "    sampler = RandomSampler(datas, num_samples = batch_size)\n",
    "    \n",
    "    for step in range(epoch):\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for index in sampler:\n",
    "            batch_x, batch_y = datas[index][0], datas[index][1]\n",
    "            b_x = batch_x.unsqueeze(0)\n",
    "            b_y = batch_y.unsqueeze(0)\n",
    "            \n",
    "            output = model(b_x)\n",
    "\n",
    "            loss = loss_func(output, b_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {step + 1}/{epoch}: Loss: {train_loss / batch_size}')\n",
    "        losses.append(train_loss / batch_size)\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef07934f",
   "metadata": {},
   "source": [
    "## 3.2 Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57e9b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_E(actual, predict):\n",
    "    E = actual[0] - predict[0]\n",
    "    E = torch.norm(Tensor(E))\n",
    "    E /= torch.norm(Tensor(predict))\n",
    "    \n",
    "    return E\n",
    "\n",
    "def calculate_accuarcy(model, X_test, y_test, scaler=test_scaler):\n",
    "    n = len(X_test)\n",
    "    \n",
    "    err = 0.0\n",
    "    for i in range(0, n):\n",
    "        err += calculate_E(\n",
    "            scaler.inverse_transform(y_test[i:i + 1].data), # actual\n",
    "            scaler.inverse_transform(model(X_test[i:i + 1]).data) # predict\n",
    "        ) ** 2\n",
    "    err /= n\n",
    "    \n",
    "    return 1 - err ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6407f969",
   "metadata": {},
   "source": [
    "## 3.3 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "305c9471",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_size = 4\n",
    "window_size = features_size * 3 # \n",
    "predict_size = features_size # features\n",
    "\n",
    "input_size = window_size\n",
    "num_output = predict_size\n",
    "\n",
    "hidden_size = 32\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50472565",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: 1\n",
      "Epoch 1/100: Loss: 0.9653659224510193\n",
      "Epoch 2/100: Loss: 0.9860976874828339\n",
      "Epoch 3/100: Loss: 1.0342773169279098\n",
      "Epoch 4/100: Loss: 0.9852130055427551\n",
      "Epoch 5/100: Loss: 1.0431435078382492\n",
      "Epoch 6/100: Loss: 0.9986117154359817\n",
      "Epoch 7/100: Loss: 0.9991187006235123\n",
      "Epoch 8/100: Loss: 0.9971051305532456\n",
      "Epoch 9/100: Loss: 0.9963036000728607\n",
      "Epoch 10/100: Loss: 0.9852702498435975\n",
      "Epoch 11/100: Loss: 0.9820417940616608\n",
      "Epoch 12/100: Loss: 0.986913937330246\n",
      "Epoch 13/100: Loss: 0.9891015589237213\n",
      "Epoch 14/100: Loss: 0.9711294174194336\n",
      "Epoch 15/100: Loss: 0.953196108341217\n",
      "Epoch 16/100: Loss: 0.9497069984674453\n",
      "Epoch 17/100: Loss: 0.9473606824874878\n",
      "Epoch 18/100: Loss: 0.9578390717506409\n",
      "Epoch 19/100: Loss: 0.9459779143333436\n",
      "Epoch 20/100: Loss: 0.9326870411634445\n",
      "Epoch 21/100: Loss: 0.9241948336362839\n",
      "Epoch 22/100: Loss: 0.8902485281229019\n",
      "Epoch 23/100: Loss: 0.8965077430009842\n",
      "Epoch 24/100: Loss: 0.8556585907936096\n",
      "Epoch 25/100: Loss: 0.8432328671216964\n",
      "Epoch 26/100: Loss: 0.79947449862957\n",
      "Epoch 27/100: Loss: 0.8158625364303589\n",
      "Epoch 28/100: Loss: 0.8922662079334259\n",
      "Epoch 29/100: Loss: 0.7617879956960678\n",
      "Epoch 30/100: Loss: 0.7951800376176834\n",
      "Epoch 31/100: Loss: 0.7197043061256408\n",
      "Epoch 32/100: Loss: 0.7614011883735656\n",
      "Epoch 33/100: Loss: 0.7809384360909462\n",
      "Epoch 34/100: Loss: 0.6668085932731629\n",
      "Epoch 35/100: Loss: 0.7345993682742119\n",
      "Epoch 36/100: Loss: 0.740518468618393\n",
      "Epoch 37/100: Loss: 0.7132537260651588\n",
      "Epoch 38/100: Loss: 0.7026926964521408\n",
      "Epoch 39/100: Loss: 0.5199479073286056\n",
      "Epoch 40/100: Loss: 0.6202904090285302\n",
      "Epoch 41/100: Loss: 0.6084210216999054\n",
      "Epoch 42/100: Loss: 0.6717301487922669\n",
      "Epoch 43/100: Loss: 0.699797834455967\n",
      "Epoch 44/100: Loss: 0.6259122937917709\n",
      "Epoch 45/100: Loss: 0.651314777135849\n",
      "Epoch 46/100: Loss: 0.6370543032884598\n",
      "Epoch 47/100: Loss: 0.5454824402928352\n",
      "Epoch 48/100: Loss: 0.6027244925498962\n",
      "Epoch 49/100: Loss: 0.620429427921772\n",
      "Epoch 50/100: Loss: 0.6455712124705315\n",
      "Epoch 51/100: Loss: 0.5997390285134315\n",
      "Epoch 52/100: Loss: 0.628671619296074\n",
      "Epoch 53/100: Loss: 0.641397987306118\n",
      "Epoch 54/100: Loss: 0.5471982583403587\n",
      "Epoch 55/100: Loss: 0.5153785765171051\n",
      "Epoch 56/100: Loss: 0.5537191435694695\n",
      "Epoch 57/100: Loss: 0.5359898865222931\n",
      "Epoch 58/100: Loss: 0.5111879363656044\n",
      "Epoch 59/100: Loss: 0.46491100937128066\n",
      "Epoch 60/100: Loss: 0.5906341649591923\n",
      "Epoch 61/100: Loss: 0.5448759883642197\n",
      "Epoch 62/100: Loss: 0.44173055440187453\n",
      "Epoch 63/100: Loss: 0.5708012215793132\n",
      "Epoch 64/100: Loss: 0.51740493029356\n",
      "Epoch 65/100: Loss: 0.39937540590763093\n",
      "Epoch 66/100: Loss: 0.36888016760349274\n",
      "Epoch 67/100: Loss: 0.427547762542963\n",
      "Epoch 68/100: Loss: 0.45240583419799807\n",
      "Epoch 69/100: Loss: 0.4425719350576401\n",
      "Epoch 70/100: Loss: 0.37265004217624664\n",
      "Epoch 71/100: Loss: 0.48464184552431105\n",
      "Epoch 72/100: Loss: 0.49227514043450354\n",
      "Epoch 73/100: Loss: 0.26720629036426546\n",
      "Epoch 74/100: Loss: 0.4341683775186539\n",
      "Epoch 75/100: Loss: 0.46451702192425726\n",
      "Epoch 76/100: Loss: 0.34396186247467997\n",
      "Epoch 77/100: Loss: 0.40963890179991724\n",
      "Epoch 78/100: Loss: 0.46331226974725725\n",
      "Epoch 79/100: Loss: 0.3606023721396923\n",
      "Epoch 80/100: Loss: 0.3245388589799404\n",
      "Epoch 81/100: Loss: 0.2724707543849945\n",
      "Epoch 82/100: Loss: 0.2668050456792116\n",
      "Epoch 83/100: Loss: 0.20999727696180343\n",
      "Epoch 84/100: Loss: 0.29094332866370676\n",
      "Epoch 85/100: Loss: 0.37120621725916864\n",
      "Epoch 86/100: Loss: 0.31223118752241136\n",
      "Epoch 87/100: Loss: 0.3494686707854271\n",
      "Epoch 88/100: Loss: 0.38118745014071465\n",
      "Epoch 89/100: Loss: 0.30819769240915773\n",
      "Epoch 90/100: Loss: 0.3654803838580847\n",
      "Epoch 91/100: Loss: 0.38898455649614333\n",
      "Epoch 92/100: Loss: 0.3263109553605318\n",
      "Epoch 93/100: Loss: 0.3414458565413952\n",
      "Epoch 94/100: Loss: 0.1976849976927042\n",
      "Epoch 95/100: Loss: 0.27464576810598373\n",
      "Epoch 96/100: Loss: 0.24251398518681527\n",
      "Epoch 97/100: Loss: 0.20952575262635947\n",
      "Epoch 98/100: Loss: 0.22593270521610975\n",
      "Epoch 99/100: Loss: 0.22655491288751364\n",
      "Epoch 100/100: Loss: 0.3139961952343583\n",
      "time costs: 517.5941486358643\n",
      "--------------------\n",
      "training epoch: 2\n",
      "Epoch 1/100: Loss: 0.9582258373498916\n",
      "Epoch 2/100: Loss: 1.024128755927086\n",
      "Epoch 3/100: Loss: 1.0567007541656495\n",
      "Epoch 4/100: Loss: 0.978525561094284\n",
      "Epoch 5/100: Loss: 1.016318643093109\n",
      "Epoch 6/100: Loss: 1.0094064474105835\n",
      "Epoch 7/100: Loss: 1.0186460077762605\n",
      "Epoch 8/100: Loss: 1.0106614828109741\n",
      "Epoch 9/100: Loss: 0.9716385453939438\n",
      "Epoch 10/100: Loss: 0.9940695464611053\n",
      "Epoch 11/100: Loss: 0.9665799856185913\n",
      "Epoch 12/100: Loss: 1.003553655743599\n",
      "Epoch 13/100: Loss: 0.9688023746013641\n",
      "Epoch 14/100: Loss: 0.9666035115718842\n",
      "Epoch 15/100: Loss: 1.008012381196022\n",
      "Epoch 16/100: Loss: 1.0004476875066757\n",
      "Epoch 17/100: Loss: 0.9607738435268403\n",
      "Epoch 18/100: Loss: 0.9805712878704071\n",
      "Epoch 19/100: Loss: 0.9772543907165527\n",
      "Epoch 20/100: Loss: 0.9758228570222854\n",
      "Epoch 21/100: Loss: 0.9786000162363052\n",
      "Epoch 22/100: Loss: 0.9561533182859421\n",
      "Epoch 23/100: Loss: 0.9683368057012558\n",
      "Epoch 24/100: Loss: 0.9598281353712081\n",
      "Epoch 25/100: Loss: 0.9482173055410386\n",
      "Epoch 26/100: Loss: 0.9507732361555099\n",
      "Epoch 27/100: Loss: 0.9262952953577042\n",
      "Epoch 28/100: Loss: 0.9385578244924545\n",
      "Epoch 29/100: Loss: 0.9286511212587356\n",
      "Epoch 30/100: Loss: 0.9263817131519317\n",
      "Epoch 31/100: Loss: 0.9082211077213287\n",
      "Epoch 32/100: Loss: 0.8919185370206832\n",
      "Epoch 33/100: Loss: 0.9216092020273209\n",
      "Epoch 34/100: Loss: 0.8874333828687668\n",
      "Epoch 35/100: Loss: 0.8720008611679078\n",
      "Epoch 36/100: Loss: 0.8473111629486084\n",
      "Epoch 37/100: Loss: 0.8814710259437561\n",
      "Epoch 38/100: Loss: 0.8449647068977356\n",
      "Epoch 39/100: Loss: 0.8382406085729599\n",
      "Epoch 40/100: Loss: 0.8155757486820221\n",
      "Epoch 41/100: Loss: 0.7807631403207779\n",
      "Epoch 42/100: Loss: 0.7710608005523681\n",
      "Epoch 43/100: Loss: 0.8204494535923004\n",
      "Epoch 44/100: Loss: 0.7730298638343811\n",
      "Epoch 45/100: Loss: 0.7367987483739853\n",
      "Epoch 46/100: Loss: 0.7640247374773026\n",
      "Epoch 47/100: Loss: 0.7777313202619552\n",
      "Epoch 48/100: Loss: 0.7731966555118561\n",
      "Epoch 49/100: Loss: 0.7364572167396546\n",
      "Epoch 50/100: Loss: 0.6884119391441346\n",
      "Epoch 51/100: Loss: 0.7116478845477104\n",
      "Epoch 52/100: Loss: 0.7400373876094818\n",
      "Epoch 53/100: Loss: 0.6629319012165069\n",
      "Epoch 54/100: Loss: 0.6763534873723984\n",
      "Epoch 55/100: Loss: 0.7081014662981033\n",
      "Epoch 56/100: Loss: 0.6322065234184265\n",
      "Epoch 57/100: Loss: 0.5585514441132545\n",
      "Epoch 58/100: Loss: 0.6130276143550872\n",
      "Epoch 59/100: Loss: 0.6201099880039692\n",
      "Epoch 60/100: Loss: 0.5437931522727013\n",
      "Epoch 61/100: Loss: 0.5990914888679981\n",
      "Epoch 62/100: Loss: 0.6354093492031098\n",
      "Epoch 63/100: Loss: 0.5848734013736248\n",
      "Epoch 64/100: Loss: 0.5910243324935436\n",
      "Epoch 65/100: Loss: 0.5731100112199783\n",
      "Epoch 66/100: Loss: 0.5782107055187226\n",
      "Epoch 67/100: Loss: 0.5668909214437008\n",
      "Epoch 68/100: Loss: 0.5557443305850029\n",
      "Epoch 69/100: Loss: 0.528194933384657\n",
      "Epoch 70/100: Loss: 0.5330865159630775\n",
      "Epoch 71/100: Loss: 0.6246705323457717\n",
      "Epoch 72/100: Loss: 0.4992456674575806\n",
      "Epoch 73/100: Loss: 0.49357700943946836\n",
      "Epoch 74/100: Loss: 0.49231528416275977\n",
      "Epoch 75/100: Loss: 0.5431801754981279\n",
      "Epoch 76/100: Loss: 0.4326497346162796\n",
      "Epoch 77/100: Loss: 0.5689408004283905\n",
      "Epoch 78/100: Loss: 0.5124889753758908\n",
      "Epoch 79/100: Loss: 0.4694604206830263\n",
      "Epoch 80/100: Loss: 0.4707290556281805\n",
      "Epoch 81/100: Loss: 0.4238083578646183\n",
      "Epoch 82/100: Loss: 0.4800434295088053\n",
      "Epoch 83/100: Loss: 0.4577758613973856\n",
      "Epoch 84/100: Loss: 0.434179400280118\n",
      "Epoch 85/100: Loss: 0.3464387448504567\n",
      "Epoch 86/100: Loss: 0.4537569912150502\n",
      "Epoch 87/100: Loss: 0.4662695301696658\n",
      "Epoch 88/100: Loss: 0.4201624419540167\n",
      "Epoch 89/100: Loss: 0.4179797671735287\n",
      "Epoch 90/100: Loss: 0.4698003465309739\n",
      "Epoch 91/100: Loss: 0.33988075889647007\n",
      "Epoch 92/100: Loss: 0.3803431948646903\n",
      "Epoch 93/100: Loss: 0.3960875926539302\n",
      "Epoch 94/100: Loss: 0.37530496157705784\n",
      "Epoch 95/100: Loss: 0.28249337077140807\n",
      "Epoch 96/100: Loss: 0.47725381813943385\n",
      "Epoch 97/100: Loss: 0.45435826741158963\n",
      "Epoch 98/100: Loss: 0.3033996408805251\n",
      "Epoch 99/100: Loss: 0.4557951072230935\n",
      "Epoch 100/100: Loss: 0.2821438107639551\n",
      "time costs: 687.0889000892639\n",
      "--------------------\n",
      "training epoch: 3\n",
      "Epoch 1/100: Loss: 1.0371670693159103\n",
      "Epoch 2/100: Loss: 1.0232260912656783\n",
      "Epoch 3/100: Loss: 1.0186658680438996\n",
      "Epoch 4/100: Loss: 1.0065231263637542\n",
      "Epoch 5/100: Loss: 1.0013371288776398\n",
      "Epoch 6/100: Loss: 0.985650759935379\n",
      "Epoch 7/100: Loss: 0.990873709321022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: Loss: 0.9795180946588516\n",
      "Epoch 9/100: Loss: 0.9640720903873443\n",
      "Epoch 10/100: Loss: 0.9982296317815781\n",
      "Epoch 11/100: Loss: 0.9826251477003097\n",
      "Epoch 12/100: Loss: 0.9856144309043884\n",
      "Epoch 13/100: Loss: 0.9508419275283814\n",
      "Epoch 14/100: Loss: 0.9448765963315964\n",
      "Epoch 15/100: Loss: 0.9587258517742157\n",
      "Epoch 16/100: Loss: 0.9368462830781936\n",
      "Epoch 17/100: Loss: 0.9287066757678986\n",
      "Epoch 18/100: Loss: 0.9008324563503265\n",
      "Epoch 19/100: Loss: 0.9005699336528779\n",
      "Epoch 20/100: Loss: 0.881693747639656\n",
      "Epoch 21/100: Loss: 0.8629623144865036\n",
      "Epoch 22/100: Loss: 0.8752820730209351\n",
      "Epoch 23/100: Loss: 0.8246921390295029\n",
      "Epoch 24/100: Loss: 0.8255922228097916\n",
      "Epoch 25/100: Loss: 0.787739223241806\n",
      "Epoch 26/100: Loss: 0.7237590461969375\n",
      "Epoch 27/100: Loss: 0.7427086442708969\n",
      "Epoch 28/100: Loss: 0.7019092202186584\n",
      "Epoch 29/100: Loss: 0.7139377743005753\n",
      "Epoch 30/100: Loss: 0.6564094394445419\n",
      "Epoch 31/100: Loss: 0.6401114955544471\n",
      "Epoch 32/100: Loss: 0.6380437389016151\n",
      "Epoch 33/100: Loss: 0.5794229686260224\n",
      "Epoch 34/100: Loss: 0.5876984834671021\n",
      "Epoch 35/100: Loss: 0.5424842193722725\n",
      "Epoch 36/100: Loss: 0.5650618433952331\n",
      "Epoch 37/100: Loss: 0.538465216755867\n",
      "Epoch 38/100: Loss: 0.4864766985177994\n",
      "Epoch 39/100: Loss: 0.47557915151119234\n",
      "Epoch 40/100: Loss: 0.45983539074659346\n",
      "Epoch 41/100: Loss: 0.580197237432003\n",
      "Epoch 42/100: Loss: 0.33750541135668755\n",
      "Epoch 43/100: Loss: 0.5260113947093487\n",
      "Epoch 44/100: Loss: 0.39901396408677103\n",
      "Epoch 45/100: Loss: 0.3953938245773315\n",
      "Epoch 46/100: Loss: 0.3783799231052399\n",
      "Epoch 47/100: Loss: 0.3713907565921545\n",
      "Epoch 48/100: Loss: 0.2525999218225479\n",
      "Epoch 49/100: Loss: 0.371858505345881\n",
      "Epoch 50/100: Loss: 0.3310110468417406\n",
      "Epoch 51/100: Loss: 0.35413022749125955\n",
      "Epoch 52/100: Loss: 0.3448721271008253\n",
      "Epoch 53/100: Loss: 0.3362733919173479\n",
      "Epoch 54/100: Loss: 0.3259185966104269\n",
      "Epoch 55/100: Loss: 0.2234118239954114\n",
      "Epoch 56/100: Loss: 0.22870164811611177\n",
      "Epoch 57/100: Loss: 0.21321570673026144\n",
      "Epoch 58/100: Loss: 0.3405379616189748\n",
      "Epoch 59/100: Loss: 0.2877041805535555\n",
      "Epoch 60/100: Loss: 0.3260165659710765\n",
      "Epoch 61/100: Loss: 0.32387050744146106\n",
      "Epoch 62/100: Loss: 0.2602494908962399\n",
      "Epoch 63/100: Loss: 0.284334160736762\n",
      "Epoch 64/100: Loss: 0.23045492423698305\n",
      "Epoch 65/100: Loss: 0.35084133038762955\n",
      "Epoch 66/100: Loss: 0.3264916623942554\n",
      "Epoch 67/100: Loss: 0.16307785708922892\n",
      "Epoch 68/100: Loss: 0.2004536549327895\n",
      "Epoch 69/100: Loss: 0.2951735145354178\n",
      "Epoch 70/100: Loss: 0.20777533669024706\n",
      "Epoch 71/100: Loss: 0.1685278469696641\n",
      "Epoch 72/100: Loss: 0.1949637289653765\n",
      "Epoch 73/100: Loss: 0.26598540745435456\n",
      "Epoch 74/100: Loss: 0.3903762363595888\n",
      "Epoch 75/100: Loss: 0.1761097688693553\n",
      "Epoch 76/100: Loss: 0.2512868669931777\n",
      "Epoch 77/100: Loss: 0.3182315299287438\n",
      "Epoch 78/100: Loss: 0.25209009675309063\n",
      "Epoch 79/100: Loss: 0.2708807146176696\n",
      "Epoch 80/100: Loss: 0.2275814046151936\n",
      "Epoch 81/100: Loss: 0.20568869020789862\n",
      "Epoch 82/100: Loss: 0.21344184330664576\n",
      "Epoch 83/100: Loss: 0.21413097834447398\n",
      "Epoch 84/100: Loss: 0.14023684572894127\n",
      "Epoch 85/100: Loss: 0.1251301111187786\n",
      "Epoch 86/100: Loss: 0.11322136267699534\n",
      "Epoch 87/100: Loss: 0.10650703425199026\n",
      "Epoch 88/100: Loss: 0.23033671798184513\n",
      "Epoch 89/100: Loss: 0.12125088987813797\n",
      "Epoch 90/100: Loss: 0.18249839209602214\n",
      "Epoch 91/100: Loss: 0.175162531924434\n",
      "Epoch 92/100: Loss: 0.10521256309002638\n",
      "Epoch 93/100: Loss: 0.1130582859274\n",
      "Epoch 94/100: Loss: 0.10407525897026063\n",
      "Epoch 95/100: Loss: 0.07425829460698878\n",
      "Epoch 96/100: Loss: 0.09853523911297088\n",
      "Epoch 97/100: Loss: 0.06845797311980277\n",
      "Epoch 98/100: Loss: 0.0711245689264615\n",
      "Epoch 99/100: Loss: 0.07465463912085397\n",
      "Epoch 100/100: Loss: 0.069146601659304\n",
      "time costs: 718.8219017982483\n",
      "--------------------\n",
      "training epoch: 4\n",
      "Epoch 1/100: Loss: 1.015461155772209\n",
      "Epoch 2/100: Loss: 0.9983195155858994\n",
      "Epoch 3/100: Loss: 0.9796258509159088\n",
      "Epoch 4/100: Loss: 0.9783623963594437\n",
      "Epoch 5/100: Loss: 0.9825730919837952\n",
      "Epoch 6/100: Loss: 0.9881534308195115\n",
      "Epoch 7/100: Loss: 0.9742155879735946\n",
      "Epoch 8/100: Loss: 0.9621588528156281\n",
      "Epoch 9/100: Loss: 0.9241604328155517\n",
      "Epoch 10/100: Loss: 0.9624166369438172\n",
      "Epoch 11/100: Loss: 0.9369465589523316\n",
      "Epoch 12/100: Loss: 0.9393273949623108\n",
      "Epoch 13/100: Loss: 0.9120872735977172\n",
      "Epoch 14/100: Loss: 0.9041854768991471\n",
      "Epoch 15/100: Loss: 0.9265283495187759\n",
      "Epoch 16/100: Loss: 0.8529835015535354\n",
      "Epoch 17/100: Loss: 0.8616883665323257\n",
      "Epoch 18/100: Loss: 0.848320135474205\n",
      "Epoch 19/100: Loss: 0.8131441354751587\n",
      "Epoch 20/100: Loss: 0.7960495084524155\n",
      "Epoch 21/100: Loss: 0.766313225030899\n",
      "Epoch 22/100: Loss: 0.738566842675209\n",
      "Epoch 23/100: Loss: 0.7219631105661393\n",
      "Epoch 24/100: Loss: 0.6900505304336548\n",
      "Epoch 25/100: Loss: 0.6922481119632721\n",
      "Epoch 26/100: Loss: 0.6598376601934433\n",
      "Epoch 27/100: Loss: 0.6187255322933197\n",
      "Epoch 28/100: Loss: 0.5830713793635368\n",
      "Epoch 29/100: Loss: 0.6231383636593819\n",
      "Epoch 30/100: Loss: 0.6411548897624015\n",
      "Epoch 31/100: Loss: 0.5998896270990371\n",
      "Epoch 32/100: Loss: 0.591100612282753\n",
      "Epoch 33/100: Loss: 0.5765671357512474\n",
      "Epoch 34/100: Loss: 0.5356650203466415\n",
      "Epoch 35/100: Loss: 0.571651665866375\n",
      "Epoch 36/100: Loss: 0.5230471804738045\n",
      "Epoch 37/100: Loss: 0.4801079332828522\n",
      "Epoch 38/100: Loss: 0.5067768529057503\n",
      "Epoch 39/100: Loss: 0.5021774590015411\n",
      "Epoch 40/100: Loss: 0.49290931075811384\n",
      "Epoch 41/100: Loss: 0.46559746414422987\n",
      "Epoch 42/100: Loss: 0.47991773933172227\n",
      "Epoch 43/100: Loss: 0.405320031195879\n",
      "Epoch 44/100: Loss: 0.4550770938396454\n",
      "Epoch 45/100: Loss: 0.40532197877764703\n",
      "Epoch 46/100: Loss: 0.4153383284807205\n",
      "Epoch 47/100: Loss: 0.33394493386149404\n",
      "Epoch 48/100: Loss: 0.34907073751091955\n",
      "Epoch 49/100: Loss: 0.40064211562275887\n",
      "Epoch 50/100: Loss: 0.36658052206039426\n",
      "Epoch 51/100: Loss: 0.3141741298139095\n",
      "Epoch 52/100: Loss: 0.3220845632255077\n",
      "Epoch 53/100: Loss: 0.3000106692314148\n",
      "Epoch 54/100: Loss: 0.2647195875644684\n",
      "Epoch 55/100: Loss: 0.24465728998184205\n",
      "Epoch 56/100: Loss: 0.2767224833369255\n",
      "Epoch 57/100: Loss: 0.21722387596964837\n",
      "Epoch 58/100: Loss: 0.20478927865624427\n",
      "Epoch 59/100: Loss: 0.18216799721121787\n",
      "Epoch 60/100: Loss: 0.1935483179986477\n",
      "Epoch 61/100: Loss: 0.16407303065061568\n",
      "Epoch 62/100: Loss: 0.15393154248595237\n",
      "Epoch 63/100: Loss: 0.1391438102349639\n",
      "Epoch 64/100: Loss: 0.1551764566451311\n",
      "Epoch 65/100: Loss: 0.13470828123390674\n",
      "Epoch 66/100: Loss: 0.12271203976124526\n",
      "Epoch 67/100: Loss: 0.09421826638281346\n",
      "Epoch 68/100: Loss: 0.10470134243369103\n",
      "Epoch 69/100: Loss: 0.08139313738793134\n",
      "Epoch 70/100: Loss: 0.08466236647218466\n",
      "Epoch 71/100: Loss: 0.08382904510945081\n",
      "Epoch 72/100: Loss: 0.06841259053908288\n",
      "Epoch 73/100: Loss: 0.05939592970535159\n",
      "Epoch 74/100: Loss: 0.05039493897929788\n",
      "Epoch 75/100: Loss: 0.05316409952938557\n",
      "Epoch 76/100: Loss: 0.04550954019650817\n",
      "Epoch 77/100: Loss: 0.03461747253313661\n",
      "Epoch 78/100: Loss: 0.03702816255390644\n",
      "Epoch 79/100: Loss: 0.024559895880520342\n",
      "Epoch 80/100: Loss: 0.022765898052603005\n",
      "Epoch 81/100: Loss: 0.02742577912285924\n",
      "Epoch 82/100: Loss: 0.01576277583371848\n",
      "Epoch 83/100: Loss: 0.01887767962180078\n",
      "Epoch 84/100: Loss: 0.016513254167512058\n",
      "Epoch 85/100: Loss: 0.011363984667696059\n",
      "Epoch 86/100: Loss: 0.010619621793739497\n",
      "Epoch 87/100: Loss: 0.008226046641357242\n",
      "Epoch 88/100: Loss: 0.007486170448828489\n",
      "Epoch 89/100: Loss: 0.006099332717712969\n",
      "Epoch 90/100: Loss: 0.005512038653250784\n",
      "Epoch 91/100: Loss: 0.006058889231644571\n",
      "Epoch 92/100: Loss: 0.0033121892833150924\n",
      "Epoch 93/100: Loss: 0.003770802589133382\n",
      "Epoch 94/100: Loss: 0.0029120302584487944\n",
      "Epoch 95/100: Loss: 0.002226303386851214\n",
      "Epoch 96/100: Loss: 0.0023102663166355343\n",
      "Epoch 97/100: Loss: 0.001973536501463968\n",
      "Epoch 98/100: Loss: 0.0016312879975885152\n",
      "Epoch 99/100: Loss: 0.00172629117150791\n",
      "Epoch 100/100: Loss: 0.0010391125309979544\n",
      "time costs: 720.9197895526886\n",
      "--------------------\n",
      "training epoch: 5\n",
      "Epoch 1/100: Loss: 1.0444636970758439\n",
      "Epoch 2/100: Loss: 1.0092395812273025\n",
      "Epoch 3/100: Loss: 1.0087563455104829\n",
      "Epoch 4/100: Loss: 1.022088348865509\n",
      "Epoch 5/100: Loss: 1.0231054991483688\n",
      "Epoch 6/100: Loss: 1.0053321897983551\n",
      "Epoch 7/100: Loss: 0.9978640049695968\n",
      "Epoch 8/100: Loss: 0.9996124833822251\n",
      "Epoch 9/100: Loss: 1.001246729493141\n",
      "Epoch 10/100: Loss: 0.987063854932785\n",
      "Epoch 11/100: Loss: 0.9798114836215973\n",
      "Epoch 12/100: Loss: 0.9984227806329727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: Loss: 0.9862931132316589\n",
      "Epoch 14/100: Loss: 0.9668026715517044\n",
      "Epoch 15/100: Loss: 0.9915644317865372\n",
      "Epoch 16/100: Loss: 0.9594405144453049\n",
      "Epoch 17/100: Loss: 0.973014059662819\n",
      "Epoch 18/100: Loss: 0.9648343056440354\n",
      "Epoch 19/100: Loss: 0.9287772715091706\n",
      "Epoch 20/100: Loss: 0.9581995487213135\n",
      "Epoch 21/100: Loss: 0.9325398623943328\n",
      "Epoch 22/100: Loss: 0.933443546295166\n",
      "Epoch 23/100: Loss: 0.9260393917560578\n",
      "Epoch 24/100: Loss: 0.8695650964975357\n",
      "Epoch 25/100: Loss: 0.9167714208364487\n",
      "Epoch 26/100: Loss: 0.9685019999742508\n",
      "Epoch 27/100: Loss: 0.8714360952377319\n",
      "Epoch 28/100: Loss: 0.8782244801521302\n",
      "Epoch 29/100: Loss: 0.8825990110635757\n",
      "Epoch 30/100: Loss: 0.7879328072071076\n",
      "Epoch 31/100: Loss: 0.8218895196914673\n",
      "Epoch 32/100: Loss: 0.8165373891592026\n",
      "Epoch 33/100: Loss: 0.8044213205575943\n",
      "Epoch 34/100: Loss: 0.7997868567705154\n",
      "Epoch 35/100: Loss: 0.7407412827014923\n",
      "Epoch 36/100: Loss: 0.7213975101709366\n",
      "Epoch 37/100: Loss: 0.6258938133716583\n",
      "Epoch 38/100: Loss: 0.6662978410720826\n",
      "Epoch 39/100: Loss: 0.7492280244827271\n",
      "Epoch 40/100: Loss: 0.7611802831292153\n",
      "Epoch 41/100: Loss: 0.669206939637661\n",
      "Epoch 42/100: Loss: 0.6484086334705352\n",
      "Epoch 43/100: Loss: 0.6754559203982353\n",
      "Epoch 44/100: Loss: 0.6556136101484299\n",
      "Epoch 45/100: Loss: 0.6804336726665496\n",
      "Epoch 46/100: Loss: 0.6648646026849747\n",
      "Epoch 47/100: Loss: 0.5682221174240112\n",
      "Epoch 48/100: Loss: 0.7615249991416931\n",
      "Epoch 49/100: Loss: 0.5902392536401748\n",
      "Epoch 50/100: Loss: 0.7158703982830048\n",
      "Epoch 51/100: Loss: 0.61260576993227\n",
      "Epoch 52/100: Loss: 0.551294881105423\n",
      "Epoch 53/100: Loss: 0.6910900294780731\n",
      "Epoch 54/100: Loss: 0.5904943853616714\n",
      "Epoch 55/100: Loss: 0.6283237263560295\n",
      "Epoch 56/100: Loss: 0.5596143305301666\n",
      "Epoch 57/100: Loss: 0.6627157524228096\n",
      "Epoch 58/100: Loss: 0.5576044350862503\n",
      "Epoch 59/100: Loss: 0.5266463920474053\n",
      "Epoch 60/100: Loss: 0.5402366653084755\n",
      "Epoch 61/100: Loss: 0.5747431367635727\n",
      "Epoch 62/100: Loss: 0.6332843214273453\n",
      "Epoch 63/100: Loss: 0.6306601598858833\n",
      "Epoch 64/100: Loss: 0.5993786290287971\n",
      "Epoch 65/100: Loss: 0.6000634670257569\n",
      "Epoch 66/100: Loss: 0.5759679257869721\n",
      "Epoch 67/100: Loss: 0.670351491868496\n",
      "Epoch 68/100: Loss: 0.6156988471746445\n",
      "Epoch 69/100: Loss: 0.5566980689764023\n",
      "Epoch 70/100: Loss: 0.5827308818697929\n",
      "Epoch 71/100: Loss: 0.6150964990258216\n",
      "Epoch 72/100: Loss: 0.6051388591527939\n",
      "Epoch 73/100: Loss: 0.5332469373941422\n",
      "Epoch 74/100: Loss: 0.5309311076998711\n",
      "Epoch 75/100: Loss: 0.491927008330822\n",
      "Epoch 76/100: Loss: 0.5795941635966301\n",
      "Epoch 77/100: Loss: 0.5353419661521912\n",
      "Epoch 78/100: Loss: 0.5058385357260704\n",
      "Epoch 79/100: Loss: 0.6004902392625808\n",
      "Epoch 80/100: Loss: 0.5420777425169945\n",
      "Epoch 81/100: Loss: 0.5414822205901146\n",
      "Epoch 82/100: Loss: 0.485389643907547\n",
      "Epoch 83/100: Loss: 0.5208246409893036\n",
      "Epoch 84/100: Loss: 0.46004255414009093\n",
      "Epoch 85/100: Loss: 0.4094417437911034\n",
      "Epoch 86/100: Loss: 0.5160589724779129\n",
      "Epoch 87/100: Loss: 0.4979816600680351\n",
      "Epoch 88/100: Loss: 0.4505191780626774\n",
      "Epoch 89/100: Loss: 0.4384933769702911\n",
      "Epoch 90/100: Loss: 0.4864772841334343\n",
      "Epoch 91/100: Loss: 0.4372639998793602\n",
      "Epoch 92/100: Loss: 0.38751357793807983\n",
      "Epoch 93/100: Loss: 0.4085759311914444\n",
      "Epoch 94/100: Loss: 0.35294839069247247\n",
      "Epoch 95/100: Loss: 0.3388798594474792\n",
      "Epoch 96/100: Loss: 0.33398139737546445\n",
      "Epoch 97/100: Loss: 0.35928749553859235\n",
      "Epoch 98/100: Loss: 0.3499547202140093\n",
      "Epoch 99/100: Loss: 0.35712812542915345\n",
      "Epoch 100/100: Loss: 0.323678957670927\n",
      "time costs: 720.3996584415436\n",
      "--------------------\n",
      "training epoch: 6\n",
      "Epoch 1/100: Loss: 1.023473286628723\n",
      "Epoch 2/100: Loss: 1.0147756516933442\n",
      "Epoch 3/100: Loss: 0.9954357504844665\n",
      "Epoch 4/100: Loss: 1.0063866138458253\n",
      "Epoch 5/100: Loss: 0.9930793106555938\n",
      "Epoch 6/100: Loss: 0.9877350151538848\n",
      "Epoch 7/100: Loss: 0.9763910591602325\n",
      "Epoch 8/100: Loss: 0.991731870174408\n",
      "Epoch 9/100: Loss: 0.9721560329198837\n",
      "Epoch 10/100: Loss: 0.9908344089984894\n",
      "Epoch 11/100: Loss: 0.9982190668582916\n",
      "Epoch 12/100: Loss: 0.9789651423692703\n",
      "Epoch 13/100: Loss: 0.966272959113121\n",
      "Epoch 14/100: Loss: 0.9753476917743683\n",
      "Epoch 15/100: Loss: 0.9696784555912018\n",
      "Epoch 16/100: Loss: 0.970350307226181\n",
      "Epoch 17/100: Loss: 0.9420743376016617\n",
      "Epoch 18/100: Loss: 0.9502991884946823\n",
      "Epoch 19/100: Loss: 0.9333524197340012\n",
      "Epoch 20/100: Loss: 0.9320973366498947\n",
      "Epoch 21/100: Loss: 0.8992989093065262\n",
      "Epoch 22/100: Loss: 0.9388429403305054\n",
      "Epoch 23/100: Loss: 0.8711418300867081\n",
      "Epoch 24/100: Loss: 0.925540542602539\n",
      "Epoch 25/100: Loss: 0.8804488629102707\n",
      "Epoch 26/100: Loss: 0.865020215511322\n",
      "Epoch 27/100: Loss: 0.8630674868822098\n",
      "Epoch 28/100: Loss: 0.8704280644655228\n",
      "Epoch 29/100: Loss: 0.7982811152935028\n",
      "Epoch 30/100: Loss: 0.8025477230548859\n",
      "Epoch 31/100: Loss: 0.7626694083213806\n",
      "Epoch 32/100: Loss: 0.7618792682886124\n",
      "Epoch 33/100: Loss: 0.6746900856494904\n",
      "Epoch 34/100: Loss: 0.6852486193180084\n",
      "Epoch 35/100: Loss: 0.5765713602304459\n",
      "Epoch 36/100: Loss: 0.7399233222007752\n",
      "Epoch 37/100: Loss: 0.6306403279304504\n",
      "Epoch 38/100: Loss: 0.6874300122261048\n",
      "Epoch 39/100: Loss: 0.5652818560600281\n",
      "Epoch 40/100: Loss: 0.5529987916350365\n",
      "Epoch 41/100: Loss: 0.553557176887989\n",
      "Epoch 42/100: Loss: 0.6200499385595322\n",
      "Epoch 43/100: Loss: 0.5858051121234894\n",
      "Epoch 44/100: Loss: 0.4993882432579994\n",
      "Epoch 45/100: Loss: 0.5711353585124016\n",
      "Epoch 46/100: Loss: 0.524195209145546\n",
      "Epoch 47/100: Loss: 0.4629343837499619\n",
      "Epoch 48/100: Loss: 0.4569279685616493\n",
      "Epoch 49/100: Loss: 0.4568104431033134\n",
      "Epoch 50/100: Loss: 0.3290919505059719\n",
      "Epoch 51/100: Loss: 0.44140982031822207\n",
      "Epoch 52/100: Loss: 0.4248754471540451\n",
      "Epoch 53/100: Loss: 0.430588548630476\n",
      "Epoch 54/100: Loss: 0.3711529284715652\n",
      "Epoch 55/100: Loss: 0.4358874559402466\n",
      "Epoch 56/100: Loss: 0.335770258307457\n",
      "Epoch 57/100: Loss: 0.3812312580645084\n",
      "Epoch 58/100: Loss: 0.3495628297328949\n",
      "Epoch 59/100: Loss: 0.3203865632414818\n",
      "Epoch 60/100: Loss: 0.41910678818821906\n",
      "Epoch 61/100: Loss: 0.3161190189421177\n",
      "Epoch 62/100: Loss: 0.2508593313395977\n",
      "Epoch 63/100: Loss: 0.41127749606966973\n",
      "Epoch 64/100: Loss: 0.3583419606089592\n",
      "Epoch 65/100: Loss: 0.33596119806170466\n",
      "Epoch 66/100: Loss: 0.2867650344967842\n",
      "Epoch 67/100: Loss: 0.34572754688560964\n",
      "Epoch 68/100: Loss: 0.23741918727755545\n",
      "Epoch 69/100: Loss: 0.2512586034834385\n",
      "Epoch 70/100: Loss: 0.3536863211542368\n",
      "Epoch 71/100: Loss: 0.25028303973376753\n",
      "Epoch 72/100: Loss: 0.36519929468631745\n",
      "Epoch 73/100: Loss: 0.36141423378139736\n",
      "Epoch 74/100: Loss: 0.20325972195714712\n",
      "Epoch 75/100: Loss: 0.33797849994152784\n",
      "Epoch 76/100: Loss: 0.30200940985232594\n",
      "Epoch 77/100: Loss: 0.3284075750038028\n",
      "Epoch 78/100: Loss: 0.29678002670407294\n",
      "Epoch 79/100: Loss: 0.3079122737050056\n",
      "Epoch 80/100: Loss: 0.3231234775856137\n",
      "Epoch 81/100: Loss: 0.26342620234936476\n",
      "Epoch 82/100: Loss: 0.28264748584479094\n",
      "Epoch 83/100: Loss: 0.27131993416696787\n",
      "Epoch 84/100: Loss: 0.23998035117983818\n",
      "Epoch 85/100: Loss: 0.21456257496029138\n",
      "Epoch 86/100: Loss: 0.2663104470819235\n",
      "Epoch 87/100: Loss: 0.2901582868769765\n",
      "Epoch 88/100: Loss: 0.23341101314872503\n",
      "Epoch 89/100: Loss: 0.2083379054442048\n",
      "Epoch 90/100: Loss: 0.25143841803073885\n",
      "Epoch 91/100: Loss: 0.2702659863978624\n",
      "Epoch 92/100: Loss: 0.19061701763421296\n",
      "Epoch 93/100: Loss: 0.21880895365029573\n",
      "Epoch 94/100: Loss: 0.21403691237792372\n",
      "Epoch 95/100: Loss: 0.2548737281002104\n",
      "Epoch 96/100: Loss: 0.26142096938565373\n",
      "Epoch 97/100: Loss: 0.20886317528784276\n",
      "Epoch 98/100: Loss: 0.24048612508922815\n",
      "Epoch 99/100: Loss: 0.2971105401404202\n",
      "Epoch 100/100: Loss: 0.2371255866717547\n",
      "time costs: 718.5845718383789\n",
      "--------------------\n",
      "training epoch: 7\n",
      "Epoch 1/100: Loss: 1.0481232136487961\n",
      "Epoch 2/100: Loss: 1.0762329965829849\n",
      "Epoch 3/100: Loss: 1.0069972068071364\n",
      "Epoch 4/100: Loss: 0.9892978578805923\n",
      "Epoch 5/100: Loss: 1.0333472818136216\n",
      "Epoch 6/100: Loss: 0.9924368292093277\n",
      "Epoch 7/100: Loss: 0.9812028527259826\n",
      "Epoch 8/100: Loss: 1.0243605136871339\n",
      "Epoch 9/100: Loss: 0.9655930936336518\n",
      "Epoch 10/100: Loss: 0.9613016337156296\n",
      "Epoch 11/100: Loss: 1.0158959776163101\n",
      "Epoch 12/100: Loss: 0.9672192394733429\n",
      "Epoch 13/100: Loss: 0.9734417349100113\n",
      "Epoch 14/100: Loss: 0.9668815672397614\n",
      "Epoch 15/100: Loss: 0.9615120112895965\n",
      "Epoch 16/100: Loss: 0.9588339269161225\n",
      "Epoch 17/100: Loss: 0.9376796901226043\n",
      "Epoch 18/100: Loss: 0.9220403492450714\n",
      "Epoch 19/100: Loss: 0.9407875210046768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: Loss: 0.9156384915113449\n",
      "Epoch 21/100: Loss: 0.8909336924552917\n",
      "Epoch 22/100: Loss: 0.8697272211313247\n",
      "Epoch 23/100: Loss: 0.8681740909814835\n",
      "Epoch 24/100: Loss: 0.8516983598470688\n",
      "Epoch 25/100: Loss: 0.8259332478046417\n",
      "Epoch 26/100: Loss: 0.8062167733907699\n",
      "Epoch 27/100: Loss: 0.788256648182869\n",
      "Epoch 28/100: Loss: 0.7613367974758148\n",
      "Epoch 29/100: Loss: 0.7602924138307572\n",
      "Epoch 30/100: Loss: 0.7154940843582154\n",
      "Epoch 31/100: Loss: 0.6800560265779495\n",
      "Epoch 32/100: Loss: 0.644211721420288\n",
      "Epoch 33/100: Loss: 0.6129444405436516\n",
      "Epoch 34/100: Loss: 0.6471734523773194\n",
      "Epoch 35/100: Loss: 0.595687310397625\n",
      "Epoch 36/100: Loss: 0.5252812385559082\n",
      "Epoch 37/100: Loss: 0.519928154349327\n",
      "Epoch 38/100: Loss: 0.49599552154541016\n",
      "Epoch 39/100: Loss: 0.5306808724999428\n",
      "Epoch 40/100: Loss: 0.4903876259922981\n",
      "Epoch 41/100: Loss: 0.4312795802950859\n",
      "Epoch 42/100: Loss: 0.5252118542790413\n",
      "Epoch 43/100: Loss: 0.39464082643389703\n",
      "Epoch 44/100: Loss: 0.4194123759865761\n",
      "Epoch 45/100: Loss: 0.41233187466859816\n",
      "Epoch 46/100: Loss: 0.37540605552494527\n",
      "Epoch 47/100: Loss: 0.423087701946497\n",
      "Epoch 48/100: Loss: 0.4430159918963909\n",
      "Epoch 49/100: Loss: 0.4081431902945042\n",
      "Epoch 50/100: Loss: 0.35678373724222184\n",
      "Epoch 51/100: Loss: 0.35332057885825635\n",
      "Epoch 52/100: Loss: 0.40190250128507615\n",
      "Epoch 53/100: Loss: 0.39356380328536034\n",
      "Epoch 54/100: Loss: 0.3722451634705067\n",
      "Epoch 55/100: Loss: 0.3424874596297741\n",
      "Epoch 56/100: Loss: 0.3288372315466404\n",
      "Epoch 57/100: Loss: 0.30753816589713096\n",
      "Epoch 58/100: Loss: 0.3418689616024494\n",
      "Epoch 59/100: Loss: 0.3319150537252426\n",
      "Epoch 60/100: Loss: 0.3420024558901787\n",
      "Epoch 61/100: Loss: 0.2952648438513279\n",
      "Epoch 62/100: Loss: 0.32643223032355306\n",
      "Epoch 63/100: Loss: 0.28940249904990195\n",
      "Epoch 64/100: Loss: 0.2653362341225147\n",
      "Epoch 65/100: Loss: 0.3019438087940216\n",
      "Epoch 66/100: Loss: 0.2556155748665333\n",
      "Epoch 67/100: Loss: 0.30385406613349913\n",
      "Epoch 68/100: Loss: 0.29377573579549787\n",
      "Epoch 69/100: Loss: 0.3169342286884785\n",
      "Epoch 70/100: Loss: 0.3320285931229591\n",
      "Epoch 71/100: Loss: 0.28792761638760567\n",
      "Epoch 72/100: Loss: 0.2773453459143639\n",
      "Epoch 73/100: Loss: 0.284672062844038\n",
      "Epoch 74/100: Loss: 0.2580610327422619\n",
      "Epoch 75/100: Loss: 0.2918131574988365\n",
      "Epoch 76/100: Loss: 0.28944080471992495\n",
      "Epoch 77/100: Loss: 0.26463890001177787\n",
      "Epoch 78/100: Loss: 0.2637645587325096\n",
      "Epoch 79/100: Loss: 0.2314216673374176\n",
      "Epoch 80/100: Loss: 0.2636215202510357\n",
      "Epoch 81/100: Loss: 0.265352101624012\n",
      "Epoch 82/100: Loss: 0.2300983965396881\n",
      "Epoch 83/100: Loss: 0.2585554964840412\n",
      "Epoch 84/100: Loss: 0.23679309263825415\n",
      "Epoch 85/100: Loss: 0.20101042613387107\n",
      "Epoch 86/100: Loss: 0.2689919300377369\n",
      "Epoch 87/100: Loss: 0.24895897284150123\n",
      "Epoch 88/100: Loss: 0.25876144841313364\n",
      "Epoch 89/100: Loss: 0.2361449755728245\n",
      "Epoch 90/100: Loss: 0.2253331072628498\n",
      "Epoch 91/100: Loss: 0.20489233620464803\n",
      "Epoch 92/100: Loss: 0.22235715612769127\n",
      "Epoch 93/100: Loss: 0.22072666622698306\n",
      "Epoch 94/100: Loss: 0.2069767214357853\n",
      "Epoch 95/100: Loss: 0.22502900883555413\n",
      "Epoch 96/100: Loss: 0.18650955557823182\n",
      "Epoch 97/100: Loss: 0.2003028191626072\n",
      "Epoch 98/100: Loss: 0.21366338096559048\n",
      "Epoch 99/100: Loss: 0.19428830407559872\n",
      "Epoch 100/100: Loss: 0.22620858065783978\n",
      "time costs: 720.1282196044922\n",
      "--------------------\n",
      "training epoch: 8\n",
      "Epoch 1/100: Loss: 1.0430231839418411\n",
      "Epoch 2/100: Loss: 1.0186307907104493\n",
      "Epoch 3/100: Loss: 1.0396456003189087\n",
      "Epoch 4/100: Loss: 0.9977272570133209\n",
      "Epoch 5/100: Loss: 1.011339572072029\n",
      "Epoch 6/100: Loss: 0.9938390672206878\n",
      "Epoch 7/100: Loss: 0.9864219903945923\n",
      "Epoch 8/100: Loss: 0.9549615919589997\n",
      "Epoch 9/100: Loss: 0.9471017390489578\n",
      "Epoch 10/100: Loss: 0.9432822048664093\n",
      "Epoch 11/100: Loss: 1.0160882264375686\n",
      "Epoch 12/100: Loss: 0.9536626160144805\n",
      "Epoch 13/100: Loss: 0.9567108422517776\n",
      "Epoch 14/100: Loss: 0.9314890265464782\n",
      "Epoch 15/100: Loss: 0.9164900481700897\n",
      "Epoch 16/100: Loss: 0.933765497803688\n",
      "Epoch 17/100: Loss: 0.9388614773750306\n",
      "Epoch 18/100: Loss: 0.8533034205436707\n",
      "Epoch 19/100: Loss: 0.8945387661457062\n",
      "Epoch 20/100: Loss: 0.8708249628543854\n",
      "Epoch 21/100: Loss: 0.8709634989500046\n",
      "Epoch 22/100: Loss: 0.8335545390844346\n",
      "Epoch 23/100: Loss: 0.8225799202919006\n",
      "Epoch 24/100: Loss: 0.8398097068071365\n",
      "Epoch 25/100: Loss: 0.8072656929492951\n",
      "Epoch 26/100: Loss: 0.7155766814947129\n",
      "Epoch 27/100: Loss: 0.7631358996033668\n",
      "Epoch 28/100: Loss: 0.6869766369462014\n",
      "Epoch 29/100: Loss: 0.6990710586309433\n",
      "Epoch 30/100: Loss: 0.6591822355985641\n",
      "Epoch 31/100: Loss: 0.7692030891776085\n",
      "Epoch 32/100: Loss: 0.7104787781834603\n",
      "Epoch 33/100: Loss: 0.6587746024131775\n",
      "Epoch 34/100: Loss: 0.4879178315401077\n",
      "Epoch 35/100: Loss: 0.596536985039711\n",
      "Epoch 36/100: Loss: 0.6352594658732414\n",
      "Epoch 37/100: Loss: 0.5630775839090347\n",
      "Epoch 38/100: Loss: 0.4714515909552574\n",
      "Epoch 39/100: Loss: 0.4153030261397362\n",
      "Epoch 40/100: Loss: 0.497115520387888\n",
      "Epoch 41/100: Loss: 0.5387306414544583\n",
      "Epoch 42/100: Loss: 0.2660753771662712\n",
      "Epoch 43/100: Loss: 0.461107387393713\n",
      "Epoch 44/100: Loss: 0.4029682159423828\n",
      "Epoch 45/100: Loss: 0.44456963390111925\n",
      "Epoch 46/100: Loss: 0.4124369964003563\n",
      "Epoch 47/100: Loss: 0.4921069838106632\n",
      "Epoch 48/100: Loss: 0.39190038442611697\n",
      "Epoch 49/100: Loss: 0.29204747527837754\n",
      "Epoch 50/100: Loss: 0.49731912836432457\n",
      "Epoch 51/100: Loss: 0.2741592418402433\n",
      "Epoch 52/100: Loss: 0.32510845325887205\n",
      "Epoch 53/100: Loss: 0.3501365166157484\n",
      "Epoch 54/100: Loss: 0.33433005921542647\n",
      "Epoch 55/100: Loss: 0.31375095769762995\n",
      "Epoch 56/100: Loss: 0.26862045731395484\n",
      "Epoch 57/100: Loss: 0.20452541280537845\n",
      "Epoch 58/100: Loss: 0.18943827785551548\n",
      "Epoch 59/100: Loss: 0.30401442288421093\n",
      "Epoch 60/100: Loss: 0.3718814730644226\n",
      "Epoch 61/100: Loss: 0.1999965174123645\n",
      "Epoch 62/100: Loss: 0.289802358020097\n",
      "Epoch 63/100: Loss: 0.23235449939966202\n",
      "Epoch 64/100: Loss: 0.3173466868698597\n",
      "Epoch 65/100: Loss: 0.22699803235009314\n",
      "Epoch 66/100: Loss: 0.2567149999551475\n",
      "Epoch 67/100: Loss: 0.30219850642606616\n",
      "Epoch 68/100: Loss: 0.22147944737225772\n",
      "Epoch 69/100: Loss: 0.19391957307234406\n",
      "Epoch 70/100: Loss: 0.1959676330909133\n",
      "Epoch 71/100: Loss: 0.23165562176145613\n",
      "Epoch 72/100: Loss: 0.20313888750970363\n",
      "Epoch 73/100: Loss: 0.1656474767718464\n",
      "Epoch 74/100: Loss: 0.34204325871542096\n",
      "Epoch 75/100: Loss: 0.24441107967868447\n",
      "Epoch 76/100: Loss: 0.20088841756805778\n",
      "Epoch 77/100: Loss: 0.19279629532247783\n",
      "Epoch 78/100: Loss: 0.3032236769795418\n",
      "Epoch 79/100: Loss: 0.16374418567866086\n",
      "Epoch 80/100: Loss: 0.18197213606908919\n",
      "Epoch 81/100: Loss: 0.287724697869271\n",
      "Epoch 82/100: Loss: 0.17692257883027196\n",
      "Epoch 83/100: Loss: 0.13120856918394566\n",
      "Epoch 84/100: Loss: 0.1416673816740513\n",
      "Epoch 85/100: Loss: 0.18714552633464338\n",
      "Epoch 86/100: Loss: 0.2516822615172714\n",
      "Epoch 87/100: Loss: 0.14471033960580826\n",
      "Epoch 88/100: Loss: 0.1674552781507373\n",
      "Epoch 89/100: Loss: 0.22540343711152672\n",
      "Epoch 90/100: Loss: 0.15358249731361867\n",
      "Epoch 91/100: Loss: 0.09373702360317111\n",
      "Epoch 92/100: Loss: 0.14315476400079205\n",
      "Epoch 93/100: Loss: 0.10391567933838815\n",
      "Epoch 94/100: Loss: 0.10528540265513583\n",
      "Epoch 95/100: Loss: 0.18890056710224598\n",
      "Epoch 96/100: Loss: 0.1356194826308638\n",
      "Epoch 97/100: Loss: 0.08508485881611705\n",
      "Epoch 98/100: Loss: 0.12142133455490693\n",
      "Epoch 99/100: Loss: 0.13899336266331375\n",
      "Epoch 100/100: Loss: 0.09344435723032803\n",
      "time costs: 731.4701611995697\n",
      "--------------------\n",
      "training epoch: 9\n",
      "Epoch 1/100: Loss: 1.001807725429535\n",
      "Epoch 2/100: Loss: 0.9998913675546646\n",
      "Epoch 3/100: Loss: 1.0210850894451142\n",
      "Epoch 4/100: Loss: 1.026769185066223\n",
      "Epoch 5/100: Loss: 1.009298825263977\n",
      "Epoch 6/100: Loss: 1.0003977179527284\n",
      "Epoch 7/100: Loss: 0.9881911784410476\n",
      "Epoch 8/100: Loss: 0.9803664267063141\n",
      "Epoch 9/100: Loss: 0.9966106444597245\n",
      "Epoch 10/100: Loss: 0.9799828648567199\n",
      "Epoch 11/100: Loss: 0.998256367444992\n",
      "Epoch 12/100: Loss: 0.9707820862531662\n",
      "Epoch 13/100: Loss: 0.9786032736301422\n",
      "Epoch 14/100: Loss: 0.9892622351646423\n",
      "Epoch 15/100: Loss: 0.9887873232364655\n",
      "Epoch 16/100: Loss: 0.9764655381441116\n",
      "Epoch 17/100: Loss: 0.9652455598115921\n",
      "Epoch 18/100: Loss: 0.9668755620718003\n",
      "Epoch 19/100: Loss: 0.9572444558143616\n",
      "Epoch 20/100: Loss: 0.9597051233053208\n",
      "Epoch 21/100: Loss: 0.9838384628295899\n",
      "Epoch 22/100: Loss: 0.9523203432559967\n",
      "Epoch 23/100: Loss: 0.957326865196228\n",
      "Epoch 24/100: Loss: 0.9748431295156479\n",
      "Epoch 25/100: Loss: 0.9286686837673187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: Loss: 0.9296365141868591\n",
      "Epoch 27/100: Loss: 0.9482493996620178\n",
      "Epoch 28/100: Loss: 0.9187966287136078\n",
      "Epoch 29/100: Loss: 0.9564121276140213\n",
      "Epoch 30/100: Loss: 0.9050131052732467\n",
      "Epoch 31/100: Loss: 0.9381346195936203\n",
      "Epoch 32/100: Loss: 0.9199914932250977\n",
      "Epoch 33/100: Loss: 0.9195729970932007\n",
      "Epoch 34/100: Loss: 0.895105391740799\n",
      "Epoch 35/100: Loss: 0.8971206963062286\n",
      "Epoch 36/100: Loss: 0.9034933716058731\n",
      "Epoch 37/100: Loss: 0.8947763949632644\n",
      "Epoch 38/100: Loss: 0.8987348854541779\n",
      "Epoch 39/100: Loss: 0.8672333896160126\n",
      "Epoch 40/100: Loss: 0.8817924052476883\n",
      "Epoch 41/100: Loss: 0.8520208388566971\n",
      "Epoch 42/100: Loss: 0.8521577656269074\n",
      "Epoch 43/100: Loss: 0.824180343747139\n",
      "Epoch 44/100: Loss: 0.8145592212677002\n",
      "Epoch 45/100: Loss: 0.8596517592668533\n",
      "Epoch 46/100: Loss: 0.8142196983098984\n",
      "Epoch 47/100: Loss: 0.7859239518642426\n",
      "Epoch 48/100: Loss: 0.7395644456148147\n",
      "Epoch 49/100: Loss: 0.7791801035404206\n",
      "Epoch 50/100: Loss: 0.6847646653652191\n",
      "Epoch 51/100: Loss: 0.7689659327268601\n",
      "Epoch 52/100: Loss: 0.7439814865589142\n",
      "Epoch 53/100: Loss: 0.7075594514608383\n",
      "Epoch 54/100: Loss: 0.7608172655105591\n",
      "Epoch 55/100: Loss: 0.6653298974037171\n",
      "Epoch 56/100: Loss: 0.7136454343795776\n",
      "Epoch 57/100: Loss: 0.6453438520431518\n",
      "Epoch 58/100: Loss: 0.5938741162419319\n",
      "Epoch 59/100: Loss: 0.6011225283145905\n",
      "Epoch 60/100: Loss: 0.5986578941345215\n",
      "Epoch 61/100: Loss: 0.5938502669334411\n",
      "Epoch 62/100: Loss: 0.5657232828438282\n",
      "Epoch 63/100: Loss: 0.6333083942532539\n",
      "Epoch 64/100: Loss: 0.4501505747437477\n",
      "Epoch 65/100: Loss: 0.5001502983272076\n",
      "Epoch 66/100: Loss: 0.5023046225309372\n",
      "Epoch 67/100: Loss: 0.5386653646826745\n",
      "Epoch 68/100: Loss: 0.533033162355423\n",
      "Epoch 69/100: Loss: 0.44583157226443293\n",
      "Epoch 70/100: Loss: 0.4627258189022541\n",
      "Epoch 71/100: Loss: 0.45235099643468857\n",
      "Epoch 72/100: Loss: 0.4892937257885933\n",
      "Epoch 73/100: Loss: 0.348716676235199\n",
      "Epoch 74/100: Loss: 0.31260415352880955\n",
      "Epoch 75/100: Loss: 0.4309352312237024\n",
      "Epoch 76/100: Loss: 0.40557309463620184\n",
      "Epoch 77/100: Loss: 0.2999602179974318\n",
      "Epoch 78/100: Loss: 0.3857870828360319\n",
      "Epoch 79/100: Loss: 0.32131377682089807\n",
      "Epoch 80/100: Loss: 0.32196745574474334\n",
      "Epoch 81/100: Loss: 0.2404072742909193\n",
      "Epoch 82/100: Loss: 0.25707514826208355\n",
      "Epoch 83/100: Loss: 0.3711357958614826\n",
      "Epoch 84/100: Loss: 0.33821282666176555\n",
      "Epoch 85/100: Loss: 0.253003541752696\n",
      "Epoch 86/100: Loss: 0.2694844491779804\n",
      "Epoch 87/100: Loss: 0.17708629723638297\n",
      "Epoch 88/100: Loss: 0.1965249389410019\n",
      "Epoch 89/100: Loss: 0.21701091695576907\n",
      "Epoch 90/100: Loss: 0.2505695315077901\n",
      "Epoch 91/100: Loss: 0.2935012985020876\n",
      "Epoch 92/100: Loss: 0.2632032850757241\n",
      "Epoch 93/100: Loss: 0.2621522853150964\n",
      "Epoch 94/100: Loss: 0.19828935749828816\n",
      "Epoch 95/100: Loss: 0.1472071262076497\n",
      "Epoch 96/100: Loss: 0.2094470419920981\n",
      "Epoch 97/100: Loss: 0.22739234380424023\n",
      "Epoch 98/100: Loss: 0.3060825541615486\n",
      "Epoch 99/100: Loss: 0.2011549562215805\n",
      "Epoch 100/100: Loss: 0.19744044579565526\n",
      "time costs: 691.3353614807129\n",
      "--------------------\n",
      "training epoch: 10\n",
      "Epoch 1/100: Loss: 1.0001922756433488\n",
      "Epoch 2/100: Loss: 0.9963554352521896\n",
      "Epoch 3/100: Loss: 1.0095563560724259\n",
      "Epoch 4/100: Loss: 0.987711688876152\n",
      "Epoch 5/100: Loss: 0.9736851751804352\n",
      "Epoch 6/100: Loss: 0.9715387552976609\n",
      "Epoch 7/100: Loss: 0.9644393026828766\n",
      "Epoch 8/100: Loss: 0.9830095589160919\n",
      "Epoch 9/100: Loss: 0.9625352770090103\n",
      "Epoch 10/100: Loss: 0.9465259671211242\n",
      "Epoch 11/100: Loss: 0.9253261357545852\n",
      "Epoch 12/100: Loss: 0.9233815252780915\n",
      "Epoch 13/100: Loss: 0.9062898099422455\n",
      "Epoch 14/100: Loss: 0.9217792510986328\n",
      "Epoch 15/100: Loss: 0.8829393416643143\n",
      "Epoch 16/100: Loss: 0.861610135436058\n",
      "Epoch 17/100: Loss: 0.8643870234489441\n",
      "Epoch 18/100: Loss: 0.8112038880586624\n",
      "Epoch 19/100: Loss: 0.791194424033165\n",
      "Epoch 20/100: Loss: 0.7471075683832169\n",
      "Epoch 21/100: Loss: 0.8151719987392425\n",
      "Epoch 22/100: Loss: 0.7458401054143906\n",
      "Epoch 23/100: Loss: 0.731453287601471\n",
      "Epoch 24/100: Loss: 0.674943083524704\n",
      "Epoch 25/100: Loss: 0.7350761160254479\n",
      "Epoch 26/100: Loss: 0.7135050982236862\n",
      "Epoch 27/100: Loss: 0.670476633310318\n",
      "Epoch 28/100: Loss: 0.6943822473287582\n",
      "Epoch 29/100: Loss: 0.6293691411614418\n",
      "Epoch 30/100: Loss: 0.6331579104065895\n",
      "Epoch 31/100: Loss: 0.5731617391109467\n",
      "Epoch 32/100: Loss: 0.6748473569750786\n",
      "Epoch 33/100: Loss: 0.6902010187506675\n",
      "Epoch 34/100: Loss: 0.5903291925787926\n",
      "Epoch 35/100: Loss: 0.5523403346538543\n",
      "Epoch 36/100: Loss: 0.5841694042086601\n",
      "Epoch 37/100: Loss: 0.5818211764097214\n",
      "Epoch 38/100: Loss: 0.534852409362793\n",
      "Epoch 39/100: Loss: 0.5633327245712281\n",
      "Epoch 40/100: Loss: 0.566374134272337\n",
      "Epoch 41/100: Loss: 0.5127945870161057\n",
      "Epoch 42/100: Loss: 0.5604771405458451\n",
      "Epoch 43/100: Loss: 0.5145661860704422\n",
      "Epoch 44/100: Loss: 0.53669119104743\n",
      "Epoch 45/100: Loss: 0.4889462284743786\n",
      "Epoch 46/100: Loss: 0.5146378755569458\n",
      "Epoch 47/100: Loss: 0.4842353671789169\n",
      "Epoch 48/100: Loss: 0.4558041751384735\n",
      "Epoch 49/100: Loss: 0.4213222339749336\n",
      "Epoch 50/100: Loss: 0.40619062185287474\n",
      "Epoch 51/100: Loss: 0.3745456084609032\n",
      "Epoch 52/100: Loss: 0.46208733804523944\n",
      "Epoch 53/100: Loss: 0.42429564967751504\n",
      "Epoch 54/100: Loss: 0.40505607426166534\n",
      "Epoch 55/100: Loss: 0.37731350362300875\n",
      "Epoch 56/100: Loss: 0.2840999409556389\n",
      "Epoch 57/100: Loss: 0.31547469943761824\n",
      "Epoch 58/100: Loss: 0.30786205381155013\n",
      "Epoch 59/100: Loss: 0.33096763119101524\n",
      "Epoch 60/100: Loss: 0.25445543266832826\n",
      "Epoch 61/100: Loss: 0.28091339841485025\n",
      "Epoch 62/100: Loss: 0.22860289961099625\n",
      "Epoch 63/100: Loss: 0.21138798221945762\n",
      "Epoch 64/100: Loss: 0.20908902697265147\n",
      "Epoch 65/100: Loss: 0.19533465728163718\n",
      "Epoch 66/100: Loss: 0.18082699310034514\n",
      "Epoch 67/100: Loss: 0.16507441494613886\n",
      "Epoch 68/100: Loss: 0.14219291638582945\n",
      "Epoch 69/100: Loss: 0.18037877660244703\n",
      "Epoch 70/100: Loss: 0.11050661187618971\n",
      "Epoch 71/100: Loss: 0.1232516624033451\n",
      "Epoch 72/100: Loss: 0.10403906013816595\n",
      "Epoch 73/100: Loss: 0.09918481875211001\n",
      "Epoch 74/100: Loss: 0.09210591269657016\n",
      "Epoch 75/100: Loss: 0.07167441146448254\n",
      "Epoch 76/100: Loss: 0.06999432533048093\n",
      "Epoch 77/100: Loss: 0.056420176103711125\n",
      "Epoch 78/100: Loss: 0.038306513847783205\n",
      "Epoch 79/100: Loss: 0.04961010784609243\n",
      "Epoch 80/100: Loss: 0.047604398196563126\n",
      "Epoch 81/100: Loss: 0.037250656459946185\n",
      "Epoch 82/100: Loss: 0.03196392540121451\n",
      "Epoch 83/100: Loss: 0.0209870251477696\n",
      "Epoch 84/100: Loss: 0.03714806270727422\n",
      "Epoch 85/100: Loss: 0.028112147157662547\n",
      "Epoch 86/100: Loss: 0.01631029712734744\n",
      "Epoch 87/100: Loss: 0.017678808455821128\n",
      "Epoch 88/100: Loss: 0.018412511417409405\n",
      "Epoch 89/100: Loss: 0.018409027223242447\n",
      "Epoch 90/100: Loss: 0.011967452069802676\n",
      "Epoch 91/100: Loss: 0.018143405046430416\n",
      "Epoch 92/100: Loss: 0.013909776555374264\n",
      "Epoch 93/100: Loss: 0.01645088771765586\n",
      "Epoch 94/100: Loss: 0.007353553516441025\n",
      "Epoch 95/100: Loss: 0.009579750009288545\n",
      "Epoch 96/100: Loss: 0.006013619507575641\n",
      "Epoch 97/100: Loss: 0.008635528042941588\n",
      "Epoch 98/100: Loss: 0.00847769868269097\n",
      "Epoch 99/100: Loss: 0.006803924887208268\n",
      "Epoch 100/100: Loss: 0.003713433095254004\n",
      "time costs: 727.4283258914948\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "accuarcies = []\n",
    "times = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'training epoch: {i + 1}')\n",
    "    qmodel = QModel(input_size, hidden_size, num_output, \n",
    "                num_layers = num_layers, ctx = ctx, mode='classical')\n",
    "    optimizer = torch.optim.Adam(qmodel.parameters(), lr = 0.001)\n",
    "    loss_func = nn.MSELoss()\n",
    "    start = time.time()\n",
    "    losses = train_model(qmodel, train_data, batch_size=20,          \n",
    "                   loss_func = loss_func, optimizer = optimizer, epoch = 100)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'time costs: {end - start}')\n",
    "    times.append(end - start)\n",
    "    \n",
    "    acc = calculate_accuarcy(qmodel, X_test, y_test)\n",
    "    accuarcies.append(acc)\n",
    "    \n",
    "    with open(f'loss/layer4/loss{i + 1}.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(losses, pkl_file)\n",
    "    \n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ac9c597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c395ba7b50>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses, color=\"#FF6666\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3343ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantum paramers: 112\n"
     ]
    }
   ],
   "source": [
    "print(f'quantum paramers: {QLSTM(1, 1, ctx = ctx).qparameters_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5af0126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5555867"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.5555867\n",
    "# \n",
    "# [tensor(0.2734),\n",
    "#  tensor(0.2269),\n",
    "#  tensor(0.7576),\n",
    "#  tensor(0.9693),\n",
    "#  tensor(0.3253),\n",
    "#  tensor(0.5055),\n",
    "#  tensor(0.5879),\n",
    "#  tensor(0.5167),\n",
    "#  tensor(0.4568),\n",
    "#  tensor(0.9364)]\n",
    "np.mean(accuarcies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e317ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "695.3771038532257"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 695.3771038532257 \n",
    "np.mean(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2370cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
