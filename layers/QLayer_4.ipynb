{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6581af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94243045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff0ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyqpanda import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "455c7f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a618a3a",
   "metadata": {},
   "source": [
    "# 1. Prepare Dadaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c64faba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/sumanthvrao/daily-climate-time-series-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de627766",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './../data/DailyDelhiClimateTrain.csv'\n",
    "test_path = './../data/DailyDelhiClimateTest.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d0827fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [1,2,3,4]\n",
    "\n",
    "train = pd.read_csv(train_path, usecols=cols, engine=\"python\")\n",
    "test = pd.read_csv(test_path, usecols=cols, engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c3039c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train)=1462\n",
      "len(test)=114\n"
     ]
    }
   ],
   "source": [
    "print(f'len(train)={len(train)}')\n",
    "print(f'len(test)={len(test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b941db2e",
   "metadata": {},
   "source": [
    "## 1.1 Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e59fddc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove outliers num: 9\n"
     ]
    }
   ],
   "source": [
    "unnormal_num = 0\n",
    "for i in range(len(train)):\n",
    "    mp = train.iloc[i][3]\n",
    "    if mp > 1200 or mp < 950:\n",
    "        unnormal_num += 1\n",
    "        train.iloc[i][3] = train.iloc[i + 1][3]\n",
    "print(f'remove outliers num: {unnormal_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fefec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.iloc[0][3] = test.iloc[1][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6035297e",
   "metadata": {},
   "source": [
    "## 1.2 Transfer data to LSTM representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a1277fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "884bc13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(data, window_size, predict_size):\n",
    "    scaler = StandardScaler()\n",
    "    data = scaler.fit_transform(np.array(data).reshape(-1, 1))\n",
    "    \n",
    "    data_in = []\n",
    "    data_out = []\n",
    "    \n",
    "    for i in range(data.shape[0] - window_size - predict_size):\n",
    "        data_in.append(data[i:i + window_size].reshape(1, window_size)[0])\n",
    "        data_out.append(data[i + window_size:i + window_size + predict_size].reshape(1, predict_size)[0])\n",
    "        \n",
    "    data_in = np.array(data_in).reshape(-1, window_size)\n",
    "    data_out = np.array(data_out).reshape(-1, predict_size)\n",
    "    \n",
    "    data_process = {'datain': data_in, 'dataout': data_out}\n",
    "    \n",
    "    return data_process, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6517fe60",
   "metadata": {},
   "source": [
    "## 1.3 prepare train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d333c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_size = 4\n",
    "window_size = features_size * 3 # features num * time steps\n",
    "predict_size = features_size # features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ef548b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed, train_scaler = data_process(train, window_size, predict_size)\n",
    "X_train, y_train = train_processed['datain'], train_processed['dataout']\n",
    "\n",
    "test_processed, test_scaler = data_process(test, window_size, predict_size)\n",
    "X_test, y_test = test_processed['datain'], test_processed['dataout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f779325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dda516d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "\n",
    "train_data = Data.TensorDataset(X_train, y_train)\n",
    "test_data = Data.TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb132c",
   "metadata": {},
   "source": [
    "# 2. Quantum Enhanced LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d517f5f3",
   "metadata": {},
   "source": [
    "## 2.1 initiate quantum environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffc85d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitQMachine:\n",
    "    def __init__(self, qubitsCount, cbitsCount = 0, machineType = QMachineType.CPU):\n",
    "        self.machine = init_quantum_machine(machineType)\n",
    "        \n",
    "        self.qubits = self.machine.qAlloc_many(qubitsCount)\n",
    "        self.cbits = self.machine.cAlloc_many(cbitsCount)\n",
    "        \n",
    "        print(f'Init Quantum Machine with qubits:[{qubitsCount}] / cbits:[{cbitsCount}] Successfully')\n",
    "    \n",
    "    def __del__(self):\n",
    "        destroy_quantum_machine(self.machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "741a6a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Quantum Machine with qubits:[4] / cbits:[0] Successfully\n"
     ]
    }
   ],
   "source": [
    "# maximum qubits size\n",
    "ctx = InitQMachine(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a39cf3",
   "metadata": {},
   "source": [
    "## 2.2 Quantum Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b029d",
   "metadata": {},
   "source": [
    "### 2.2.1 Quantum layer base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5570e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f9cfd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayerBase(nn.Module):\n",
    "    def __init__(self, input_size, output_size, *, n_qubits, n_layers = 1, ctx = None):\n",
    "        super(QuantumLayerBase, self).__init__()\n",
    "        \n",
    "        self.data = None # need to input during forward\n",
    "    \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size # hidden size, not n_qubits\n",
    "        \n",
    "        # quantum infos\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.ctx = ctx\n",
    "        self.qubits = ctx.qubits\n",
    "        self.machine = ctx.machine\n",
    "        \n",
    "        # convert quantum input/output to match classical computation\n",
    "        self.qin = nn.Linear(self.input_size, self.n_qubits)\n",
    "        self.qout = nn.Linear(self.n_qubits, self.output_size)\n",
    "        \n",
    "    @property\n",
    "    def circuit(self):\n",
    "        raise NotImplementedError('Should init circuit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94c5bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(self):\n",
    "    HamiZ = [ PauliOperator({f'Z{i}': 1}) for i in range(len(self.qubits)) ]\n",
    "    res = [ eval(qop(self.circuit, Hami, self.machine, self.qubits))[0,0] for Hami in HamiZ ]\n",
    "    \n",
    "    return Parameter(Tensor(res[:self.n_qubits]))\n",
    "\n",
    "QuantumLayerBase.measure = measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4341341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, inputs):\n",
    "    y_t = self.qin(Parameter(inputs))\n",
    "    self.data = y_t[0]\n",
    "    \n",
    "    return self.qout(self.measure())\n",
    "\n",
    "QuantumLayerBase.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4f794b",
   "metadata": {},
   "source": [
    "### 2.2.2 Quantum layer design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b98a8984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayer(QuantumLayerBase):\n",
    "    def __init__(self, input_size, output_size, *, n_qubits, degree = 1, n_layers = 1, ctx = None):\n",
    "        super(QuantumLayer, self).__init__(input_size, output_size, \n",
    "                                         n_qubits = n_qubits, n_layers = n_layers, ctx = ctx)\n",
    "        \n",
    "        self.degree = degree\n",
    "        self.angles = Parameter(torch.rand(n_layers * 7, degree, self.n_qubits))\n",
    "        \n",
    "    @property\n",
    "    def qparameters_size(self):\n",
    "        return self.angles.flatten().size()[0]\n",
    "        \n",
    "    @property\n",
    "    def circuit(self):\n",
    "        if self.data == None:\n",
    "            raise ValueError('Need to feed a input data!')\n",
    "        \n",
    "        n = self.n_qubits\n",
    "        q = self.qubits\n",
    "        x = self.data\n",
    "        p = self.angles\n",
    "        degree = self.degree\n",
    "        \n",
    "        h = VariationalQuantumGate_H\n",
    "        rx = VariationalQuantumGate_RX\n",
    "        ry = VariationalQuantumGate_RY\n",
    "        rz = VariationalQuantumGate_RZ\n",
    "        crx = VariationalQuantumGate_CRX\n",
    "        \n",
    "        # init variational quantum circuit\n",
    "        vqc = VariationalQuantumCircuit()\n",
    "\n",
    "        # encoding layer\n",
    "        [ vqc.insert( h(q[i]) ) for i in range(n) ]\n",
    "        [ vqc.insert( ry(q[i], var(x[i] * torch.pi / 2)) ) for i in range(n) ]\n",
    "        \n",
    "        # variational layer\n",
    "        [ vqc.insert( rx(q[i], var(p[0][0][i]) )) for i in range(n) ]\n",
    "        [ vqc.insert( rz(q[i], var(p[1][0][i]) )) for i in range(n) ]\n",
    "        \n",
    "        vqc.insert(crx(q[2], q[3], var(p[2][0][0])))\n",
    "        vqc.insert(crx(q[1], q[3], var(p[2][0][1])))\n",
    "        vqc.insert(crx(q[0], q[3], var(p[2][0][2])))\n",
    "        vqc.insert(crx(q[3], q[2], var(p[2][0][3])))\n",
    "        \n",
    "        vqc.insert(crx(q[1], q[2], var(p[3][0][0])))\n",
    "        vqc.insert(crx(q[0], q[2], var(p[3][0][1])))\n",
    "        vqc.insert(crx(q[3], q[1], var(p[3][0][2])))\n",
    "        vqc.insert(crx(q[2], q[1], var(p[3][0][3])))\n",
    "        \n",
    "        vqc.insert(crx(q[0], q[1], var(p[4][0][0])))\n",
    "        vqc.insert(crx(q[3], q[0], var(p[4][0][1])))\n",
    "        vqc.insert(crx(q[2], q[0], var(p[4][0][2])))\n",
    "        vqc.insert(crx(q[1], q[0], var(p[4][0][3])))\n",
    "        \n",
    "        [ vqc.insert( rx(q[i], var(p[5][0][i]) )) for i in range(n) ]\n",
    "        [ vqc.insert( rz(q[i], var(p[6][0][i]) )) for i in range(n) ]\n",
    "        \n",
    "        return vqc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766692de",
   "metadata": {},
   "source": [
    "### 2.2.3 Plot Quantum Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88ff286b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyqpanda.pyQPanda.QProg at 0x1f3fa8fe970>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Tensor([[0.1, 0.2, 0.3, 0.4]])\n",
    "layer = QuantumLayer(4, 4, n_qubits=4, n_layers=1, degree=3, ctx=ctx)\n",
    "layer.data = data[0]\n",
    "vqc = layer.circuit\n",
    "prog = create_empty_qprog()\n",
    "prog.insert(vqc.feed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fd364d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'null'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_qprog(prog, 'pic', filename=f'pic/layer4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50742459",
   "metadata": {},
   "source": [
    "## 2.3 Quantum-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e53ae0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLSTMBase(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, *, ctx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ctx = ctx\n",
    "        \n",
    "    @property\n",
    "    def qparameters_size(self):\n",
    "        num = 0\n",
    "        for attr in dir(self):\n",
    "            if attr.endswith('_circuit'):\n",
    "                num += getattr(self, attr).qparameters_size\n",
    "        return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "582b17af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, inputs, init_states = None):\n",
    "    sequence_size, batch_size, _ = inputs.size()\n",
    "    hidden_sequence = []\n",
    "    \n",
    "    if init_states == None:\n",
    "        h_t, c_t = (\n",
    "            torch.zeros(1, batch_size, self.hidden_size).to(inputs.device),\n",
    "            torch.zeros(1, batch_size, self.hidden_size).to(inputs.device),\n",
    "        )\n",
    "    else:\n",
    "        h_t, c_t = init_states\n",
    "    \n",
    "    return hidden_sequence, (h_t, c_t)\n",
    "\n",
    "QLSTMBase.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d6b0af",
   "metadata": {},
   "source": [
    "## - classical quatum enhanced LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adc8fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLSTM(QLSTMBase):\n",
    "    def __init__(self, input_size, hidden_size, *, ctx):\n",
    "        super().__init__(input_size, hidden_size, ctx = ctx)\n",
    "    \n",
    "        # input gates\n",
    "        self.input_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                        n_qubits = 4, ctx = ctx) # 15\n",
    "        # forget gates\n",
    "        self.forget_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                         n_qubits = 4, ctx = ctx) # 15\n",
    "        # candidate\n",
    "        self.candidate_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                       n_qubits = 4, ctx = ctx) # 15\n",
    "        # output gates\n",
    "        self.output_circuit = QuantumLayer(input_size + hidden_size, hidden_size, \n",
    "                                         n_qubits = 4, ctx = ctx) # 15\n",
    "        \n",
    "    def forward(self, inputs, init_states = None):\n",
    "        hidden_sequence, (h_t, c_t) = super(QLSTM, self).forward(inputs, init_states)\n",
    "\n",
    "        for t in range(inputs.size()[0]):\n",
    "            x_t = inputs[t, :, :]\n",
    "            v_t = torch.cat((h_t[0], x_t), dim = 1)\n",
    "\n",
    "            # input gates\n",
    "            i_t = torch.sigmoid(self.input_circuit(v_t))\n",
    "            # forget gates\n",
    "            f_t = torch.sigmoid(self.forget_circuit(v_t))\n",
    "            # candidate for cell state update\n",
    "            g_t = torch.tanh(self.candidate_circuit(v_t))\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "\n",
    "            # output gates\n",
    "            o_t = torch.sigmoid(self.output_circuit(v_t))\n",
    "            # update output ht\n",
    "            h_t = o_t * (torch.tanh(c_t))\n",
    "\n",
    "            hidden_sequence.append(h_t)\n",
    "\n",
    "        # reshape hidden_seq p/ retornar\n",
    "        #\n",
    "        # [tensor([[[0.0444, ...]]] => tensor([[[0.0444, ...]]]\n",
    "        # \n",
    "        hidden_sequence = torch.cat(hidden_sequence, dim = 0)\n",
    "\n",
    "        return hidden_sequence, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28836475",
   "metadata": {},
   "source": [
    "## 2.4 Stacked QLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ec06c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class StackedQLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, *, num_layers = 1, ctx = None, mode = 'classical'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.qlstms = nn.Sequential(OrderedDict([\n",
    "            (f'QLSTM {i + 1}', QLSTM(input_size if i == 0 else hidden_size , hidden_size, ctx = ctx)) \n",
    "                for i in range(num_layers)\n",
    "        ]))\n",
    "\n",
    "    def forward(self, inputs, parameters = None):\n",
    "        outputs = None\n",
    "        \n",
    "        for i, qlstm in enumerate(self.qlstms):\n",
    "            if i != 0:\n",
    "                inputs = outputs\n",
    "            \n",
    "            outputs, parameters = qlstm(inputs, parameters)\n",
    "        \n",
    "        return outputs, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0389535c",
   "metadata": {},
   "source": [
    "# 3. Quantum Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "413150bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_output, *, num_layers = 1, ctx = None, mode = 'classical'):\n",
    "        super(QModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.qlstm = StackedQLSTM(input_size, hidden_size, \n",
    "                                  num_layers = num_layers, ctx = ctx, mode = mode)\n",
    "        self.predict = nn.Linear(hidden_size, num_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        # sequence lenth , batch_size, features length\n",
    "        # \n",
    "        h0 = torch.zeros(1, x.size(1), self.hidden_size)\n",
    "        c0 = torch.zeros(1, x.size(1), self.hidden_size)\n",
    "        \n",
    "        out, _ = self.qlstm(x, (h0, c0))\n",
    "        out = self.predict(out[0])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111aebe4",
   "metadata": {},
   "source": [
    "## 3.1 train QModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc1d1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "def train_model(model, datas, batch_size, *, loss_func, optimizer, epoch = 50):\n",
    "    losses = []\n",
    "    sampler = RandomSampler(datas, num_samples = batch_size)\n",
    "    \n",
    "    for step in range(epoch):\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for index in sampler:\n",
    "            batch_x, batch_y = datas[index][0], datas[index][1]\n",
    "            b_x = batch_x.unsqueeze(0)\n",
    "            b_y = batch_y.unsqueeze(0)\n",
    "            \n",
    "            output = model(b_x)\n",
    "\n",
    "            loss = loss_func(output, b_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {step + 1}/{epoch}: Loss: {train_loss / batch_size}')\n",
    "        losses.append(train_loss / batch_size)\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef07934f",
   "metadata": {},
   "source": [
    "## 3.2 Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57e9b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "def MAE_naive(actuals, predicteds):\n",
    "    n = len(actuals)\n",
    "    err = 0.0\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        err += np.abs(actuals[i] - actuals[i - 1])\n",
    "    return err / (n - 1)\n",
    "\n",
    "def calculate_accuarcy(model, X_test, y_test, scaler=test_scaler):\n",
    "    n = len(X_test)\n",
    "    \n",
    "    actuals = []\n",
    "    predicteds = []\n",
    "    \n",
    "    for i in range(0, n, predict_size):\n",
    "        actual = scaler.inverse_transform(y_test[i:i+1].data)\n",
    "        actuals.append(np.array(actual[0]))\n",
    "        predicted = scaler.inverse_transform(model(X_test[i:i+1]).data)\n",
    "        predicteds.append(np.array(predicted[0]))\n",
    "    \n",
    "    actuals = np.array(actuals)\n",
    "    predicteds = np.array(predicteds)\n",
    "    \n",
    "    mae = mean_absolute_error(actuals, predicteds)\n",
    "    mase = mae / MAE_naive(actuals.flatten(), predicteds.flatten())\n",
    "    mape = mean_absolute_percentage_error(actuals, predicteds)\n",
    "    mse = mean_squared_error(actuals, predicteds)\n",
    "    rmse = mse ** 0.5\n",
    "    \n",
    "    return np.array([(1 - mase) * 100, rmse, mse, mae, mape])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6407f969",
   "metadata": {},
   "source": [
    "## 3.3 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "305c9471",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_size = 4\n",
    "window_size = features_size * 3 # \n",
    "predict_size = features_size # features\n",
    "\n",
    "input_size = window_size\n",
    "num_output = predict_size\n",
    "\n",
    "hidden_size = 32\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50472565",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: 1\n",
      "Epoch 1/100: Loss: 1.0081073313951492\n",
      "Epoch 2/100: Loss: 1.005624595284462\n",
      "Epoch 3/100: Loss: 0.9963132113218307\n",
      "Epoch 4/100: Loss: 0.9757258176803589\n",
      "Epoch 5/100: Loss: 1.0204775601625442\n",
      "Epoch 6/100: Loss: 0.9969383656978608\n",
      "Epoch 7/100: Loss: 0.9845762997865677\n",
      "Epoch 8/100: Loss: 1.0010033220052719\n",
      "Epoch 9/100: Loss: 0.9695729076862335\n",
      "Epoch 10/100: Loss: 0.9882864028215408\n",
      "Epoch 11/100: Loss: 0.9458590835332871\n",
      "Epoch 12/100: Loss: 0.9939503580331802\n",
      "Epoch 13/100: Loss: 0.9711661905050277\n",
      "Epoch 14/100: Loss: 0.9401040464639664\n",
      "Epoch 15/100: Loss: 0.9619677245616913\n",
      "Epoch 16/100: Loss: 0.9395138710737229\n",
      "Epoch 17/100: Loss: 0.9601024985313416\n",
      "Epoch 18/100: Loss: 0.9190952509641648\n",
      "Epoch 19/100: Loss: 0.9314173787832261\n",
      "Epoch 20/100: Loss: 0.9327853202819825\n",
      "Epoch 21/100: Loss: 0.9338593155145645\n",
      "Epoch 22/100: Loss: 0.897594478726387\n",
      "Epoch 23/100: Loss: 0.9322699189186097\n",
      "Epoch 24/100: Loss: 0.8786177635192871\n",
      "Epoch 25/100: Loss: 0.8641764461994171\n",
      "Epoch 26/100: Loss: 0.8818723320960998\n",
      "Epoch 27/100: Loss: 0.8468759208917618\n",
      "Epoch 28/100: Loss: 0.8168394863605499\n",
      "Epoch 29/100: Loss: 0.7909252852201462\n",
      "Epoch 30/100: Loss: 0.8277621299028397\n",
      "Epoch 31/100: Loss: 0.7703674614429474\n",
      "Epoch 32/100: Loss: 0.7353487282991409\n",
      "Epoch 33/100: Loss: 0.7009169369935989\n",
      "Epoch 34/100: Loss: 0.7027826756238937\n",
      "Epoch 35/100: Loss: 0.6890395864844322\n",
      "Epoch 36/100: Loss: 0.6625871539115906\n",
      "Epoch 37/100: Loss: 0.5632879242300988\n",
      "Epoch 38/100: Loss: 0.5170754685997963\n",
      "Epoch 39/100: Loss: 0.5871099196374416\n",
      "Epoch 40/100: Loss: 0.4262084186077118\n",
      "Epoch 41/100: Loss: 0.5331041112542152\n",
      "Epoch 42/100: Loss: 0.5441777274012566\n",
      "Epoch 43/100: Loss: 0.4386500738561153\n",
      "Epoch 44/100: Loss: 0.5310613363981247\n",
      "Epoch 45/100: Loss: 0.449641615152359\n",
      "Epoch 46/100: Loss: 0.449000845849514\n",
      "Epoch 47/100: Loss: 0.4599756522104144\n",
      "Epoch 48/100: Loss: 0.44755604937672616\n",
      "Epoch 49/100: Loss: 0.37413263134658337\n",
      "Epoch 50/100: Loss: 0.34467906821519134\n",
      "Epoch 51/100: Loss: 0.4066923337057233\n",
      "Epoch 52/100: Loss: 0.32043326292186974\n",
      "Epoch 53/100: Loss: 0.32745119109749793\n",
      "Epoch 54/100: Loss: 0.3840932507067919\n",
      "Epoch 55/100: Loss: 0.3109230637550354\n",
      "Epoch 56/100: Loss: 0.42174736261367796\n",
      "Epoch 57/100: Loss: 0.2767273941077292\n",
      "Epoch 58/100: Loss: 0.19683501459658145\n",
      "Epoch 59/100: Loss: 0.3539889924228191\n",
      "Epoch 60/100: Loss: 0.340475616697222\n",
      "Epoch 61/100: Loss: 0.29222522899508474\n",
      "Epoch 62/100: Loss: 0.2565619551576674\n",
      "Epoch 63/100: Loss: 0.1923731964081526\n",
      "Epoch 64/100: Loss: 0.30066840462386607\n",
      "Epoch 65/100: Loss: 0.3043135403655469\n",
      "Epoch 66/100: Loss: 0.3789272231515497\n",
      "Epoch 67/100: Loss: 0.24304793667979538\n",
      "Epoch 68/100: Loss: 0.38403938205447047\n",
      "Epoch 69/100: Loss: 0.2358125548809767\n",
      "Epoch 70/100: Loss: 0.37159324837848545\n",
      "Epoch 71/100: Loss: 0.2771349317394197\n",
      "Epoch 72/100: Loss: 0.2330506098922342\n",
      "Epoch 73/100: Loss: 0.15040652479510755\n",
      "Epoch 74/100: Loss: 0.34507245174609125\n",
      "Epoch 75/100: Loss: 0.23602127187186853\n",
      "Epoch 76/100: Loss: 0.34167828493518754\n",
      "Epoch 77/100: Loss: 0.3146622706786729\n",
      "Epoch 78/100: Loss: 0.2905761303496547\n",
      "Epoch 79/100: Loss: 0.3645166642148979\n",
      "Epoch 80/100: Loss: 0.22373454774497076\n",
      "Epoch 81/100: Loss: 0.3099388164759148\n",
      "Epoch 82/100: Loss: 0.3081648600491462\n",
      "Epoch 83/100: Loss: 0.3489229723752942\n",
      "Epoch 84/100: Loss: 0.344358303968329\n",
      "Epoch 85/100: Loss: 0.3595907727023587\n",
      "Epoch 86/100: Loss: 0.2709394991747104\n",
      "Epoch 87/100: Loss: 0.2260065712733194\n",
      "Epoch 88/100: Loss: 0.303835218353197\n",
      "Epoch 89/100: Loss: 0.22592029985389672\n",
      "Epoch 90/100: Loss: 0.3423219052478089\n",
      "Epoch 91/100: Loss: 0.3070094422932016\n",
      "Epoch 92/100: Loss: 0.27001493665156884\n",
      "Epoch 93/100: Loss: 0.2911237011852791\n",
      "Epoch 94/100: Loss: 0.16023648307600524\n",
      "Epoch 95/100: Loss: 0.31694120735337494\n",
      "Epoch 96/100: Loss: 0.28717599547526335\n",
      "Epoch 97/100: Loss: 0.2116420349026157\n",
      "Epoch 98/100: Loss: 0.2709440247854218\n",
      "Epoch 99/100: Loss: 0.21271212697611191\n",
      "Epoch 100/100: Loss: 0.17362969162204536\n",
      "time costs: 447.47914361953735\n",
      "--------------------\n",
      "training epoch: 2\n",
      "Epoch 1/100: Loss: 1.0414294332265854\n",
      "Epoch 2/100: Loss: 0.9788968622684479\n",
      "Epoch 3/100: Loss: 1.023052766919136\n",
      "Epoch 4/100: Loss: 0.9799429178237915\n",
      "Epoch 5/100: Loss: 1.0166207015514375\n",
      "Epoch 6/100: Loss: 1.005630338191986\n",
      "Epoch 7/100: Loss: 1.022210818529129\n",
      "Epoch 8/100: Loss: 1.00862897336483\n",
      "Epoch 9/100: Loss: 0.9790787398815155\n",
      "Epoch 10/100: Loss: 0.9881917715072632\n",
      "Epoch 11/100: Loss: 0.9947690814733505\n",
      "Epoch 12/100: Loss: 0.9737566769123077\n",
      "Epoch 13/100: Loss: 0.9776848077774047\n",
      "Epoch 14/100: Loss: 0.9779800146818161\n",
      "Epoch 15/100: Loss: 0.9649380177259446\n",
      "Epoch 16/100: Loss: 0.9682085186243057\n",
      "Epoch 17/100: Loss: 0.9536501735448837\n",
      "Epoch 18/100: Loss: 0.9673107951879502\n",
      "Epoch 19/100: Loss: 0.9443429410457611\n",
      "Epoch 20/100: Loss: 0.9560281604528427\n",
      "Epoch 21/100: Loss: 0.9374442249536514\n",
      "Epoch 22/100: Loss: 0.9282171905040741\n",
      "Epoch 23/100: Loss: 0.9444594264030457\n",
      "Epoch 24/100: Loss: 0.9234724879264832\n",
      "Epoch 25/100: Loss: 0.8938633561134338\n",
      "Epoch 26/100: Loss: 0.9089537709951401\n",
      "Epoch 27/100: Loss: 0.8688263118267059\n",
      "Epoch 28/100: Loss: 0.8728982031345367\n",
      "Epoch 29/100: Loss: 0.8726184993982316\n",
      "Epoch 30/100: Loss: 0.8190590292215347\n",
      "Epoch 31/100: Loss: 0.8760260015726089\n",
      "Epoch 32/100: Loss: 0.783030840754509\n",
      "Epoch 33/100: Loss: 0.8841042578220367\n",
      "Epoch 34/100: Loss: 0.7861694931983948\n",
      "Epoch 35/100: Loss: 0.7910550326108933\n",
      "Epoch 36/100: Loss: 0.714917105436325\n",
      "Epoch 37/100: Loss: 0.7167349725961685\n",
      "Epoch 38/100: Loss: 0.7528883069753647\n",
      "Epoch 39/100: Loss: 0.7421978175640106\n",
      "Epoch 40/100: Loss: 0.7068239748477936\n",
      "Epoch 41/100: Loss: 0.642345979809761\n",
      "Epoch 42/100: Loss: 0.706117895245552\n",
      "Epoch 43/100: Loss: 0.6004505053162574\n",
      "Epoch 44/100: Loss: 0.6543634951114654\n",
      "Epoch 45/100: Loss: 0.6520877659320832\n",
      "Epoch 46/100: Loss: 0.6224631696939469\n",
      "Epoch 47/100: Loss: 0.556518018245697\n",
      "Epoch 48/100: Loss: 0.667755988240242\n",
      "Epoch 49/100: Loss: 0.5764072954654693\n",
      "Epoch 50/100: Loss: 0.5131757661700249\n",
      "Epoch 51/100: Loss: 0.685166896879673\n",
      "Epoch 52/100: Loss: 0.5642605870962143\n",
      "Epoch 53/100: Loss: 0.5458551809191704\n",
      "Epoch 54/100: Loss: 0.5808473274111747\n",
      "Epoch 55/100: Loss: 0.587660337984562\n",
      "Epoch 56/100: Loss: 0.5769327461719513\n",
      "Epoch 57/100: Loss: 0.5698239058256149\n",
      "Epoch 58/100: Loss: 0.5556842431426048\n",
      "Epoch 59/100: Loss: 0.5939340308308602\n",
      "Epoch 60/100: Loss: 0.5388490334153175\n",
      "Epoch 61/100: Loss: 0.5441315561532974\n",
      "Epoch 62/100: Loss: 0.47968263030052183\n",
      "Epoch 63/100: Loss: 0.5680817142128944\n",
      "Epoch 64/100: Loss: 0.5274945229291916\n",
      "Epoch 65/100: Loss: 0.5309204772114754\n",
      "Epoch 66/100: Loss: 0.6066029131412506\n",
      "Epoch 67/100: Loss: 0.5200700029730797\n",
      "Epoch 68/100: Loss: 0.49244857132434844\n",
      "Epoch 69/100: Loss: 0.42169795632362367\n",
      "Epoch 70/100: Loss: 0.5051953889429569\n",
      "Epoch 71/100: Loss: 0.5884916171431541\n",
      "Epoch 72/100: Loss: 0.5639445915818214\n",
      "Epoch 73/100: Loss: 0.5282789692282677\n",
      "Epoch 74/100: Loss: 0.5251294419169426\n",
      "Epoch 75/100: Loss: 0.41904209107160567\n",
      "Epoch 76/100: Loss: 0.4343929998576641\n",
      "Epoch 77/100: Loss: 0.3823419094085693\n",
      "Epoch 78/100: Loss: 0.48823736533522605\n",
      "Epoch 79/100: Loss: 0.4299273706972599\n",
      "Epoch 80/100: Loss: 0.352418202906847\n",
      "Epoch 81/100: Loss: 0.41831743642687796\n",
      "Epoch 82/100: Loss: 0.3711620680987835\n",
      "Epoch 83/100: Loss: 0.3678739495575428\n",
      "Epoch 84/100: Loss: 0.37522725835442544\n",
      "Epoch 85/100: Loss: 0.32861570790410044\n",
      "Epoch 86/100: Loss: 0.46810988858342173\n",
      "Epoch 87/100: Loss: 0.3929756782948971\n",
      "Epoch 88/100: Loss: 0.4786763586103916\n",
      "Epoch 89/100: Loss: 0.36243841126561166\n",
      "Epoch 90/100: Loss: 0.3362972728908062\n",
      "Epoch 91/100: Loss: 0.3581696398556232\n",
      "Epoch 92/100: Loss: 0.27448794394731524\n",
      "Epoch 93/100: Loss: 0.30289174281060693\n",
      "Epoch 94/100: Loss: 0.30404650531709193\n",
      "Epoch 95/100: Loss: 0.2525389827787876\n",
      "Epoch 96/100: Loss: 0.3790105953812599\n",
      "Epoch 97/100: Loss: 0.5543641656637192\n",
      "Epoch 98/100: Loss: 0.33238556012511256\n",
      "Epoch 99/100: Loss: 0.4029517017304897\n",
      "Epoch 100/100: Loss: 0.39422245398163797\n",
      "time costs: 468.26232743263245\n",
      "--------------------\n",
      "training epoch: 3\n",
      "Epoch 1/100: Loss: 1.013881301879883\n",
      "Epoch 2/100: Loss: 0.9953812777996063\n",
      "Epoch 3/100: Loss: 0.9778852701187134\n",
      "Epoch 4/100: Loss: 1.0237582355737687\n",
      "Epoch 5/100: Loss: 0.9767329782247544\n",
      "Epoch 6/100: Loss: 0.9865604698657989\n",
      "Epoch 7/100: Loss: 0.9798303514719009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: Loss: 0.9870753407478332\n",
      "Epoch 9/100: Loss: 0.9649147689342499\n",
      "Epoch 10/100: Loss: 0.9945405125617981\n",
      "Epoch 11/100: Loss: 0.9587217837572097\n",
      "Epoch 12/100: Loss: 0.9829624861478805\n",
      "Epoch 13/100: Loss: 0.9298780292272568\n",
      "Epoch 14/100: Loss: 0.9776293694972992\n",
      "Epoch 15/100: Loss: 0.9546035408973694\n",
      "Epoch 16/100: Loss: 0.9284756928682327\n",
      "Epoch 17/100: Loss: 0.9380387753248215\n",
      "Epoch 18/100: Loss: 0.9394188761711121\n",
      "Epoch 19/100: Loss: 0.8992513656616211\n",
      "Epoch 20/100: Loss: 0.9083225756883622\n",
      "Epoch 21/100: Loss: 0.9129215329885483\n",
      "Epoch 22/100: Loss: 0.9210378140211105\n",
      "Epoch 23/100: Loss: 0.8496768534183502\n",
      "Epoch 24/100: Loss: 0.8642576336860657\n",
      "Epoch 25/100: Loss: 0.9151501297950745\n",
      "Epoch 26/100: Loss: 0.8469285577535629\n",
      "Epoch 27/100: Loss: 0.839489820599556\n",
      "Epoch 28/100: Loss: 0.8162018477916717\n",
      "Epoch 29/100: Loss: 0.746232059597969\n",
      "Epoch 30/100: Loss: 0.7302178710699081\n",
      "Epoch 31/100: Loss: 0.7742597490549088\n",
      "Epoch 32/100: Loss: 0.7668728560209275\n",
      "Epoch 33/100: Loss: 0.712369081377983\n",
      "Epoch 34/100: Loss: 0.724848747253418\n",
      "Epoch 35/100: Loss: 0.6705304995179177\n",
      "Epoch 36/100: Loss: 0.6876384660601615\n",
      "Epoch 37/100: Loss: 0.6930544659495353\n",
      "Epoch 38/100: Loss: 0.622950866818428\n",
      "Epoch 39/100: Loss: 0.6518580988049507\n",
      "Epoch 40/100: Loss: 0.7382153093814849\n",
      "Epoch 41/100: Loss: 0.6402736499905586\n",
      "Epoch 42/100: Loss: 0.6468404188752175\n",
      "Epoch 43/100: Loss: 0.608951373398304\n",
      "Epoch 44/100: Loss: 0.6595162436366081\n",
      "Epoch 45/100: Loss: 0.521499314904213\n",
      "Epoch 46/100: Loss: 0.5913081020116806\n",
      "Epoch 47/100: Loss: 0.5891077533364296\n",
      "Epoch 48/100: Loss: 0.5739271745085717\n",
      "Epoch 49/100: Loss: 0.5609603807330131\n",
      "Epoch 50/100: Loss: 0.5291906490921974\n",
      "Epoch 51/100: Loss: 0.5520816683769226\n",
      "Epoch 52/100: Loss: 0.5824907347559929\n",
      "Epoch 53/100: Loss: 0.5966696038842201\n",
      "Epoch 54/100: Loss: 0.529475300014019\n",
      "Epoch 55/100: Loss: 0.5610668689012528\n",
      "Epoch 56/100: Loss: 0.5191008806228637\n",
      "Epoch 57/100: Loss: 0.4239992618560791\n",
      "Epoch 58/100: Loss: 0.4962443202733994\n",
      "Epoch 59/100: Loss: 0.5713012993335724\n",
      "Epoch 60/100: Loss: 0.43062599673867225\n",
      "Epoch 61/100: Loss: 0.48567408248782157\n",
      "Epoch 62/100: Loss: 0.4473802953958511\n",
      "Epoch 63/100: Loss: 0.45328271985054014\n",
      "Epoch 64/100: Loss: 0.4497445046901703\n",
      "Epoch 65/100: Loss: 0.3364123046398163\n",
      "Epoch 66/100: Loss: 0.45829001888632775\n",
      "Epoch 67/100: Loss: 0.3090524785220623\n",
      "Epoch 68/100: Loss: 0.45108042620122435\n",
      "Epoch 69/100: Loss: 0.3035039685666561\n",
      "Epoch 70/100: Loss: 0.37007854226976633\n",
      "Epoch 71/100: Loss: 0.3890049647539854\n",
      "Epoch 72/100: Loss: 0.3326980847865343\n",
      "Epoch 73/100: Loss: 0.24135462660342455\n",
      "Epoch 74/100: Loss: 0.4546621495857835\n",
      "Epoch 75/100: Loss: 0.3499538468196988\n",
      "Epoch 76/100: Loss: 0.32456107456237077\n",
      "Epoch 77/100: Loss: 0.2929313827306032\n",
      "Epoch 78/100: Loss: 0.28762569641694424\n",
      "Epoch 79/100: Loss: 0.3356970771215856\n",
      "Epoch 80/100: Loss: 0.24910403555259109\n",
      "Epoch 81/100: Loss: 0.4214308927766979\n",
      "Epoch 82/100: Loss: 0.3616053347242996\n",
      "Epoch 83/100: Loss: 0.3009382291696966\n",
      "Epoch 84/100: Loss: 0.3275089694187045\n",
      "Epoch 85/100: Loss: 0.3982411463512108\n",
      "Epoch 86/100: Loss: 0.35486244650091975\n",
      "Epoch 87/100: Loss: 0.24557404960505663\n",
      "Epoch 88/100: Loss: 0.268465505389031\n",
      "Epoch 89/100: Loss: 0.24980722432956098\n",
      "Epoch 90/100: Loss: 0.3176789706107229\n",
      "Epoch 91/100: Loss: 0.26217919171322135\n",
      "Epoch 92/100: Loss: 0.30831297222757714\n",
      "Epoch 93/100: Loss: 0.22634795054909773\n",
      "Epoch 94/100: Loss: 0.2602663891739212\n",
      "Epoch 95/100: Loss: 0.22724798867711798\n",
      "Epoch 96/100: Loss: 0.3429000166943297\n",
      "Epoch 97/100: Loss: 0.45813752645044586\n",
      "Epoch 98/100: Loss: 0.28917472394532523\n",
      "Epoch 99/100: Loss: 0.25655803725239823\n",
      "Epoch 100/100: Loss: 0.27631261824280956\n",
      "time costs: 431.2723605632782\n",
      "--------------------\n",
      "training epoch: 4\n",
      "Epoch 1/100: Loss: 1.0221553862094879\n",
      "Epoch 2/100: Loss: 0.9886162608861924\n",
      "Epoch 3/100: Loss: 1.0084310293197631\n",
      "Epoch 4/100: Loss: 1.0056892842054368\n",
      "Epoch 5/100: Loss: 0.9957972586154937\n",
      "Epoch 6/100: Loss: 0.9962327688932419\n",
      "Epoch 7/100: Loss: 0.9867784231901169\n",
      "Epoch 8/100: Loss: 0.9971937596797943\n",
      "Epoch 9/100: Loss: 0.9878966689109803\n",
      "Epoch 10/100: Loss: 1.000859123468399\n",
      "Epoch 11/100: Loss: 1.0074699610471725\n",
      "Epoch 12/100: Loss: 1.0026861518621444\n",
      "Epoch 13/100: Loss: 0.9952293902635574\n",
      "Epoch 14/100: Loss: 0.9771357625722885\n",
      "Epoch 15/100: Loss: 1.0077122896909714\n",
      "Epoch 16/100: Loss: 0.9824906796216964\n",
      "Epoch 17/100: Loss: 0.9770627468824387\n",
      "Epoch 18/100: Loss: 0.9804291218519211\n",
      "Epoch 19/100: Loss: 0.9733053088188172\n",
      "Epoch 20/100: Loss: 0.9735783517360688\n",
      "Epoch 21/100: Loss: 0.9758676379919052\n",
      "Epoch 22/100: Loss: 0.9681376844644547\n",
      "Epoch 23/100: Loss: 0.9737587004899979\n",
      "Epoch 24/100: Loss: 0.9367221742868423\n",
      "Epoch 25/100: Loss: 0.9590246379375458\n",
      "Epoch 26/100: Loss: 0.9706559926271439\n",
      "Epoch 27/100: Loss: 0.9384214520454407\n",
      "Epoch 28/100: Loss: 0.9439080983400345\n",
      "Epoch 29/100: Loss: 0.9478632986545563\n",
      "Epoch 30/100: Loss: 0.9753096371889114\n",
      "Epoch 31/100: Loss: 0.9413644105196\n",
      "Epoch 32/100: Loss: 0.9368735253810883\n",
      "Epoch 33/100: Loss: 0.9399511128664017\n",
      "Epoch 34/100: Loss: 0.9164128780364991\n",
      "Epoch 35/100: Loss: 0.9181952387094497\n",
      "Epoch 36/100: Loss: 0.928902679681778\n",
      "Epoch 37/100: Loss: 0.9069936454296113\n",
      "Epoch 38/100: Loss: 0.8851466953754425\n",
      "Epoch 39/100: Loss: 0.87367182970047\n",
      "Epoch 40/100: Loss: 0.9216637045145035\n",
      "Epoch 41/100: Loss: 0.8810516357421875\n",
      "Epoch 42/100: Loss: 0.8534768462181092\n",
      "Epoch 43/100: Loss: 0.8514876663684845\n",
      "Epoch 44/100: Loss: 0.847201743721962\n",
      "Epoch 45/100: Loss: 0.8114844918251037\n",
      "Epoch 46/100: Loss: 0.811422336101532\n",
      "Epoch 47/100: Loss: 0.8143114417791366\n",
      "Epoch 48/100: Loss: 0.7666826575994492\n",
      "Epoch 49/100: Loss: 0.7914044588804245\n",
      "Epoch 50/100: Loss: 0.7706872820854187\n",
      "Epoch 51/100: Loss: 0.7468588829040528\n",
      "Epoch 52/100: Loss: 0.8070509135723114\n",
      "Epoch 53/100: Loss: 0.698832793533802\n",
      "Epoch 54/100: Loss: 0.5959776759147644\n",
      "Epoch 55/100: Loss: 0.7077079430222512\n",
      "Epoch 56/100: Loss: 0.69844441562891\n",
      "Epoch 57/100: Loss: 0.6107243940234184\n",
      "Epoch 58/100: Loss: 0.7076750501990319\n",
      "Epoch 59/100: Loss: 0.6603874400258064\n",
      "Epoch 60/100: Loss: 0.6130123361945152\n",
      "Epoch 61/100: Loss: 0.5667554005980492\n",
      "Epoch 62/100: Loss: 0.5679434254765511\n",
      "Epoch 63/100: Loss: 0.5495345979928971\n",
      "Epoch 64/100: Loss: 0.5949542813003064\n",
      "Epoch 65/100: Loss: 0.5534420546144247\n",
      "Epoch 66/100: Loss: 0.44484067484736445\n",
      "Epoch 67/100: Loss: 0.6323445796966553\n",
      "Epoch 68/100: Loss: 0.446289224922657\n",
      "Epoch 69/100: Loss: 0.45753265395760534\n",
      "Epoch 70/100: Loss: 0.4648475404828787\n",
      "Epoch 71/100: Loss: 0.4620345389470458\n",
      "Epoch 72/100: Loss: 0.4470635076984763\n",
      "Epoch 73/100: Loss: 0.45434128791093825\n",
      "Epoch 74/100: Loss: 0.5377305824309587\n",
      "Epoch 75/100: Loss: 0.43828853257000444\n",
      "Epoch 76/100: Loss: 0.47970519717782734\n",
      "Epoch 77/100: Loss: 0.48892086520791056\n",
      "Epoch 78/100: Loss: 0.46539203971624377\n",
      "Epoch 79/100: Loss: 0.3511453495360911\n",
      "Epoch 80/100: Loss: 0.5235884185880423\n",
      "Epoch 81/100: Loss: 0.3883007944561541\n",
      "Epoch 82/100: Loss: 0.5304650176782161\n",
      "Epoch 83/100: Loss: 0.5823986906558275\n",
      "Epoch 84/100: Loss: 0.41138345981016755\n",
      "Epoch 85/100: Loss: 0.43209403855726125\n",
      "Epoch 86/100: Loss: 0.4375539031811059\n",
      "Epoch 87/100: Loss: 0.36779140452854336\n",
      "Epoch 88/100: Loss: 0.3702347983140498\n",
      "Epoch 89/100: Loss: 0.3975262830965221\n",
      "Epoch 90/100: Loss: 0.37291232608258723\n",
      "Epoch 91/100: Loss: 0.4156576772220433\n",
      "Epoch 92/100: Loss: 0.5148166372440756\n",
      "Epoch 93/100: Loss: 0.4044445062987506\n",
      "Epoch 94/100: Loss: 0.4127150052692741\n",
      "Epoch 95/100: Loss: 0.31642297802027314\n",
      "Epoch 96/100: Loss: 0.44363700470421463\n",
      "Epoch 97/100: Loss: 0.2288681695703417\n",
      "Epoch 98/100: Loss: 0.41102006745059044\n",
      "Epoch 99/100: Loss: 0.3260006561875343\n",
      "Epoch 100/100: Loss: 0.2836584229022264\n",
      "time costs: 444.3707528114319\n",
      "--------------------\n",
      "training epoch: 5\n",
      "Epoch 1/100: Loss: 1.00400570333004\n",
      "Epoch 2/100: Loss: 1.0045237004756928\n",
      "Epoch 3/100: Loss: 1.0355614989995956\n",
      "Epoch 4/100: Loss: 1.0214268654584884\n",
      "Epoch 5/100: Loss: 0.9911895751953125\n",
      "Epoch 6/100: Loss: 0.9993466824293137\n",
      "Epoch 7/100: Loss: 0.9757953584194183\n",
      "Epoch 8/100: Loss: 0.9552847385406494\n",
      "Epoch 9/100: Loss: 0.9438874632120132\n",
      "Epoch 10/100: Loss: 0.9404377281665802\n",
      "Epoch 11/100: Loss: 0.9216171503067017\n",
      "Epoch 12/100: Loss: 0.9195371508598328\n",
      "Epoch 13/100: Loss: 0.9007017850875855\n",
      "Epoch 14/100: Loss: 0.8777228564023971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: Loss: 0.8805039197206497\n",
      "Epoch 16/100: Loss: 0.8309952318668365\n",
      "Epoch 17/100: Loss: 0.8210094302892685\n",
      "Epoch 18/100: Loss: 0.7726749569177628\n",
      "Epoch 19/100: Loss: 0.7744501531124115\n",
      "Epoch 20/100: Loss: 0.7805285215377807\n",
      "Epoch 21/100: Loss: 0.6601187095046044\n",
      "Epoch 22/100: Loss: 0.6884353250265122\n",
      "Epoch 23/100: Loss: 0.6676134929060936\n",
      "Epoch 24/100: Loss: 0.6250513508915901\n",
      "Epoch 25/100: Loss: 0.6831857770681381\n",
      "Epoch 26/100: Loss: 0.6126991420984268\n",
      "Epoch 27/100: Loss: 0.6763328164815903\n",
      "Epoch 28/100: Loss: 0.5661404572427273\n",
      "Epoch 29/100: Loss: 0.5345914542675019\n",
      "Epoch 30/100: Loss: 0.4583026275038719\n",
      "Epoch 31/100: Loss: 0.5519491232931614\n",
      "Epoch 32/100: Loss: 0.43386373296380043\n",
      "Epoch 33/100: Loss: 0.4867071956396103\n",
      "Epoch 34/100: Loss: 0.48540504053235056\n",
      "Epoch 35/100: Loss: 0.44282525330781936\n",
      "Epoch 36/100: Loss: 0.4534398503601551\n",
      "Epoch 37/100: Loss: 0.4584676086902618\n",
      "Epoch 38/100: Loss: 0.3691130317747593\n",
      "Epoch 39/100: Loss: 0.3998152755200863\n",
      "Epoch 40/100: Loss: 0.3458369471132755\n",
      "Epoch 41/100: Loss: 0.36102268621325495\n",
      "Epoch 42/100: Loss: 0.32732395455241203\n",
      "Epoch 43/100: Loss: 0.2962170600891113\n",
      "Epoch 44/100: Loss: 0.29691429287195203\n",
      "Epoch 45/100: Loss: 0.2519969452172518\n",
      "Epoch 46/100: Loss: 0.304682620242238\n",
      "Epoch 47/100: Loss: 0.2308184988796711\n",
      "Epoch 48/100: Loss: 0.26215649470686914\n",
      "Epoch 49/100: Loss: 0.26304756924510003\n",
      "Epoch 50/100: Loss: 0.22899963408708573\n",
      "Epoch 51/100: Loss: 0.22472169250249863\n",
      "Epoch 52/100: Loss: 0.21010419353842735\n",
      "Epoch 53/100: Loss: 0.20777908265590667\n",
      "Epoch 54/100: Loss: 0.1840233862400055\n",
      "Epoch 55/100: Loss: 0.16829049959778786\n",
      "Epoch 56/100: Loss: 0.16089913696050645\n",
      "Epoch 57/100: Loss: 0.14543760679662227\n",
      "Epoch 58/100: Loss: 0.16650438569486142\n",
      "Epoch 59/100: Loss: 0.15441795140504838\n",
      "Epoch 60/100: Loss: 0.1393093064427376\n",
      "Epoch 61/100: Loss: 0.11512929610908032\n",
      "Epoch 62/100: Loss: 0.15000709593296052\n",
      "Epoch 63/100: Loss: 0.11457287259399891\n",
      "Epoch 64/100: Loss: 0.10234391205012798\n",
      "Epoch 65/100: Loss: 0.08233734332025051\n",
      "Epoch 66/100: Loss: 0.1152655329555273\n",
      "Epoch 67/100: Loss: 0.09269883204251528\n",
      "Epoch 68/100: Loss: 0.08258016239851713\n",
      "Epoch 69/100: Loss: 0.08877732530236244\n",
      "Epoch 70/100: Loss: 0.06875240160152316\n",
      "Epoch 71/100: Loss: 0.0701737567782402\n",
      "Epoch 72/100: Loss: 0.07096903035417199\n",
      "Epoch 73/100: Loss: 0.05702939201146364\n",
      "Epoch 74/100: Loss: 0.04653286840766668\n",
      "Epoch 75/100: Loss: 0.05244416547939181\n",
      "Epoch 76/100: Loss: 0.03688274463638663\n",
      "Epoch 77/100: Loss: 0.04206642517820001\n",
      "Epoch 78/100: Loss: 0.04034449737519026\n",
      "Epoch 79/100: Loss: 0.03125651925802231\n",
      "Epoch 80/100: Loss: 0.030872649792581797\n",
      "Epoch 81/100: Loss: 0.02313080579042435\n",
      "Epoch 82/100: Loss: 0.023967938078567386\n",
      "Epoch 83/100: Loss: 0.02045611187350005\n",
      "Epoch 84/100: Loss: 0.018765745661221446\n",
      "Epoch 85/100: Loss: 0.017613895633257926\n",
      "Epoch 86/100: Loss: 0.013923439080826938\n",
      "Epoch 87/100: Loss: 0.014742216421291232\n",
      "Epoch 88/100: Loss: 0.011330055724829435\n",
      "Epoch 89/100: Loss: 0.008959212596528232\n",
      "Epoch 90/100: Loss: 0.008610312337987125\n",
      "Epoch 91/100: Loss: 0.007044619508087635\n",
      "Epoch 92/100: Loss: 0.007039011339657009\n",
      "Epoch 93/100: Loss: 0.005151284078601747\n",
      "Epoch 94/100: Loss: 0.005143912194762379\n",
      "Epoch 95/100: Loss: 0.004221598367439583\n",
      "Epoch 96/100: Loss: 0.003948398405918851\n",
      "Epoch 97/100: Loss: 0.002971265418455005\n",
      "Epoch 98/100: Loss: 0.0030443205207120625\n",
      "Epoch 99/100: Loss: 0.002418052313441876\n",
      "Epoch 100/100: Loss: 0.002248485994641669\n",
      "time costs: 439.93897676467896\n",
      "--------------------\n",
      "training epoch: 6\n",
      "Epoch 1/100: Loss: 1.0214021265506745\n",
      "Epoch 2/100: Loss: 1.0074678301811217\n",
      "Epoch 3/100: Loss: 1.008550900220871\n",
      "Epoch 4/100: Loss: 1.0043280482292176\n",
      "Epoch 5/100: Loss: 0.9955217957496643\n",
      "Epoch 6/100: Loss: 0.9952830016613007\n",
      "Epoch 7/100: Loss: 0.9838114231824875\n",
      "Epoch 8/100: Loss: 0.9979357063770294\n",
      "Epoch 9/100: Loss: 1.0184953093528748\n",
      "Epoch 10/100: Loss: 1.0007137209177017\n",
      "Epoch 11/100: Loss: 0.9798049122095108\n",
      "Epoch 12/100: Loss: 0.9674810916185379\n",
      "Epoch 13/100: Loss: 0.9388468712568283\n",
      "Epoch 14/100: Loss: 0.9871999979019165\n",
      "Epoch 15/100: Loss: 1.0127770602703094\n",
      "Epoch 16/100: Loss: 0.9946222096681595\n",
      "Epoch 17/100: Loss: 0.9632499068975449\n",
      "Epoch 18/100: Loss: 0.973631602525711\n",
      "Epoch 19/100: Loss: 0.9954723805189133\n",
      "Epoch 20/100: Loss: 0.9597052067518235\n",
      "Epoch 21/100: Loss: 0.937695375084877\n",
      "Epoch 22/100: Loss: 0.9632922351360321\n",
      "Epoch 23/100: Loss: 0.9687769204378128\n",
      "Epoch 24/100: Loss: 0.9541901707649231\n",
      "Epoch 25/100: Loss: 0.9622485607862472\n",
      "Epoch 26/100: Loss: 0.9738224238157273\n",
      "Epoch 27/100: Loss: 0.9493624448776246\n",
      "Epoch 28/100: Loss: 0.9510633438825608\n",
      "Epoch 29/100: Loss: 0.9450716078281403\n",
      "Epoch 30/100: Loss: 0.9281900882720947\n",
      "Epoch 31/100: Loss: 0.9166307628154755\n",
      "Epoch 32/100: Loss: 0.9318040192127228\n",
      "Epoch 33/100: Loss: 0.9161285042762757\n",
      "Epoch 34/100: Loss: 0.9060518234968186\n",
      "Epoch 35/100: Loss: 0.9179773092269897\n",
      "Epoch 36/100: Loss: 0.9041380614042283\n",
      "Epoch 37/100: Loss: 0.8823361366987228\n",
      "Epoch 38/100: Loss: 0.8650570631027221\n",
      "Epoch 39/100: Loss: 0.861432084441185\n",
      "Epoch 40/100: Loss: 0.8891550689935684\n",
      "Epoch 41/100: Loss: 0.8585909605026245\n",
      "Epoch 42/100: Loss: 0.8317342549562454\n",
      "Epoch 43/100: Loss: 0.8494440376758575\n",
      "Epoch 44/100: Loss: 0.8077373832464219\n",
      "Epoch 45/100: Loss: 0.8115857541561127\n",
      "Epoch 46/100: Loss: 0.7892230927944184\n",
      "Epoch 47/100: Loss: 0.7114999681711197\n",
      "Epoch 48/100: Loss: 0.7345072120428086\n",
      "Epoch 49/100: Loss: 0.8088168874382973\n",
      "Epoch 50/100: Loss: 0.7331887945532799\n",
      "Epoch 51/100: Loss: 0.7282246872782707\n",
      "Epoch 52/100: Loss: 0.7732869416475296\n",
      "Epoch 53/100: Loss: 0.6930759072303772\n",
      "Epoch 54/100: Loss: 0.6453075349330902\n",
      "Epoch 55/100: Loss: 0.6908710122108459\n",
      "Epoch 56/100: Loss: 0.641210600733757\n",
      "Epoch 57/100: Loss: 0.5773687198758125\n",
      "Epoch 58/100: Loss: 0.6540810987353325\n",
      "Epoch 59/100: Loss: 0.6024119533598423\n",
      "Epoch 60/100: Loss: 0.5749873630702496\n",
      "Epoch 61/100: Loss: 0.5324053049087525\n",
      "Epoch 62/100: Loss: 0.5733221128582955\n",
      "Epoch 63/100: Loss: 0.4494454048573971\n",
      "Epoch 64/100: Loss: 0.6381983533501625\n",
      "Epoch 65/100: Loss: 0.5525555655360221\n",
      "Epoch 66/100: Loss: 0.5029591217637062\n",
      "Epoch 67/100: Loss: 0.6242637373507023\n",
      "Epoch 68/100: Loss: 0.42909923382103443\n",
      "Epoch 69/100: Loss: 0.5643280901014804\n",
      "Epoch 70/100: Loss: 0.6801607213914395\n",
      "Epoch 71/100: Loss: 0.5103681892156601\n",
      "Epoch 72/100: Loss: 0.45919925421476365\n",
      "Epoch 73/100: Loss: 0.46430124044418336\n",
      "Epoch 74/100: Loss: 0.43230626210570333\n",
      "Epoch 75/100: Loss: 0.43313266336917877\n",
      "Epoch 76/100: Loss: 0.48921163864433764\n",
      "Epoch 77/100: Loss: 0.5215641424059868\n",
      "Epoch 78/100: Loss: 0.38760797008872033\n",
      "Epoch 79/100: Loss: 0.44062225632369517\n",
      "Epoch 80/100: Loss: 0.5502211794257164\n",
      "Epoch 81/100: Loss: 0.5322240680456162\n",
      "Epoch 82/100: Loss: 0.4161164201796055\n",
      "Epoch 83/100: Loss: 0.3340959344059229\n",
      "Epoch 84/100: Loss: 0.3030694410204887\n",
      "Epoch 85/100: Loss: 0.4873594118282199\n",
      "Epoch 86/100: Loss: 0.4658310808241367\n",
      "Epoch 87/100: Loss: 0.39716475568711757\n",
      "Epoch 88/100: Loss: 0.3061850003898144\n",
      "Epoch 89/100: Loss: 0.41118165403604506\n",
      "Epoch 90/100: Loss: 0.2885808670893312\n",
      "Epoch 91/100: Loss: 0.4656151231378317\n",
      "Epoch 92/100: Loss: 0.4243547337129712\n",
      "Epoch 93/100: Loss: 0.36034068316221235\n",
      "Epoch 94/100: Loss: 0.34045321866869926\n",
      "Epoch 95/100: Loss: 0.33766299318522214\n",
      "Epoch 96/100: Loss: 0.38649728819727897\n",
      "Epoch 97/100: Loss: 0.454837454482913\n",
      "Epoch 98/100: Loss: 0.3219372447580099\n",
      "Epoch 99/100: Loss: 0.3466069120913744\n",
      "Epoch 100/100: Loss: 0.4200080815702677\n",
      "time costs: 505.7696454524994\n",
      "--------------------\n",
      "training epoch: 7\n",
      "Epoch 1/100: Loss: 1.0296144962310791\n",
      "Epoch 2/100: Loss: 1.0230324119329453\n",
      "Epoch 3/100: Loss: 1.010230189561844\n",
      "Epoch 4/100: Loss: 1.0164399951696397\n",
      "Epoch 5/100: Loss: 0.9742037534713746\n",
      "Epoch 6/100: Loss: 0.9529728829860687\n",
      "Epoch 7/100: Loss: 0.9982695192098617\n",
      "Epoch 8/100: Loss: 1.0142906576395034\n",
      "Epoch 9/100: Loss: 0.9852917402982712\n",
      "Epoch 10/100: Loss: 0.9514123916625976\n",
      "Epoch 11/100: Loss: 0.9769556283950805\n",
      "Epoch 12/100: Loss: 0.9281373471021652\n",
      "Epoch 13/100: Loss: 0.9500681966543197\n",
      "Epoch 14/100: Loss: 0.9189719170331955\n",
      "Epoch 15/100: Loss: 0.9174394994974137\n",
      "Epoch 16/100: Loss: 0.9510645776987076\n",
      "Epoch 17/100: Loss: 0.9075960427522659\n",
      "Epoch 18/100: Loss: 0.8836664527654647\n",
      "Epoch 19/100: Loss: 0.8767137974500656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: Loss: 0.8785566449165344\n",
      "Epoch 21/100: Loss: 0.8427524417638779\n",
      "Epoch 22/100: Loss: 0.8598359555006028\n",
      "Epoch 23/100: Loss: 0.7747902303934098\n",
      "Epoch 24/100: Loss: 0.768320968747139\n",
      "Epoch 25/100: Loss: 0.7952550396323204\n",
      "Epoch 26/100: Loss: 0.7958550944924354\n",
      "Epoch 27/100: Loss: 0.6174100920557976\n",
      "Epoch 28/100: Loss: 0.7742229416966439\n",
      "Epoch 29/100: Loss: 0.6850416898727417\n",
      "Epoch 30/100: Loss: 0.6593569725751877\n",
      "Epoch 31/100: Loss: 0.588148795068264\n",
      "Epoch 32/100: Loss: 0.6103693816810847\n",
      "Epoch 33/100: Loss: 0.6650911178439856\n",
      "Epoch 34/100: Loss: 0.7110437545925379\n",
      "Epoch 35/100: Loss: 0.6370287828147412\n",
      "Epoch 36/100: Loss: 0.6010923482477665\n",
      "Epoch 37/100: Loss: 0.6737911693751812\n",
      "Epoch 38/100: Loss: 0.6637267449870705\n",
      "Epoch 39/100: Loss: 0.4226053785532713\n",
      "Epoch 40/100: Loss: 0.5985745741054416\n",
      "Epoch 41/100: Loss: 0.6620442848652601\n",
      "Epoch 42/100: Loss: 0.5131136488169432\n",
      "Epoch 43/100: Loss: 0.5295061249285936\n",
      "Epoch 44/100: Loss: 0.6427737219259143\n",
      "Epoch 45/100: Loss: 0.5518535496667027\n",
      "Epoch 46/100: Loss: 0.5875989839434623\n",
      "Epoch 47/100: Loss: 0.4074247259646654\n",
      "Epoch 48/100: Loss: 0.47729980750009415\n",
      "Epoch 49/100: Loss: 0.546059350669384\n",
      "Epoch 50/100: Loss: 0.5691238148137927\n",
      "Epoch 51/100: Loss: 0.5816454149782657\n",
      "Epoch 52/100: Loss: 0.6054454514756799\n",
      "Epoch 53/100: Loss: 0.5105288902297616\n",
      "Epoch 54/100: Loss: 0.4563181011006236\n",
      "Epoch 55/100: Loss: 0.5386740157380701\n",
      "Epoch 56/100: Loss: 0.5079337146133185\n",
      "Epoch 57/100: Loss: 0.505025739595294\n",
      "Epoch 58/100: Loss: 0.45151929557323456\n",
      "Epoch 59/100: Loss: 0.42773353289812804\n",
      "Epoch 60/100: Loss: 0.47884379848837855\n",
      "Epoch 61/100: Loss: 0.4464556541293859\n",
      "Epoch 62/100: Loss: 0.49579689688980577\n",
      "Epoch 63/100: Loss: 0.4233599351719022\n",
      "Epoch 64/100: Loss: 0.3782787790521979\n",
      "Epoch 65/100: Loss: 0.4450299970805645\n",
      "Epoch 66/100: Loss: 0.3500504959374666\n",
      "Epoch 67/100: Loss: 0.38008263912051915\n",
      "Epoch 68/100: Loss: 0.35996856149286033\n",
      "Epoch 69/100: Loss: 0.4134135594591498\n",
      "Epoch 70/100: Loss: 0.3478378187865019\n",
      "Epoch 71/100: Loss: 0.3452016895636916\n",
      "Epoch 72/100: Loss: 0.3980942364782095\n",
      "Epoch 73/100: Loss: 0.40452476479113103\n",
      "Epoch 74/100: Loss: 0.3189476113766432\n",
      "Epoch 75/100: Loss: 0.4526312485337257\n",
      "Epoch 76/100: Loss: 0.43981567546725275\n",
      "Epoch 77/100: Loss: 0.2850465174764395\n",
      "Epoch 78/100: Loss: 0.25641715452075003\n",
      "Epoch 79/100: Loss: 0.2731374971568584\n",
      "Epoch 80/100: Loss: 0.3718470633029938\n",
      "Epoch 81/100: Loss: 0.3053391110152006\n",
      "Epoch 82/100: Loss: 0.21899195574223995\n",
      "Epoch 83/100: Loss: 0.22535608988255262\n",
      "Epoch 84/100: Loss: 0.20002746302634478\n",
      "Epoch 85/100: Loss: 0.3688026759773493\n",
      "Epoch 86/100: Loss: 0.30938047673553226\n",
      "Epoch 87/100: Loss: 0.23078000582754613\n",
      "Epoch 88/100: Loss: 0.20271067321300507\n",
      "Epoch 89/100: Loss: 0.24649128764867784\n",
      "Epoch 90/100: Loss: 0.3325236874632537\n",
      "Epoch 91/100: Loss: 0.28105967179872093\n",
      "Epoch 92/100: Loss: 0.24535816381685435\n",
      "Epoch 93/100: Loss: 0.17902441984042525\n",
      "Epoch 94/100: Loss: 0.2197369888308458\n",
      "Epoch 95/100: Loss: 0.19107947488082572\n",
      "Epoch 96/100: Loss: 0.2159829217940569\n",
      "Epoch 97/100: Loss: 0.23281759634846821\n",
      "Epoch 98/100: Loss: 0.27860667849890886\n",
      "Epoch 99/100: Loss: 0.3177418031729758\n",
      "Epoch 100/100: Loss: 0.29475954923545944\n",
      "time costs: 505.9477779865265\n",
      "--------------------\n",
      "training epoch: 8\n",
      "Epoch 1/100: Loss: 1.008791446685791\n",
      "Epoch 2/100: Loss: 1.0075921803712844\n",
      "Epoch 3/100: Loss: 0.9856038272380829\n",
      "Epoch 4/100: Loss: 0.9972839236259461\n",
      "Epoch 5/100: Loss: 0.9921009242534637\n",
      "Epoch 6/100: Loss: 0.9865324229001999\n",
      "Epoch 7/100: Loss: 0.9879239708185196\n",
      "Epoch 8/100: Loss: 0.97099090218544\n",
      "Epoch 9/100: Loss: 0.9599382966756821\n",
      "Epoch 10/100: Loss: 0.9560218960046768\n",
      "Epoch 11/100: Loss: 0.9532635241746903\n",
      "Epoch 12/100: Loss: 0.9103679627180099\n",
      "Epoch 13/100: Loss: 0.901727420091629\n",
      "Epoch 14/100: Loss: 0.8949573546648025\n",
      "Epoch 15/100: Loss: 0.9084513455629348\n",
      "Epoch 16/100: Loss: 0.835498109459877\n",
      "Epoch 17/100: Loss: 0.8628661870956421\n",
      "Epoch 18/100: Loss: 0.8616751044988632\n",
      "Epoch 19/100: Loss: 0.8173403054475784\n",
      "Epoch 20/100: Loss: 0.7561920911073685\n",
      "Epoch 21/100: Loss: 0.7252601981163025\n",
      "Epoch 22/100: Loss: 0.6281340569257736\n",
      "Epoch 23/100: Loss: 0.7453356720507145\n",
      "Epoch 24/100: Loss: 0.7225256331264973\n",
      "Epoch 25/100: Loss: 0.6984683305025101\n",
      "Epoch 26/100: Loss: 0.6353742249310017\n",
      "Epoch 27/100: Loss: 0.7473196610808372\n",
      "Epoch 28/100: Loss: 0.7367268189787864\n",
      "Epoch 29/100: Loss: 0.7039196342229843\n",
      "Epoch 30/100: Loss: 0.7248629305511713\n",
      "Epoch 31/100: Loss: 0.7311303541064262\n",
      "Epoch 32/100: Loss: 0.6541718455031514\n",
      "Epoch 33/100: Loss: 0.5855208568274974\n",
      "Epoch 34/100: Loss: 0.6343604148365557\n",
      "Epoch 35/100: Loss: 0.5981907081790269\n",
      "Epoch 36/100: Loss: 0.7597100756131112\n",
      "Epoch 37/100: Loss: 0.648519944306463\n",
      "Epoch 38/100: Loss: 0.7181840782053769\n",
      "Epoch 39/100: Loss: 0.6762458882294595\n",
      "Epoch 40/100: Loss: 0.5326607519760728\n",
      "Epoch 41/100: Loss: 0.6546064426191152\n",
      "Epoch 42/100: Loss: 0.5945006491150707\n",
      "Epoch 43/100: Loss: 0.5836880919523537\n",
      "Epoch 44/100: Loss: 0.6516738954931498\n",
      "Epoch 45/100: Loss: 0.4204876724630594\n",
      "Epoch 46/100: Loss: 0.6525543642230331\n",
      "Epoch 47/100: Loss: 0.439411912066862\n",
      "Epoch 48/100: Loss: 0.5977246009279042\n",
      "Epoch 49/100: Loss: 0.57528550256975\n",
      "Epoch 50/100: Loss: 0.46503381701186297\n",
      "Epoch 51/100: Loss: 0.5490037877112627\n",
      "Epoch 52/100: Loss: 0.6498788577504456\n",
      "Epoch 53/100: Loss: 0.5110309558920563\n",
      "Epoch 54/100: Loss: 0.6172320120036602\n",
      "Epoch 55/100: Loss: 0.5387224158272147\n",
      "Epoch 56/100: Loss: 0.4446773552335799\n",
      "Epoch 57/100: Loss: 0.4825431598350406\n",
      "Epoch 58/100: Loss: 0.49767555501312016\n",
      "Epoch 59/100: Loss: 0.3813429041299969\n",
      "Epoch 60/100: Loss: 0.5828610133845359\n",
      "Epoch 61/100: Loss: 0.4756921292282641\n",
      "Epoch 62/100: Loss: 0.46676148199476303\n",
      "Epoch 63/100: Loss: 0.5992343126796186\n",
      "Epoch 64/100: Loss: 0.49014019295573236\n",
      "Epoch 65/100: Loss: 0.5483563421294093\n",
      "Epoch 66/100: Loss: 0.5131828071549535\n",
      "Epoch 67/100: Loss: 0.41469296319410204\n",
      "Epoch 68/100: Loss: 0.33756891526281835\n",
      "Epoch 69/100: Loss: 0.4750509146600962\n",
      "Epoch 70/100: Loss: 0.3683104306459427\n",
      "Epoch 71/100: Loss: 0.39482501689344646\n",
      "Epoch 72/100: Loss: 0.38120729345828297\n",
      "Epoch 73/100: Loss: 0.3236310009844601\n",
      "Epoch 74/100: Loss: 0.47159505169838667\n",
      "Epoch 75/100: Loss: 0.4519256057217717\n",
      "Epoch 76/100: Loss: 0.3098300932906568\n",
      "Epoch 77/100: Loss: 0.4552499858662486\n",
      "Epoch 78/100: Loss: 0.3893244187347591\n",
      "Epoch 79/100: Loss: 0.32039093486964704\n",
      "Epoch 80/100: Loss: 0.32720674471929667\n",
      "Epoch 81/100: Loss: 0.3727524525485933\n",
      "Epoch 82/100: Loss: 0.393160249479115\n",
      "Epoch 83/100: Loss: 0.35940128229558466\n",
      "Epoch 84/100: Loss: 0.3555320297367871\n",
      "Epoch 85/100: Loss: 0.31637189285829664\n",
      "Epoch 86/100: Loss: 0.3339519852772355\n",
      "Epoch 87/100: Loss: 0.241558008082211\n",
      "Epoch 88/100: Loss: 0.31556641599163415\n",
      "Epoch 89/100: Loss: 0.2502691733650863\n",
      "Epoch 90/100: Loss: 0.30765683939680455\n",
      "Epoch 91/100: Loss: 0.1900227904319763\n",
      "Epoch 92/100: Loss: 0.1980571798980236\n",
      "Epoch 93/100: Loss: 0.23356343330815435\n",
      "Epoch 94/100: Loss: 0.23836341686546803\n",
      "Epoch 95/100: Loss: 0.3001484110020101\n",
      "Epoch 96/100: Loss: 0.2332401828840375\n",
      "Epoch 97/100: Loss: 0.2812353793531656\n",
      "Epoch 98/100: Loss: 0.3001391887664795\n",
      "Epoch 99/100: Loss: 0.2861996376886964\n",
      "Epoch 100/100: Loss: 0.2031450554728508\n",
      "time costs: 558.6895506381989\n",
      "--------------------\n",
      "training epoch: 9\n",
      "Epoch 1/100: Loss: 1.0187413334846496\n",
      "Epoch 2/100: Loss: 0.9969565212726593\n",
      "Epoch 3/100: Loss: 0.9816070079803467\n",
      "Epoch 4/100: Loss: 1.0208384037017821\n",
      "Epoch 5/100: Loss: 0.9834814697504044\n",
      "Epoch 6/100: Loss: 0.9712864458560944\n",
      "Epoch 7/100: Loss: 0.9882848232984542\n",
      "Epoch 8/100: Loss: 0.9840614795684814\n",
      "Epoch 9/100: Loss: 0.9918922781944275\n",
      "Epoch 10/100: Loss: 0.971267294883728\n",
      "Epoch 11/100: Loss: 0.9571856290102005\n",
      "Epoch 12/100: Loss: 0.9607234090566635\n",
      "Epoch 13/100: Loss: 0.967097869515419\n",
      "Epoch 14/100: Loss: 0.9562636941671372\n",
      "Epoch 15/100: Loss: 0.9442006915807724\n",
      "Epoch 16/100: Loss: 0.9413637608289719\n",
      "Epoch 17/100: Loss: 0.9493744760751724\n",
      "Epoch 18/100: Loss: 0.9287696540355682\n",
      "Epoch 19/100: Loss: 0.8881499111652374\n",
      "Epoch 20/100: Loss: 0.8925958484411239\n",
      "Epoch 21/100: Loss: 0.9091166734695435\n",
      "Epoch 22/100: Loss: 0.8628751039505005\n",
      "Epoch 23/100: Loss: 0.8177323400974273\n",
      "Epoch 24/100: Loss: 0.8426622778177262\n",
      "Epoch 25/100: Loss: 0.8693847984075547\n",
      "Epoch 26/100: Loss: 0.8437981933355332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: Loss: 0.8268505066633225\n",
      "Epoch 28/100: Loss: 0.7822646111249923\n",
      "Epoch 29/100: Loss: 0.7371292173862457\n",
      "Epoch 30/100: Loss: 0.8081522077322006\n",
      "Epoch 31/100: Loss: 0.6642576232552528\n",
      "Epoch 32/100: Loss: 0.7160627990961075\n",
      "Epoch 33/100: Loss: 0.5578299596905708\n",
      "Epoch 34/100: Loss: 0.7451296828687191\n",
      "Epoch 35/100: Loss: 0.6721149869263172\n",
      "Epoch 36/100: Loss: 0.6566960334777832\n",
      "Epoch 37/100: Loss: 0.614534518122673\n",
      "Epoch 38/100: Loss: 0.6964429818093777\n",
      "Epoch 39/100: Loss: 0.47119549848139286\n",
      "Epoch 40/100: Loss: 0.5308350160717964\n",
      "Epoch 41/100: Loss: 0.5563281141221523\n",
      "Epoch 42/100: Loss: 0.5759158715605736\n",
      "Epoch 43/100: Loss: 0.739711994677782\n",
      "Epoch 44/100: Loss: 0.4045522568747401\n",
      "Epoch 45/100: Loss: 0.5201793547719717\n",
      "Epoch 46/100: Loss: 0.4351925926283002\n",
      "Epoch 47/100: Loss: 0.4247325582429767\n",
      "Epoch 48/100: Loss: 0.41551062017679213\n",
      "Epoch 49/100: Loss: 0.5394846081733704\n",
      "Epoch 50/100: Loss: 0.5844028901308775\n",
      "Epoch 51/100: Loss: 0.46599756702780726\n",
      "Epoch 52/100: Loss: 0.5035714737139643\n",
      "Epoch 53/100: Loss: 0.3752662525512278\n",
      "Epoch 54/100: Loss: 0.37460502712056043\n",
      "Epoch 55/100: Loss: 0.4684685087762773\n",
      "Epoch 56/100: Loss: 0.35191563740372656\n",
      "Epoch 57/100: Loss: 0.3116077594459057\n",
      "Epoch 58/100: Loss: 0.3177150941453874\n",
      "Epoch 59/100: Loss: 0.3212986007332802\n",
      "Epoch 60/100: Loss: 0.43463338930159806\n",
      "Epoch 61/100: Loss: 0.4603730064816773\n",
      "Epoch 62/100: Loss: 0.24244163506664335\n",
      "Epoch 63/100: Loss: 0.2775934993289411\n",
      "Epoch 64/100: Loss: 0.3270890033338219\n",
      "Epoch 65/100: Loss: 0.42078475933521986\n",
      "Epoch 66/100: Loss: 0.3575936732813716\n",
      "Epoch 67/100: Loss: 0.2940152714494616\n",
      "Epoch 68/100: Loss: 0.31464604726061224\n",
      "Epoch 69/100: Loss: 0.21084373453631997\n",
      "Epoch 70/100: Loss: 0.3253849571570754\n",
      "Epoch 71/100: Loss: 0.3511416707187891\n",
      "Epoch 72/100: Loss: 0.30288014439865946\n",
      "Epoch 73/100: Loss: 0.34666331149637697\n",
      "Epoch 74/100: Loss: 0.3067350532859564\n",
      "Epoch 75/100: Loss: 0.24610969237983227\n",
      "Epoch 76/100: Loss: 0.27053101975470784\n",
      "Epoch 77/100: Loss: 0.271646854840219\n",
      "Epoch 78/100: Loss: 0.2814175393432379\n",
      "Epoch 79/100: Loss: 0.22061790581792592\n",
      "Epoch 80/100: Loss: 0.20826046094298362\n",
      "Epoch 81/100: Loss: 0.2342755740508437\n",
      "Epoch 82/100: Loss: 0.19305532691068947\n",
      "Epoch 83/100: Loss: 0.21913232230581342\n",
      "Epoch 84/100: Loss: 0.19182626488618554\n",
      "Epoch 85/100: Loss: 0.11215812359005213\n",
      "Epoch 86/100: Loss: 0.09654098926112056\n",
      "Epoch 87/100: Loss: 0.14283021520823241\n",
      "Epoch 88/100: Loss: 0.2095771449385211\n",
      "Epoch 89/100: Loss: 0.0946250407025218\n",
      "Epoch 90/100: Loss: 0.1407133714761585\n",
      "Epoch 91/100: Loss: 0.15830840573180466\n",
      "Epoch 92/100: Loss: 0.1411164896097034\n",
      "Epoch 93/100: Loss: 0.10782240356784314\n",
      "Epoch 94/100: Loss: 0.11785707424860448\n",
      "Epoch 95/100: Loss: 0.14169540101429448\n",
      "Epoch 96/100: Loss: 0.11270504904678091\n",
      "Epoch 97/100: Loss: 0.11312710006022826\n",
      "Epoch 98/100: Loss: 0.0877748055383563\n",
      "Epoch 99/100: Loss: 0.12522626146674157\n",
      "Epoch 100/100: Loss: 0.07334449710324406\n",
      "time costs: 610.5481033325195\n",
      "--------------------\n",
      "training epoch: 10\n",
      "Epoch 1/100: Loss: 1.0009708732366562\n",
      "Epoch 2/100: Loss: 1.0226537883281708\n",
      "Epoch 3/100: Loss: 0.9854562789201736\n",
      "Epoch 4/100: Loss: 0.9915114909410476\n",
      "Epoch 5/100: Loss: 0.9368016183376312\n",
      "Epoch 6/100: Loss: 1.0009836941957473\n",
      "Epoch 7/100: Loss: 0.9761932551860809\n",
      "Epoch 8/100: Loss: 0.9876248776912689\n",
      "Epoch 9/100: Loss: 0.9714583873748779\n",
      "Epoch 10/100: Loss: 1.0084737300872804\n",
      "Epoch 11/100: Loss: 1.020842546224594\n",
      "Epoch 12/100: Loss: 0.9943403780460358\n",
      "Epoch 13/100: Loss: 1.0033039152622223\n",
      "Epoch 14/100: Loss: 0.9967534393072128\n",
      "Epoch 15/100: Loss: 0.9886680603027344\n",
      "Epoch 16/100: Loss: 0.960142856836319\n",
      "Epoch 17/100: Loss: 0.9410513132810593\n",
      "Epoch 18/100: Loss: 0.9855383843183517\n",
      "Epoch 19/100: Loss: 0.9417548567056656\n",
      "Epoch 20/100: Loss: 0.9239830136299133\n",
      "Epoch 21/100: Loss: 0.9615763187408447\n",
      "Epoch 22/100: Loss: 0.9392403334379196\n",
      "Epoch 23/100: Loss: 0.9460884302854538\n",
      "Epoch 24/100: Loss: 0.931249612569809\n",
      "Epoch 25/100: Loss: 0.9311958700418472\n",
      "Epoch 26/100: Loss: 0.913316285610199\n",
      "Epoch 27/100: Loss: 0.8880058050155639\n",
      "Epoch 28/100: Loss: 0.8993946254253388\n",
      "Epoch 29/100: Loss: 0.8619559615850448\n",
      "Epoch 30/100: Loss: 0.8025663256645202\n",
      "Epoch 31/100: Loss: 0.8674734085798264\n",
      "Epoch 32/100: Loss: 0.7936083823442459\n",
      "Epoch 33/100: Loss: 0.6966766774654388\n",
      "Epoch 34/100: Loss: 0.8795522779226304\n",
      "Epoch 35/100: Loss: 0.8108683496713638\n",
      "Epoch 36/100: Loss: 0.7934657722711563\n",
      "Epoch 37/100: Loss: 0.7175824284553528\n",
      "Epoch 38/100: Loss: 0.6518456339836121\n",
      "Epoch 39/100: Loss: 0.7161440327763557\n",
      "Epoch 40/100: Loss: 0.7176556617021561\n",
      "Epoch 41/100: Loss: 0.7037272810935974\n",
      "Epoch 42/100: Loss: 0.6640727609395981\n",
      "Epoch 43/100: Loss: 0.6495894595980645\n",
      "Epoch 44/100: Loss: 0.7073554456233978\n",
      "Epoch 45/100: Loss: 0.7458708226680756\n",
      "Epoch 46/100: Loss: 0.7013655871152877\n",
      "Epoch 47/100: Loss: 0.6813462376594543\n",
      "Epoch 48/100: Loss: 0.6607437193393707\n",
      "Epoch 49/100: Loss: 0.6518820106983185\n",
      "Epoch 50/100: Loss: 0.5798873499035835\n",
      "Epoch 51/100: Loss: 0.6895509019494057\n",
      "Epoch 52/100: Loss: 0.6378054708242417\n",
      "Epoch 53/100: Loss: 0.6305858463048934\n",
      "Epoch 54/100: Loss: 0.5302188262343407\n",
      "Epoch 55/100: Loss: 0.515144544839859\n",
      "Epoch 56/100: Loss: 0.5308915495872497\n",
      "Epoch 57/100: Loss: 0.560827299952507\n",
      "Epoch 58/100: Loss: 0.5831897020339966\n",
      "Epoch 59/100: Loss: 0.48509134352207184\n",
      "Epoch 60/100: Loss: 0.5628252774477005\n",
      "Epoch 61/100: Loss: 0.6890466146171093\n",
      "Epoch 62/100: Loss: 0.5776094995439053\n",
      "Epoch 63/100: Loss: 0.5586545258760452\n",
      "Epoch 64/100: Loss: 0.6178780600428582\n",
      "Epoch 65/100: Loss: 0.5568421319127083\n",
      "Epoch 66/100: Loss: 0.5847231805324554\n",
      "Epoch 67/100: Loss: 0.5029536411166191\n",
      "Epoch 68/100: Loss: 0.4775585576891899\n",
      "Epoch 69/100: Loss: 0.5134094715118408\n",
      "Epoch 70/100: Loss: 0.422158332914114\n",
      "Epoch 71/100: Loss: 0.5363395124673843\n",
      "Epoch 72/100: Loss: 0.4245090797543526\n",
      "Epoch 73/100: Loss: 0.41981595307588576\n",
      "Epoch 74/100: Loss: 0.4989119812846184\n",
      "Epoch 75/100: Loss: 0.5378424450755119\n",
      "Epoch 76/100: Loss: 0.47543990835547445\n",
      "Epoch 77/100: Loss: 0.5032292991876602\n",
      "Epoch 78/100: Loss: 0.3502190627157688\n",
      "Epoch 79/100: Loss: 0.40759657844901087\n",
      "Epoch 80/100: Loss: 0.46945851370692254\n",
      "Epoch 81/100: Loss: 0.392800810188055\n",
      "Epoch 82/100: Loss: 0.504343108087778\n",
      "Epoch 83/100: Loss: 0.3726511515676975\n",
      "Epoch 84/100: Loss: 0.46193699091672896\n",
      "Epoch 85/100: Loss: 0.4411939263343811\n",
      "Epoch 86/100: Loss: 0.4058325797319412\n",
      "Epoch 87/100: Loss: 0.4757892645895481\n",
      "Epoch 88/100: Loss: 0.42848316505551337\n",
      "Epoch 89/100: Loss: 0.36805150732398034\n",
      "Epoch 90/100: Loss: 0.31881106123328207\n",
      "Epoch 91/100: Loss: 0.29173141829669474\n",
      "Epoch 92/100: Loss: 0.4020172666758299\n",
      "Epoch 93/100: Loss: 0.31800386533141134\n",
      "Epoch 94/100: Loss: 0.33743348382413385\n",
      "Epoch 95/100: Loss: 0.3596551641821861\n",
      "Epoch 96/100: Loss: 0.3393052704632282\n",
      "Epoch 97/100: Loss: 0.32224614471197127\n",
      "Epoch 98/100: Loss: 0.2860876191407442\n",
      "Epoch 99/100: Loss: 0.28823655657470226\n",
      "Epoch 100/100: Loss: 0.30854883790016174\n",
      "time costs: 900.7922403812408\n"
     ]
    }
   ],
   "source": [
    "accuarcies = []\n",
    "times = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'training epoch: {i + 1}')\n",
    "    qmodel = QModel(input_size, hidden_size, num_output, \n",
    "                num_layers = num_layers, ctx = ctx, mode='classical')\n",
    "    optimizer = torch.optim.Adam(qmodel.parameters(), lr = 0.001)\n",
    "    loss_func = nn.MSELoss()\n",
    "    start = time.time()\n",
    "    losses = train_model(qmodel, train_data, batch_size=20,          \n",
    "                   loss_func = loss_func, optimizer = optimizer, epoch = 100)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'time costs: {end - start}')\n",
    "    times.append(end - start)\n",
    "    \n",
    "    acc = calculate_accuarcy(qmodel, X_test, y_test)[0]\n",
    "    accuarcies.append(acc)\n",
    "    \n",
    "    with open(f'loss/layer4/loss_layer4_{i + 1}.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(losses, pkl_file)\n",
    "    torch.save(qmodel.state_dict(), f\"model/layer4/model_layer4_{i+1}.pt\")\n",
    "    \n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac9c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses, color=\"#FF6666\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3343ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantum paramers: 112\n"
     ]
    }
   ],
   "source": [
    "print(f'quantum paramers: {QLSTM(1, 1, ctx = ctx).qparameters_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5af0126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5555867"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.5555867\n",
    "# \n",
    "# [tensor(0.2734),\n",
    "#  tensor(0.2269),\n",
    "#  tensor(0.7576),\n",
    "#  tensor(0.9693),\n",
    "#  tensor(0.3253),\n",
    "#  tensor(0.5055),\n",
    "#  tensor(0.5879),\n",
    "#  tensor(0.5167),\n",
    "#  tensor(0.4568),\n",
    "#  tensor(0.9364)]\n",
    "np.mean(accuarcies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e317ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "695.3771038532257"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 695.3771038532257 \n",
    "np.mean(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2370cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
